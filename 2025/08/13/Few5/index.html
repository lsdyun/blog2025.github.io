<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/blog2025.github.io/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/blog2025.github.io/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/blog2025.github.io/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/blog2025.github.io/images/logo.svg" color="#222">

<link rel="stylesheet" href="/blog2025.github.io/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"lsdyun.github.io","root":"/blog2025.github.io/","images":"/blog2025.github.io/images","scheme":"Pisces","darkmode":false,"version":"8.22.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/blog2025.github.io/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/blog2025.github.io/js/config.js"></script>

    <meta name="description" content="google-research&#x2F;big_visionhttps:&#x2F;&#x2F;github.com&#x2F;google-research&#x2F;big_vision Big Vision 此代码库旨在使用 Cloud TPU VM 或 GPU 机器训练大规模视觉模型。它基于 Jax&#x2F;Flax 库，并使用 tf.data 和 TensorFlow Datasets 构建可扩展且可复现的输入流水线。 开">
<meta property="og:type" content="article">
<meta property="og:title" content="google-research&#x2F;big_vision">
<meta property="og:url" content="https://lsdyun.github.io/blog2025.github.io/2025/08/13/Few5/index.html">
<meta property="og:site_name" content="记录博客">
<meta property="og:description" content="google-research&#x2F;big_visionhttps:&#x2F;&#x2F;github.com&#x2F;google-research&#x2F;big_vision Big Vision 此代码库旨在使用 Cloud TPU VM 或 GPU 机器训练大规模视觉模型。它基于 Jax&#x2F;Flax 库，并使用 tf.data 和 TensorFlow Datasets 构建可扩展且可复现的输入流水线。 开">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-08-13T02:39:41.825Z">
<meta property="article:modified_time" content="2025-08-15T04:25:27.851Z">
<meta property="article:author" content="lsdyun">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://lsdyun.github.io/blog2025.github.io/2025/08/13/Few5/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://lsdyun.github.io/blog2025.github.io/2025/08/13/Few5/","path":"2025/08/13/Few5/","title":"google-research/big_vision"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>google-research/big_vision | 记录博客</title>
  








  <noscript>
    <link rel="stylesheet" href="/blog2025.github.io/css/noscript.css">
  </noscript>
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/blog2025.github.io/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">记录博客</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/blog2025.github.io/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/blog2025.github.io/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-categories"><a href="/blog2025.github.io/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/blog2025.github.io/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#google-research-big-vision"><span class="nav-number">1.</span> <span class="nav-text">google-research&#x2F;big_vision</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9E%B6%E6%9E%84%E7%A0%94%E7%A9%B6"><span class="nav-number">2.</span> <span class="nav-text">架构研究</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale"><span class="nav-number">2.1.</span> <span class="nav-text">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="nav-number">2.1.1.</span> <span class="nav-text">核心思想</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E5%88%9B%E6%96%B0%E7%82%B9"><span class="nav-number">2.1.2.</span> <span class="nav-text">关键创新点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%87%8D%E8%A6%81%E5%8F%91%E7%8E%B0"><span class="nav-number">2.1.3.</span> <span class="nav-text">重要发现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94%EF%BC%88%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%A2%84%E8%AE%AD%E7%BB%83%EF%BC%89"><span class="nav-number">2.1.4.</span> <span class="nav-text">性能对比（大数据预训练）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9%E5%88%9D%E5%AD%A6%E8%80%85%E7%9A%84%E5%90%AF%E5%8F%91"><span class="nav-number">2.1.5.</span> <span class="nav-text">对初学者的启发</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%E8%A7%A3ViT%E6%B5%81%E7%A8%8B%EF%BC%88%E7%AE%80%E5%8C%96%E7%89%88%EF%BC%89"><span class="nav-number">2.1.6.</span> <span class="nav-text">图解ViT流程（简化版）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Scaling-Vision-Transformer"><span class="nav-number">2.2.</span> <span class="nav-text">Scaling Vision Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E7%A0%94%E7%A9%B6%E7%9B%AE%E6%A0%87"><span class="nav-number">2.2.1.</span> <span class="nav-text">1. 研究目标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%85%B3%E9%94%AE%E5%8F%91%E7%8E%B0"><span class="nav-number">2.2.2.</span> <span class="nav-text">2. 关键发现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E7%BC%A9%E6%94%BE%E5%AE%9A%E5%BE%8B%EF%BC%88Scaling-Laws%EF%BC%89"><span class="nav-number">2.2.2.1.</span> <span class="nav-text">(1) 缩放定律（Scaling Laws）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BC%98%E5%8A%BF"><span class="nav-number">2.2.2.2.</span> <span class="nav-text">(2) 大模型的优势</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E6%89%A9%E5%B1%95%E6%9E%81%E9%99%90"><span class="nav-number">2.2.2.3.</span> <span class="nav-text">(3) 扩展极限</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E6%8A%80%E6%9C%AF%E5%88%9B%E6%96%B0"><span class="nav-number">2.2.3.</span> <span class="nav-text">3. 技术创新</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E8%AE%AD%E7%BB%83%E4%BC%98%E5%8C%96"><span class="nav-number">2.2.3.1.</span> <span class="nav-text">(1) 训练优化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E5%86%85%E5%AD%98%E4%BC%98%E5%8C%96"><span class="nav-number">2.2.3.2.</span> <span class="nav-text">(2) 内存优化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E6%95%B0%E6%8D%AE%E6%89%A9%E5%B1%95"><span class="nav-number">2.2.3.3.</span> <span class="nav-text">(3) 数据扩展</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E5%AF%B9%E5%88%9D%E5%AD%A6%E8%80%85%E7%9A%84%E5%90%AF%E5%8F%91"><span class="nav-number">2.2.4.</span> <span class="nav-text">4. 对初学者的启发</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E9%87%8D%E8%A6%81%E5%9B%BE%E8%A1%A8%E9%80%9F%E8%A7%88"><span class="nav-number">2.2.5.</span> <span class="nav-text">5. 重要图表速览</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#How-to-train-your-ViT-Data-Augmentation-and-Regularization-in-Vision-Transformers"><span class="nav-number">2.3.</span> <span class="nav-text">How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E9%97%AE%E9%A2%98"><span class="nav-number">2.3.1.</span> <span class="nav-text">核心问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BB%E8%A6%81%E5%8F%91%E7%8E%B0%EF%BC%88%E5%AF%B9%E5%88%9D%E5%AD%A6%E8%80%85%E6%9C%80%E9%87%8D%E8%A6%81%EF%BC%89"><span class="nav-number">2.3.2.</span> <span class="nav-text">主要发现（对初学者最重要）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%99%E5%88%9D%E5%AD%A6%E8%80%85%E7%9A%84%E5%AE%9E%E7%94%A8%E5%BB%BA%E8%AE%AE%E6%80%BB%E7%BB%93"><span class="nav-number">2.3.3.</span> <span class="nav-text">给初学者的实用建议总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E5%8F%A5%E8%AF%9D%E6%A0%B8%E5%BF%83"><span class="nav-number">2.3.4.</span> <span class="nav-text">一句话核心</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MLP-Mixer-An-all-MLP-Architecture-for-Vision"><span class="nav-number">2.4.</span> <span class="nav-text">MLP-Mixer: An all-MLP Architecture for Vision</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%F0%9F%A7%A0-%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="nav-number">2.4.1.</span> <span class="nav-text">🧠 核心思想</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%F0%9F%A7%A9-%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="nav-number">2.4.2.</span> <span class="nav-text">🧩 模型结构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E8%BE%93%E5%85%A5%E5%A4%84%E7%90%86"><span class="nav-number">2.4.2.1.</span> <span class="nav-text">1. 输入处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E6%A0%B8%E5%BF%83%E6%A8%A1%E5%9D%97%EF%BC%9AMixer%E5%B1%82"><span class="nav-number">2.4.2.2.</span> <span class="nav-text">2. 核心模块：Mixer层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E8%BE%85%E5%8A%A9%E7%BB%84%E4%BB%B6"><span class="nav-number">2.4.2.3.</span> <span class="nav-text">3. 辅助组件</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%9A%A1%EF%B8%8F-%E4%BC%98%E5%8A%BF%E4%B8%8E%E7%89%B9%E6%80%A7"><span class="nav-number">2.4.3.</span> <span class="nav-text">⚡️ 优势与特性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%F0%9F%93%8A-%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94"><span class="nav-number">2.4.4.</span> <span class="nav-text">📊 性能对比</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%F0%9F%8C%9F-%E6%84%8F%E4%B9%89%E4%B8%8E%E5%90%AF%E7%A4%BA"><span class="nav-number">2.4.5.</span> <span class="nav-text">🌟 意义与启示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%F0%9F%94%8D-%E5%88%9D%E5%AD%A6%E8%80%85%E7%90%86%E8%A7%A3%E8%A6%81%E7%82%B9"><span class="nav-number">2.4.6.</span> <span class="nav-text">🔍 初学者理解要点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Better-plain-ViT-baselines-for-ImageNet-1k"><span class="nav-number">2.5.</span> <span class="nav-text">Better plain ViT baselines for ImageNet-1k</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#UViM-A-Unified-Modeling-Approach-for-Vision-with-Learned-Guiding-Codes"><span class="nav-number">2.6.</span> <span class="nav-text">UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A0%94%E7%A9%B6%E8%83%8C%E6%99%AF%E4%B8%8E%E9%97%AE%E9%A2%98"><span class="nav-number">2.6.1.</span> <span class="nav-text">研究背景与问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%EF%BC%9A%E8%BF%9B%E5%8C%96%E7%AD%96%E7%95%A5%EF%BC%88ES%EF%BC%89"><span class="nav-number">2.6.2.</span> <span class="nav-text">解决方案：进化策略（ES）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%85%B3%E9%94%AE%E5%8F%91%E7%8E%B0"><span class="nav-number">2.6.3.</span> <span class="nav-text">实验设计与关键发现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%93%E8%AE%BA%E4%B8%8E%E5%B1%95%E6%9C%9B"><span class="nav-number">2.6.4.</span> <span class="nav-text">结论与展望</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E6%A6%82%E5%BF%B5%E8%A7%A3%E9%87%8A"><span class="nav-number">2.6.5.</span> <span class="nav-text">关键概念解释</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#FlexiViT-One-Model-for-All-Patch-Sizes"><span class="nav-number">2.7.</span> <span class="nav-text">FlexiViT: One Model for All Patch Sizes</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E9%97%AE%E9%A2%98-1"><span class="nav-number">2.7.1.</span> <span class="nav-text">核心问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FlexiViT-%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="nav-number">2.7.2.</span> <span class="nav-text">FlexiViT 的解决方案</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E2%9C%85-%E6%A0%B8%E5%BF%83%E6%80%9D%E8%B7%AF"><span class="nav-number">2.7.2.1.</span> <span class="nav-text">✅ 核心思路</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E2%9C%85-%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF"><span class="nav-number">2.7.2.2.</span> <span class="nav-text">✅ 关键技术</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E4%BC%98%E5%8A%BF"><span class="nav-number">2.7.3.</span> <span class="nav-text">关键优势</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E9%AA%8C%E8%AF%81"><span class="nav-number">2.7.4.</span> <span class="nav-text">实验验证</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9D%E5%AD%A6%E8%80%85%E8%A6%81%E7%82%B9"><span class="nav-number">2.7.5.</span> <span class="nav-text">初学者要点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Dual-PatchNorm"><span class="nav-number">2.8.</span> <span class="nav-text">Dual PatchNorm</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E6%A0%B8%E5%BF%83%E9%97%AE%E9%A2%98"><span class="nav-number">2.8.1.</span> <span class="nav-text">1. 核心问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%85%B3%E9%94%AE%E5%8F%91%E7%8E%B0-1"><span class="nav-number">2.8.2.</span> <span class="nav-text">2. 关键发现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="nav-number">2.8.3.</span> <span class="nav-text">3. 实验结果</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%EF%BC%88ImageNet%EF%BC%89"><span class="nav-number">2.8.3.1.</span> <span class="nav-text">图像分类（ImageNet）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1"><span class="nav-number">2.8.3.2.</span> <span class="nav-text">下游任务</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C%EF%BC%88%E5%85%B3%E9%94%AE%E9%AA%8C%E8%AF%81%EF%BC%89"><span class="nav-number">2.8.4.</span> <span class="nav-text">4. 消融实验（关键验证）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E5%88%9D%E5%AD%A6%E8%80%85%E8%A6%81%E7%82%B9"><span class="nav-number">2.8.5.</span> <span class="nav-text">5. 初学者要点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%E7%A4%BA%E6%80%BB%E7%BB%93"><span class="nav-number">2.8.6.</span> <span class="nav-text">图示总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Getting-ViT-in-Shape-Scaling-Laws-for-Compute-Optimal-Model-Design"><span class="nav-number">2.9.</span> <span class="nav-text">Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E7%9B%AE%E6%A0%87"><span class="nav-number">2.9.1.</span> <span class="nav-text">核心目标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF"><span class="nav-number">2.9.2.</span> <span class="nav-text">关键技术</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E5%88%9B%E6%96%B0%EF%BC%9AFiber"><span class="nav-number">2.9.2.1.</span> <span class="nav-text">1. 模型架构创新：Fiber</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E4%B8%A4%E9%98%B6%E6%AE%B5%E9%A2%84%E8%AE%AD%E7%BB%83%E7%AD%96%E7%95%A5"><span class="nav-number">2.9.2.2.</span> <span class="nav-text">2. 两阶段预训练策略</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%EF%BC%88%E4%BA%AE%E7%82%B9%EF%BC%89"><span class="nav-number">2.9.3.</span> <span class="nav-text">实验结果（亮点）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%87%8D%E8%A6%81%EF%BC%9F"><span class="nav-number">2.9.4.</span> <span class="nav-text">为什么重要？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9D%E5%AD%A6%E8%80%85%E8%A6%81%E7%82%B9-1"><span class="nav-number">2.9.5.</span> <span class="nav-text">初学者要点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Scaling-Vision-Transformers-to-22-Billion-Parameters"><span class="nav-number">2.10.</span> <span class="nav-text">Scaling Vision Transformers to 22 Billion Parameters</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E7%A0%94%E7%A9%B6%E7%9B%AE%E6%A0%87-1"><span class="nav-number">2.10.1.</span> <span class="nav-text">1. 研究目标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF%E5%88%9B%E6%96%B0"><span class="nav-number">2.10.2.</span> <span class="nav-text">2. 关键技术创新</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%94%B9%E8%BF%9B"><span class="nav-number">2.10.2.1.</span> <span class="nav-text">(1) 模型架构改进</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E9%AB%98%E6%95%88%E8%AE%AD%E7%BB%83%E7%AD%96%E7%95%A5"><span class="nav-number">2.10.2.2.</span> <span class="nav-text">(2) 高效训练策略</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C-1"><span class="nav-number">2.10.3.</span> <span class="nav-text">3. 实验结果</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E6%A0%B8%E5%BF%83%E6%80%A7%E8%83%BD"><span class="nav-number">2.10.3.1.</span> <span class="nav-text">(1) 核心性能</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E8%A7%84%E6%A8%A1%E5%B8%A6%E6%9D%A5%E7%9A%84%E4%BC%98%E5%8A%BF"><span class="nav-number">2.10.3.2.</span> <span class="nav-text">(2) 规模带来的优势</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E6%B3%9B%E5%8C%96"><span class="nav-number">2.10.3.3.</span> <span class="nav-text">(3) 下游任务泛化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E8%B5%84%E6%BA%90%E4%B8%8E%E6%95%B0%E6%8D%AE"><span class="nav-number">2.10.4.</span> <span class="nav-text">4. 资源与数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E6%84%8F%E4%B9%89%E4%B8%8E%E5%90%AF%E7%A4%BA"><span class="nav-number">2.10.5.</span> <span class="nav-text">5. 意义与启示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%E7%A4%BA%E6%80%BB%E7%BB%93-1"><span class="nav-number">2.10.6.</span> <span class="nav-text">图示总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Finite-Scalar-Quantization-VQ-VAE-Made-Simple"><span class="nav-number">2.11.</span> <span class="nav-text">Finite Scalar Quantization: VQ-VAE Made Simple</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E9%97%AE%E9%A2%98-2"><span class="nav-number">2.11.1.</span> <span class="nav-text">核心问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%EF%BC%9AFSQ%EF%BC%88%E6%9C%89%E9%99%90%E6%A0%87%E9%87%8F%E9%87%8F%E5%8C%96%EF%BC%89"><span class="nav-number">2.11.2.</span> <span class="nav-text">解决方案：FSQ（有限标量量化）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-1"><span class="nav-number">2.11.2.1.</span> <span class="nav-text">核心思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E4%BC%98%E5%8A%BF-1"><span class="nav-number">2.11.2.2.</span> <span class="nav-text">关键优势</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E9%AA%8C%E8%AF%81-1"><span class="nav-number">2.11.3.</span> <span class="nav-text">实验验证</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%BB%E5%8A%A1%E8%A1%A8%E7%8E%B0"><span class="nav-number">2.11.3.1.</span> <span class="nav-text">任务表现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E5%8F%91%E7%8E%B0"><span class="nav-number">2.11.3.2.</span> <span class="nav-text">核心发现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88FSQ%E6%9C%89%E6%95%88%EF%BC%9F"><span class="nav-number">2.11.4.</span> <span class="nav-text">为什么FSQ有效？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9D%E5%AD%A6%E8%80%85%E8%A6%81%E7%82%B9-2"><span class="nav-number">2.11.5.</span> <span class="nav-text">初学者要点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GIVT-Generative-Infinite-Vocabulary-Transformers"><span class="nav-number">2.12.</span> <span class="nav-text">GIVT: Generative Infinite-Vocabulary Transformers</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E9%97%AE%E9%A2%98-3"><span class="nav-number">2.12.1.</span> <span class="nav-text">核心问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E6%A6%82%E5%BF%B5"><span class="nav-number">2.12.2.</span> <span class="nav-text">关键概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%B6%E9%97%B4-vs-%E7%A9%BA%E9%97%B4%E5%86%B3%E7%AD%96%E7%9A%84%E5%9B%9B%E5%A4%A7%E5%B7%AE%E5%BC%82"><span class="nav-number">2.12.3.</span> <span class="nav-text">时间 vs. 空间决策的四大差异</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%87%8D%E8%A6%81%EF%BC%9F-1"><span class="nav-number">2.12.4.</span> <span class="nav-text">为什么重要？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%AA%E6%9D%A5%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91"><span class="nav-number">2.12.5.</span> <span class="nav-text">未来研究方向</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E5%8F%A5%E8%AF%9D%E6%80%BB%E7%BB%93"><span class="nav-number">2.12.6.</span> <span class="nav-text">一句话总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Unified-Auto-Encoding-with-Masked-Diffusion"><span class="nav-number">2.13.</span> <span class="nav-text">Unified Auto-Encoding with Masked Diffusion</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E7%A0%94%E7%A9%B6%E7%9B%AE%E6%A0%87-2"><span class="nav-number">2.13.1.</span> <span class="nav-text">1. 研究目标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E7%8E%B0%E6%9C%89%E6%96%B9%E6%B3%95%E7%9A%84%E4%B8%8D%E8%B6%B3"><span class="nav-number">2.13.2.</span> <span class="nav-text">2. 现有方法的不足</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E6%A0%B8%E5%BF%83%E5%88%9B%E6%96%B0%EF%BC%9AUMD%EF%BC%88%E7%BB%9F%E4%B8%80%E6%8E%A9%E7%A0%81%E6%89%A9%E6%95%A3%EF%BC%89"><span class="nav-number">2.13.3.</span> <span class="nav-text">3. 核心创新：UMD（统一掩码扩散）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E8%AE%BE%E8%AE%A1"><span class="nav-number">2.13.3.1.</span> <span class="nav-text">关键设计</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E5%8A%BF"><span class="nav-number">2.13.3.2.</span> <span class="nav-text">优势</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="nav-number">2.13.4.</span> <span class="nav-text">4. 实验结果</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94%EF%BC%88ImageNet-1K%EF%BC%89"><span class="nav-number">2.13.4.1.</span> <span class="nav-text">性能对比（ImageNet-1K）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E7%BB%93%E8%AE%BA"><span class="nav-number">2.13.4.2.</span> <span class="nav-text">关键结论</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E9%87%8D%E8%A6%81%E5%9B%BE%E8%A1%A8%E8%A7%A3%E6%9E%90"><span class="nav-number">2.13.5.</span> <span class="nav-text">5. 重要图表解析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-%E5%AF%B9%E5%88%9D%E5%AD%A6%E8%80%85%E7%9A%84%E5%90%AF%E7%A4%BA"><span class="nav-number">2.13.6.</span> <span class="nav-text">6. 对初学者的启示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-%E5%85%B3%E9%94%AE%E6%9C%AF%E8%AF%AD%E9%80%9F%E6%9F%A5"><span class="nav-number">2.13.7.</span> <span class="nav-text">7. 关键术语速查</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Jet-A-Modern-Transformer-Based-Normalizing-Flow"><span class="nav-number">2.14.</span> <span class="nav-text">Jet: A Modern Transformer-Based Normalizing Flow</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E8%83%8C%E6%99%AF"><span class="nav-number">2.14.1.</span> <span class="nav-text">问题背景</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%88%E5%89%8D%E5%B7%A5%E4%BD%9C"><span class="nav-number">2.14.2.</span> <span class="nav-text">先前工作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%AC%E6%96%87%E8%B4%A1%E7%8C%AE"><span class="nav-number">2.14.3.</span> <span class="nav-text">本文贡献</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E8%AF%81%E6%98%8E%E5%9B%BE%E7%A4%BA"><span class="nav-number">2.14.4.</span> <span class="nav-text">核心证明图示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%80%E6%94%BE%E9%97%AE%E9%A2%98"><span class="nav-number">2.14.5.</span> <span class="nav-text">开放问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9D%E5%AD%A6%E8%80%85%E8%A6%81%E7%82%B9-3"><span class="nav-number">2.14.6.</span> <span class="nav-text">初学者要点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#JetFormer-An-autoregressive-generative-model-of-raw-images-and-text"><span class="nav-number">2.15.</span> <span class="nav-text">JetFormer: An autoregressive generative model of raw images and text</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E9%97%AE%E9%A2%98-4"><span class="nav-number">2.15.1.</span> <span class="nav-text">核心问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E5%88%9B%E6%96%B0%EF%BC%9A%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1"><span class="nav-number">2.15.2.</span> <span class="nav-text">关键创新：接口设计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="nav-number">2.15.3.</span> <span class="nav-text">实验结果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9D%E5%AD%A6%E8%80%85%E8%A6%81%E7%82%B9-4"><span class="nav-number">2.15.4.</span> <span class="nav-text">初学者要点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%E7%A4%BA%E8%BE%85%E5%8A%A9%E7%90%86%E8%A7%A3"><span class="nav-number">2.15.5.</span> <span class="nav-text">图示辅助理解</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A4%9A%E6%A8%A1%E6%80%81%E7%A0%94%E7%A9%B6"><span class="nav-number">3.</span> <span class="nav-text">多模态研究</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#LiT-Zero-Shot-Transfer-with-Locked-image-Text-Tuning"><span class="nav-number">3.1.</span> <span class="nav-text">LiT: Zero-Shot Transfer with Locked-image Text Tuning,</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E7%A0%94%E7%A9%B6%E7%9B%AE%E6%A0%87-3"><span class="nav-number">3.1.1.</span> <span class="nav-text">1. 研究目标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%A0%B8%E5%BF%83%E6%96%B9%E6%B3%95%EF%BC%9ALiT%EF%BC%88Locked-image-Tuning%EF%BC%89"><span class="nav-number">3.1.2.</span> <span class="nav-text">2. 核心方法：LiT（Locked-image Tuning）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-2"><span class="nav-number">3.1.2.1.</span> <span class="nav-text">核心思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%89%E6%95%88%EF%BC%9F"><span class="nav-number">3.1.2.2.</span> <span class="nav-text">为什么有效？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%85%B3%E9%94%AE%E4%BC%98%E5%8A%BF"><span class="nav-number">3.1.3.</span> <span class="nav-text">3. 关键优势</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E9%87%8D%E8%A6%81%E7%BB%93%E8%AE%BA"><span class="nav-number">3.1.4.</span> <span class="nav-text">4. 重要结论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E5%AE%9E%E7%94%A8%E4%BB%B7%E5%80%BC"><span class="nav-number">3.1.5.</span> <span class="nav-text">5. 实用价值</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%B1%BB%E6%AF%94%E7%90%86%E8%A7%A3"><span class="nav-number">3.1.6.</span> <span class="nav-text">类比理解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%99%84%E5%BD%95%EF%BC%9A%E5%85%B3%E9%94%AE%E5%9B%BE%E8%A1%A8%E9%80%9F%E8%A7%88"><span class="nav-number">3.1.7.</span> <span class="nav-text">附录：关键图表速览</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CLIPPO-Image-and-Language-Understanding-from-Pixels-Only"><span class="nav-number">3.2.</span> <span class="nav-text">CLIPPO: Image-and-Language Understanding from Pixels Only</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A0%94%E7%A9%B6%E8%83%8C%E6%99%AF"><span class="nav-number">3.2.1.</span> <span class="nav-text">研究背景</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%BB%E5%8A%A1%E5%AE%9A%E4%B9%89"><span class="nav-number">3.2.2.</span> <span class="nav-text">任务定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E8%B4%A1%E7%8C%AE"><span class="nav-number">3.2.3.</span> <span class="nav-text">关键贡献</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%AA%E6%9D%A5%E6%96%B9%E5%90%91"><span class="nav-number">3.2.4.</span> <span class="nav-text">未来方向</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B5%84%E6%BA%90%E4%B8%8E%E4%BC%A6%E7%90%86"><span class="nav-number">3.2.5.</span> <span class="nav-text">资源与伦理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9D%E5%AD%A6%E8%80%85%E8%A6%81%E7%82%B9-5"><span class="nav-number">3.2.6.</span> <span class="nav-text">初学者要点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sigmoid-Loss-for-Language-Image-Pre-Training"><span class="nav-number">3.3.</span> <span class="nav-text">Sigmoid Loss for Language Image Pre-Training</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E9%97%AE%E9%A2%98-5"><span class="nav-number">3.3.1.</span> <span class="nav-text">核心问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%EF%BC%9ASigmoid-Loss"><span class="nav-number">3.3.2.</span> <span class="nav-text">解决方案：Sigmoid Loss</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E4%BC%98%E5%8A%BF-2"><span class="nav-number">3.3.2.1.</span> <span class="nav-text">关键优势</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="nav-number">3.3.3.</span> <span class="nav-text">关键实验结果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E7%94%A8%E4%BB%B7%E5%80%BC"><span class="nav-number">3.3.4.</span> <span class="nav-text">实用价值</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93%E4%B8%80%E5%8F%A5%E8%AF%9D"><span class="nav-number">3.3.5.</span> <span class="nav-text">总结一句话</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#A-Study-of-Autoregressive-Decoders-for-Multi-Tasking-in-Computer-Vision"><span class="nav-number">3.4.</span> <span class="nav-text">A Study of Autoregressive Decoders for Multi-Tasking in Computer Vision</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A0%94%E7%A9%B6%E7%9B%AE%E6%A0%87"><span class="nav-number">3.4.1.</span> <span class="nav-text">研究目标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E6%A6%82%E5%BF%B5-1"><span class="nav-number">3.4.2.</span> <span class="nav-text">关键概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E5%8F%91%E7%8E%B0-1"><span class="nav-number">3.4.3.</span> <span class="nav-text">核心发现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C-1"><span class="nav-number">3.4.4.</span> <span class="nav-text">实验结果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A0%94%E7%A9%B6%E6%84%8F%E4%B9%89"><span class="nav-number">3.4.5.</span> <span class="nav-text">研究意义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9D%E5%AD%A6%E8%80%85%E8%A6%81%E7%82%B9-6"><span class="nav-number">3.4.6.</span> <span class="nav-text">初学者要点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Image-Captioners-Are-Scalable-Vision-Learners-Too"><span class="nav-number">3.5.</span> <span class="nav-text">Image Captioners Are Scalable Vision Learners Too</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E7%9B%AE%E6%A0%87%EF%BC%9A%E7%AE%97%E6%9C%AF%E5%8C%96%E5%AF%BC%E5%90%91%EF%BC%88Arithmetization-Oriented-AO%EF%BC%89"><span class="nav-number">3.5.1.</span> <span class="nav-text">核心目标：算术化导向（Arithmetization-Oriented, AO）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Arion-%E7%9A%84%E6%A0%B8%E5%BF%83%E5%88%9B%E6%96%B0%EF%BC%9AGTDS"><span class="nav-number">3.5.2.</span> <span class="nav-text">Arion 的核心创新：GTDS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ArionHash%EF%BC%9A%E5%9F%BA%E4%BA%8E-Arion-%E7%9A%84%E5%93%88%E5%B8%8C%E5%87%BD%E6%95%B0"><span class="nav-number">3.5.3.</span> <span class="nav-text">ArionHash：基于 Arion 的哈希函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E5%85%A8%E5%88%86%E6%9E%90%EF%BC%88%E6%A0%B8%E5%BF%83%E7%BB%93%E8%AE%BA%EF%BC%89"><span class="nav-number">3.5.4.</span> <span class="nav-text">安全分析（核心结论）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%A7%E8%83%BD%EF%BC%9A%E7%A2%BE%E5%8E%8B-Poseidon"><span class="nav-number">3.5.5.</span> <span class="nav-text">性能：碾压 Poseidon</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93%E7%BB%99%E5%88%9D%E5%AD%A6%E8%80%85"><span class="nav-number">3.5.6.</span> <span class="nav-text">总结给初学者</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Three-Towers-Flexible-Contrastive-Learning-with-Pretrained-Image-Models"><span class="nav-number">3.6.</span> <span class="nav-text">Three Towers: Flexible Contrastive Learning with Pretrained Image Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E9%97%AE%E9%A2%98-6"><span class="nav-number">3.6.1.</span> <span class="nav-text">核心问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%EF%BC%9A%E4%B8%89%E5%A1%94%E6%A8%A1%E5%9E%8B%EF%BC%883T%EF%BC%89"><span class="nav-number">3.6.2.</span> <span class="nav-text">解决方案：三塔模型（3T）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1"><span class="nav-number">3.6.2.1.</span> <span class="nav-text">架构设计</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E4%BC%98%E5%8A%BF"><span class="nav-number">3.6.2.2.</span> <span class="nav-text">核心优势</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C-1"><span class="nav-number">3.6.3.</span> <span class="nav-text">关键实验结果</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E6%A3%80%E7%B4%A2%E4%BB%BB%E5%8A%A1%EF%BC%88Image-Text-Retrieval%EF%BC%89"><span class="nav-number">3.6.3.1.</span> <span class="nav-text">1. 检索任务（Image-Text Retrieval）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%EF%BC%88Few-Zero-Shot-Classification%EF%BC%89"><span class="nav-number">3.6.3.2.</span> <span class="nav-text">2. 分类任务（Few&#x2F;Zero-Shot Classification）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E9%B2%81%E6%A3%92%E6%80%A7%E9%AA%8C%E8%AF%81"><span class="nav-number">3.6.3.3.</span> <span class="nav-text">3. 鲁棒性验证</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E6%96%B0%E7%82%B9%E4%B8%8E%E6%84%8F%E4%B9%89"><span class="nav-number">3.6.4.</span> <span class="nav-text">创新点与意义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9D%E5%AD%A6%E8%80%85%E8%A6%81%E7%82%B9-7"><span class="nav-number">3.6.5.</span> <span class="nav-text">初学者要点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PaLI-A-Jointly-Scaled-Multilingual-Language-Image-Model"><span class="nav-number">3.7.</span> <span class="nav-text">PaLI: A Jointly-Scaled Multilingual Language-Image Model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#PaLI-%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-number">3.7.1.</span> <span class="nav-text">PaLI 是什么？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PaLI-%E7%9A%84%E6%A0%B8%E5%BF%83%E5%88%9B%E6%96%B0%E7%82%B9"><span class="nav-number">3.7.2.</span> <span class="nav-text">PaLI 的核心创新点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PaLI-%E5%8F%96%E5%BE%97%E4%BA%86%E4%BB%80%E4%B9%88%E6%88%90%E5%B0%B1%EF%BC%9F"><span class="nav-number">3.7.3.</span> <span class="nav-text">PaLI 取得了什么成就？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93%E7%BB%99%E5%88%9D%E5%AD%A6%E8%80%85-1"><span class="nav-number">3.7.4.</span> <span class="nav-text">总结给初学者</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PaLI-3-Vision-Language-Models-Smaller-Faster-Stronger"><span class="nav-number">3.8.</span> <span class="nav-text">PaLI-3 Vision Language Models: Smaller, Faster, Stronger</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A0%94%E7%A9%B6%E7%9B%AE%E6%A0%87-1"><span class="nav-number">3.8.1.</span> <span class="nav-text">研究目标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E5%88%9B%E6%96%B0"><span class="nav-number">3.8.2.</span> <span class="nav-text">核心创新</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E6%88%90%E6%9E%9C"><span class="nav-number">3.8.3.</span> <span class="nav-text">关键成果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%87%8D%E8%A6%81%E7%BB%93%E8%AE%BA"><span class="nav-number">3.8.4.</span> <span class="nav-text">重要结论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%B1%80%E9%99%90"><span class="nav-number">3.8.5.</span> <span class="nav-text">模型局限</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9D%E5%AD%A6%E8%80%85%E8%A6%81%E7%82%B9-8"><span class="nav-number">3.8.6.</span> <span class="nav-text">初学者要点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LocCa"><span class="nav-number">3.9.</span> <span class="nav-text">LocCa</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E7%9B%AE%E6%A0%87-1"><span class="nav-number">3.9.1.</span> <span class="nav-text">核心目标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E5%88%9B%E6%96%B0%EF%BC%9ALocCa"><span class="nav-number">3.9.2.</span> <span class="nav-text">关键创新：LocCa</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E4%BB%BB%E5%8A%A1%E8%AE%BE%E8%AE%A1"><span class="nav-number">3.9.2.1.</span> <span class="nav-text">1. 任务设计</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="nav-number">3.9.2.2.</span> <span class="nav-text">2. 模型架构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90"><span class="nav-number">3.9.2.3.</span> <span class="nav-text">3. 数据生成</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%89%E6%95%88%EF%BC%9F-1"><span class="nav-number">3.9.3.</span> <span class="nav-text">为什么有效？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C-2"><span class="nav-number">3.9.4.</span> <span class="nav-text">实验结果</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E5%8A%BF%E9%A2%86%E5%9F%9F%EF%BC%88%E6%98%BE%E8%91%97%E6%8F%90%E5%8D%87%EF%BC%89%EF%BC%9A"><span class="nav-number">3.9.4.1.</span> <span class="nav-text">优势领域（显著提升）：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BF%9D%E6%8C%81%E7%AB%9E%E4%BA%89%E5%8A%9B%E7%9A%84%E9%A2%86%E5%9F%9F%EF%BC%9A"><span class="nav-number">3.9.4.2.</span> <span class="nav-text">保持竞争力的领域：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9D%E5%AD%A6%E8%80%85%E8%A6%81%E7%82%B9-9"><span class="nav-number">3.9.5.</span> <span class="nav-text">初学者要点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B1%80%E9%99%90%E4%B8%8E%E6%9C%AA%E6%9D%A5"><span class="nav-number">3.9.6.</span> <span class="nav-text">局限与未来</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PaliGemma-PaliGemma-2"><span class="nav-number">3.10.</span> <span class="nav-text">PaliGemma, PaliGemma 2</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E7%9B%AE%E6%A0%87-2"><span class="nav-number">3.10.1.</span> <span class="nav-text">核心目标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="nav-number">3.10.2.</span> <span class="nav-text">模型架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E7%AD%96%E7%95%A5%EF%BC%88%E5%9B%9B%E9%98%B6%E6%AE%B5%EF%BC%89"><span class="nav-number">3.10.3.</span> <span class="nav-text">训练策略（四阶段）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E4%BC%98%E5%8A%BF-3"><span class="nav-number">3.10.4.</span> <span class="nav-text">关键优势</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%87%8D%E8%A6%81%E5%8F%91%E7%8E%B0-1"><span class="nav-number">3.10.5.</span> <span class="nav-text">重要发现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9D%E5%AD%A6%E8%80%85%E8%A6%81%E7%82%B9-10"><span class="nav-number">3.10.6.</span> <span class="nav-text">初学者要点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SigLIP-2-Multilingual-Vision-Language-Encoders-with-Improved-Semantic-Understanding-Localization-and-Dense-Features"><span class="nav-number">3.11.</span> <span class="nav-text">SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A0%94%E7%A9%B6%E8%83%8C%E6%99%AF-1"><span class="nav-number">3.11.1.</span> <span class="nav-text">研究背景</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E9%97%AE%E9%A2%98"><span class="nav-number">3.11.2.</span> <span class="nav-text">关键问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A0%94%E7%A9%B6%E6%96%B9%E6%B3%95"><span class="nav-number">3.11.3.</span> <span class="nav-text">研究方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BB%E8%A6%81%E5%8F%91%E7%8E%B0"><span class="nav-number">3.11.4.</span> <span class="nav-text">主要发现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%93%E8%AE%BA%E4%B8%8E%E6%84%8F%E4%B9%89"><span class="nav-number">3.11.5.</span> <span class="nav-text">结论与意义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E6%9C%AF%E8%AF%AD%E8%A7%A3%E9%87%8A"><span class="nav-number">3.11.6.</span> <span class="nav-text">关键术语解释</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83"><span class="nav-number">4.</span> <span class="nav-text">训练</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Knowledge-distillation-A-good-teacher-is-patient-and-consistent"><span class="nav-number">4.1.</span> <span class="nav-text">Knowledge distillation: A good teacher is patient and consistent</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E9%97%AE%E9%A2%98-7"><span class="nav-number">4.1.1.</span> <span class="nav-text">核心问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E5%8F%91%E7%8E%B0%EF%BC%9A%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E7%9A%84%E6%88%90%E5%8A%9F%E8%A6%81%E7%B4%A0"><span class="nav-number">4.1.2.</span> <span class="nav-text">关键发现：知识蒸馏的成功要素</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E9%AA%8C%E8%AF%81-2"><span class="nav-number">4.1.3.</span> <span class="nav-text">实验验证</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E9%87%8D%E8%A6%81%E7%BB%93%E8%AE%BA"><span class="nav-number">4.1.4.</span> <span class="nav-text">其他重要结论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%AD%A4%E5%89%8D%E6%9C%AA%E8%A2%AB%E5%8F%91%E7%8E%B0%EF%BC%9F"><span class="nav-number">4.1.5.</span> <span class="nav-text">为什么此前未被发现？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E8%B7%B5%E5%BB%BA%E8%AE%AE"><span class="nav-number">4.1.6.</span> <span class="nav-text">实践建议</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sharpness-Aware-Minimization-for-Efficiently-Improving-Generalization"><span class="nav-number">4.2.</span> <span class="nav-text">Sharpness-Aware Minimization for Efficiently Improving Generalization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E9%97%AE%E9%A2%98-8"><span class="nav-number">4.2.1.</span> <span class="nav-text">核心问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E6%80%9D%E6%83%B3%EF%BC%9ASAM%EF%BC%88Sharpness-Aware-Minimization%EF%BC%89"><span class="nav-number">4.2.2.</span> <span class="nav-text">关键思想：SAM（Sharpness-Aware Minimization）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SAM%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4%EF%BC%88%E7%AE%80%E5%8C%96%E7%89%88%EF%BC%89"><span class="nav-number">4.2.3.</span> <span class="nav-text">SAM算法步骤（简化版）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C-3"><span class="nav-number">4.2.4.</span> <span class="nav-text">实验结果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E4%BC%98%E5%8A%BF-4"><span class="nav-number">4.2.5.</span> <span class="nav-text">关键优势</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E5%9B%BE%E7%A4%BA"><span class="nav-number">4.2.6.</span> <span class="nav-text">核心概念图示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9D%E5%AD%A6%E8%80%85%E8%A6%81%E7%82%B9-11"><span class="nav-number">4.2.7.</span> <span class="nav-text">初学者要点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Surrogate-Gap-Minimization-Improves-Sharpness-Aware-Training"><span class="nav-number">4.3.</span> <span class="nav-text">Surrogate Gap Minimization Improves Sharpness-Aware Training</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E7%A0%94%E7%A9%B6%E9%97%AE%E9%A2%98"><span class="nav-number">4.3.1.</span> <span class="nav-text">一、研究问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8C%E3%80%81%E6%A0%B8%E5%BF%83%E5%88%9B%E6%96%B0"><span class="nav-number">4.3.2.</span> <span class="nav-text">二、核心创新</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%89%E3%80%81%E7%90%86%E8%AE%BA%E8%B4%A1%E7%8C%AE"><span class="nav-number">4.3.3.</span> <span class="nav-text">三、理论贡献</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9B%E3%80%81%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="nav-number">4.3.4.</span> <span class="nav-text">四、实验结果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%94%E3%80%81%E5%85%B3%E9%94%AE%E7%BB%93%E8%AE%BA"><span class="nav-number">4.3.5.</span> <span class="nav-text">五、关键结论</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Tuning-computer-vision-models-with-task-rewards"><span class="nav-number">4.4.</span> <span class="nav-text">Tuning computer vision models with task rewards</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E9%97%AE%E9%A2%98-9"><span class="nav-number">4.4.1.</span> <span class="nav-text">核心问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%EF%BC%9A%E4%BB%BB%E5%8A%A1%E5%A5%96%E5%8A%B1%E4%BC%98%E5%8C%96"><span class="nav-number">4.4.2.</span> <span class="nav-text">解决方案：任务奖励优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E6%95%88%E6%9E%9C%EF%BC%88%E5%9B%9B%E5%A4%A7%E4%BB%BB%E5%8A%A1%EF%BC%89"><span class="nav-number">4.4.3.</span> <span class="nav-text">实验效果（四大任务）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E4%BC%98%E5%8A%BF-5"><span class="nav-number">4.4.4.</span> <span class="nav-text">关键优势</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B1%80%E9%99%90%E6%80%A7%E4%B8%8E%E6%8C%91%E6%88%98"><span class="nav-number">4.4.5.</span> <span class="nav-text">局限性与挑战</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9D%E5%AD%A6%E8%80%85%E8%A6%81%E7%82%B9-12"><span class="nav-number">4.4.6.</span> <span class="nav-text">初学者要点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#VeLO-Training-Versatile-Learned-Optimizers-by-Scaling-Up"><span class="nav-number">4.5.</span> <span class="nav-text">VeLO: Training Versatile Learned Optimizers by Scaling Up</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E7%A0%94%E7%A9%B6%E7%9B%AE%E6%A0%87-4"><span class="nav-number">4.5.1.</span> <span class="nav-text">1. 研究目标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-VeLO-%E7%9A%84%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="nav-number">4.5.2.</span> <span class="nav-text">2. VeLO 的核心思想</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF"><span class="nav-number">4.5.3.</span> <span class="nav-text">3. 关键技术</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84"><span class="nav-number">4.5.3.1.</span> <span class="nav-text">(1) 网络架构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95"><span class="nav-number">4.5.3.2.</span> <span class="nav-text">(2) 训练方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E6%A0%B8%E5%BF%83%E4%BC%98%E5%8A%BF"><span class="nav-number">4.5.4.</span> <span class="nav-text">4. 核心优势</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E6%80%A7%E8%83%BD%E8%BF%9C%E8%B6%85%E4%BC%A0%E7%BB%9F%E4%BC%98%E5%8C%96%E5%99%A8"><span class="nav-number">4.5.4.1.</span> <span class="nav-text">(1) 性能远超传统优化器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E8%87%AA%E9%80%82%E5%BA%94%E7%89%B9%E6%80%A7"><span class="nav-number">4.5.4.2.</span> <span class="nav-text">(2) 自适应特性</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E5%B1%80%E9%99%90%E6%80%A7"><span class="nav-number">4.5.5.</span> <span class="nav-text">5. 局限性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-%E5%BC%80%E6%BA%90%E4%B8%8E%E5%BD%B1%E5%93%8D"><span class="nav-number">4.5.6.</span> <span class="nav-text">6. 开源与影响</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9D%E5%AD%A6%E8%80%85%E7%90%86%E8%A7%A3%E8%A6%81%E7%82%B9"><span class="nav-number">4.5.7.</span> <span class="nav-text">初学者理解要点</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%B6%E4%BB%96"><span class="nav-number">5.</span> <span class="nav-text">其他</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Are-we-done-with-ImageNet"><span class="nav-number">5.1.</span> <span class="nav-text">Are we done with ImageNet?</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A0%94%E7%A9%B6%E8%83%8C%E6%99%AF-2"><span class="nav-number">5.1.1.</span> <span class="nav-text">研究背景</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%EF%BC%9AReassessed-Labels-Real"><span class="nav-number">5.1.2.</span> <span class="nav-text">解决方案：Reassessed Labels (Real)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E5%8F%91%E7%8E%B0"><span class="nav-number">5.1.3.</span> <span class="nav-text">关键发现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%94%B9%E8%BF%9B%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95"><span class="nav-number">5.1.4.</span> <span class="nav-text">改进训练方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%93%E8%AE%BA"><span class="nav-number">5.1.5.</span> <span class="nav-text">结论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9D%E5%AD%A6%E8%80%85%E8%A6%81%E7%82%B9-13"><span class="nav-number">5.1.6.</span> <span class="nav-text">初学者要点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#No-Filter-Cultural-and-Socioeconomic-Diversity-in-Contrastive-Vision-Language-Models"><span class="nav-number">5.2.</span> <span class="nav-text">No Filter: Cultural and Socioeconomic Diversity in Contrastive Vision-Language Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A0%94%E7%A9%B6%E8%83%8C%E6%99%AF-3"><span class="nav-number">5.2.1.</span> <span class="nav-text">研究背景</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5"><span class="nav-number">5.2.2.</span> <span class="nav-text">核心概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E6%8C%91%E6%88%98"><span class="nav-number">5.2.3.</span> <span class="nav-text">关键挑战</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="nav-number">5.2.4.</span> <span class="nav-text">解决方案</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%AA%E6%9D%A5%E6%96%B9%E5%90%91-1"><span class="nav-number">5.2.5.</span> <span class="nav-text">未来方向</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9D%E5%AD%A6%E8%80%85%E8%A6%81%E7%82%B9-14"><span class="nav-number">5.2.6.</span> <span class="nav-text">初学者要点</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="lsdyun"
      src="/blog2025.github.io/images/avatar.png">
  <p class="site-author-name" itemprop="name">lsdyun</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/blog2025.github.io/archives/">
          <span class="site-state-item-count">43</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/blog2025.github.io/categories/">
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/lsdyun" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;lsdyun" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://lsdyun.github.io/blog2025.github.io/2025/08/13/Few5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog2025.github.io/images/avatar.png">
      <meta itemprop="name" content="lsdyun">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="记录博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="google-research/big_vision | 记录博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          google-research/big_vision
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-08-13 10:39:41" itemprop="dateCreated datePublished" datetime="2025-08-13T10:39:41+08:00">2025-08-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-15 12:25:27" itemprop="dateModified" datetime="2025-08-15T12:25:27+08:00">2025-08-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog2025.github.io/categories/Few-shot-learning/" itemprop="url" rel="index"><span itemprop="name">Few-shot learning</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>


		<!--  设置置顶图标  -->
		        
		<!--  设置置顶图标  -->
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="google-research-big-vision"><a href="#google-research-big-vision" class="headerlink" title="google-research&#x2F;big_vision"></a>google-research&#x2F;big_vision</h1><p><a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision">https://github.com/google-research/big_vision</a></p>
<p><strong>Big Vision</strong></p>
<p>此代码库旨在使用 Cloud TPU VM 或 GPU 机器训练大规模视觉模型。它基于 Jax&#x2F;Flax 库，并使用 tf.data 和 TensorFlow Datasets 构建可扩展且可复现的输入流水线。</p>
<p>开源此代码库主要有两个目的：</p>
<ol>
<li>发布在此代码库中开发的研究项目的代码（参见下方列表）。</li>
<li>为在 GPU 机器和 Google Cloud TPU 上运行大规模视觉实验提供一个强大的起点，它应能无缝扩展，开箱即用地支持从单个 TPU 核心到最多 2048 个 TPU 核心的分布式设置。</li>
</ol>
<p><code>big_vision</code> 旨在支持 Google 内部的研究项目。我们不太可能处理功能请求或接受外部贡献，除非它们事先获得批准（请先在 issue 中询问）。有关一个支持良好的、仅用于迁移的代码库，另请参阅 <a target="_blank" rel="noopener" href="https://github.com/google-research/vision_transformer">vision_transformer</a>。</p>
<p>请注意，<code>big_vision</code> 是一个相当动态的代码库，虽然我们打算始终保证核心代码完全可用，但我们不能保证位于 <code>.../proj/...</code> 子文件夹中的项目特定代码得到及时更新。然而，我们提供了一个表格，列出了已知项目可以工作的最后提交。</p>
<p>以下研究项目最初是在 <code>big_vision</code> 代码库中进行的：</p>
<p><strong>架构研究</strong></p>
<ul>
<li><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.11929">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></strong>, by Alexey Dosovitskiy*, Lucas Beyer*, Alexander Kolesnikov*, Dirk Weissenborn*, Xiaohua Zhai*, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby*</li>
<li><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.04560">Scaling Vision Transformers</a></strong>, by Xiaohua Zhai*, Alexander Kolesnikov*, Neil Houlsby, and Lucas Beyer*<ul>
<li>资源: <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/blob/main/big_vision/configs/vit_i21k.py">config</a></li>
</ul>
</li>
<li><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.10270">How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers</a></strong>, by Andreas Steiner*, Alexander Kolesnikov*, Xiaohua Zhai*, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer*</li>
<li><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2105.01601">MLP-Mixer: An all-MLP Architecture for Vision</a></strong>, by Ilya Tolstikhin*, Neil Houlsby*, Alexander Kolesnikov*, Lucas Beyer*, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, Alexey Dosovitskiy<ul>
<li>资源: <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/blob/main/big_vision/configs/mlp_mixer_i1k.py">config</a></li>
</ul>
</li>
<li><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.01580">Better plain ViT baselines for ImageNet-1k</a></strong>, by Lucas Beyer, Xiaohua Zhai, Alexander Kolesnikov<ul>
<li>资源: <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/blob/main/big_vision/configs/vit_s16_i1k.py">config</a></li>
</ul>
</li>
<li><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.08059">UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes</a></strong>, by Alexander Kolesnikov^, André Susano Pinto^, Lucas Beyer*, Xiaohua Zhai*, Jeremiah Harmsen*, Neil Houlsby*<ul>
<li>资源: <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/blob/main/big_vision/proj/uvim/README.md">readme</a>, <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/tree/main/big_vision/configs/proj/uvim">configs</a>, <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/blob/main/big_vision/proj/uvim/colabs">colabs</a>.</li>
</ul>
</li>
<li><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2212.08013">FlexiViT: One Model for All Patch Sizes</a></strong>, by Lucas Beyer*, Pavel Izmailov*, Alexander Kolesnikov*, Mathilde Caron*, Simon Kornblith*, Xiaohua Zhai*, Matthias Minderer*, Michael Tschannen*, Ibrahim Alabdulmohsin*, Filip Pavetic*<ul>
<li>资源: <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/blob/main/big_vision/proj/flexivit/README.md">readme</a>, <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/tree/main/big_vision/configs/proj/flexivit">configs</a>.</li>
</ul>
</li>
<li><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2302.01327">Dual PatchNorm</a></strong>, by Manoj Kumar, Mostafa Dehghani, Neil Houlsby.</li>
<li><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2206.07643">Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design</a></strong>, by Ibrahim Alabdulmohsin*, Xiaohua Zhai*, Alexander Kolesnikov, Lucas Beyer*.</li>
<li><strong>(partial)</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2302.05442">Scaling Vision Transformers to 22 Billion Parameters</a>, by Mostafa Dehghani*, Josip Djolonga*, Basil Mustafa*, Piotr Padlewski*, Jonathan Heek*, wow many middle authors, Neil Houlsby*.</li>
<li><strong>(partial)</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2309.15505">Finite Scalar Quantization: VQ-VAE Made Simple</a>, by Fabian Mentzer, David Minnen, Eirikur Agustsson, Michael Tschannen.</li>
<li><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.02187">GIVT: Generative Infinite-Vocabulary Transformers</a></strong>, by Michael Tschannen, Cian Eastwood, Fabian Mentzer.<ul>
<li>资源: <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/blob/main/big_vision/proj/givt/README.md">readme</a>, <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/givt/givt_i1k.py">config</a>, <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/blob/main/big_vision/proj/givt/colab">colab</a>.</li>
</ul>
</li>
<li><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.17688">Unified Auto-Encoding with Masked Diffusion</a></strong>, by Philippe Hansen-Estruch, Sriram Vishwanath, Amy Zhang, Manan Tomar.</li>
<li><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.06667">Jet: A Modern Transformer-Based Normalizing Flow</a></strong>, by Alexander Kolesnikov*, André Susano Pinto*, Michael Tschannen*, <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/tree/main/big_vision/configs/proj/jet">configs</a></li>
<li><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.12209">JetFormer: An autoregressive generative model of raw images and text</a></strong>, by Michael Tschannen*, André Susano Pinto*, Alexander Kolesnikov*. <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/tree/main/big_vision/configs/proj/jetformer">configs</a>.</li>
</ul>
<p><strong>多模态研究</strong></p>
<ul>
<li><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2111.07991">LiT: Zero-Shot Transfer with Locked-image Text Tuning</a></strong>, by Xiaohua Zhai*, Xiao Wang*, Basil Mustafa*, Andreas Steiner*, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer*<ul>
<li>资源: <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/blob/main/big_vision/trainers/proj/image_text/image_text.py">trainer</a>, <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/image_text/lit_i21k.py">config</a>, <a target="_blank" rel="noopener" href="https://colab.research.google.com/github/google-research/big_vision/blob/main/big_vision/proj/image_text/lit_zeroshot_eval_colab.ipynb">colab</a>.</li>
</ul>
</li>
<li><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2212.08037">CLIPPO: Image-and-Language Understanding from Pixels Only</a></strong>, by Michael Tschannen, Basil Mustafa, Neil Houlsby<ul>
<li>资源: <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/blob/main/big_vision/proj/clippo/README.md">readme</a>, <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/image_text/clippo_vit_b16.py">config</a>, <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/blob/main/big_vision/proj/clippo/colab.ipynb">colab</a>.</li>
</ul>
</li>
<li><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.15343">Sigmoid Loss for Language Image Pre-Training</a></strong>, by Xiaohua Zhai*, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer*<ul>
<li>资源: <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/blob/main/big_vision/proj/siglip/README.md">colab and models</a>, code TODO.</li>
</ul>
</li>
<li><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2304.08479">A Study of Autoregressive Decoders for Multi-Tasking in Computer Vision</a></strong>, by Lucas Beyer*, Bo Wan*, Gagan Madan*, Filip Pavetic*, Andreas Steiner*, Alexander Kolesnikov, André Susano Pinto, Emanuele Bugliarello, Xiao Wang, Qihang Yu, Liang-Chieh Chen, Xiaohua Zhai*.</li>
<li><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.04639">Image Captioners Are Scalable Vision Learners Too</a></strong>, by Michael Tschannen*, Manoj Kumar*, Andreas Steiner*, Xiaohua Zhai, Neil Houlsby, Lucas Beyer*.<ul>
<li>资源: <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/blob/main/big_vision/proj/cappa/README.md">readme</a>, <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/cappa/cappa_base.py">config</a>, <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/blob/main/big_vision/models/proj/cappa/encdec.py">model</a>.</li>
</ul>
</li>
<li><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.16999">Three Towers: Flexible Contrastive Learning with Pretrained Image Models</a></strong>, by Jannik Kossen, Mark Collier, Basil Mustafa, Xiao Wang, Xiaohua Zhai, Lucas Beyer, Andreas Steiner, Jesse Berent, Rodolphe Jenatton, Efi Kokiopoulou.</li>
<li><strong>(partial)</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2209.06794">PaLI: A Jointly-Scaled Multilingual Language-Image Model</a>, by Xi Chen, Xiao Wang, Soravit Changpinyo, wow so many middle authors, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut.</li>
<li><strong>(partial)</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.09199">PaLI-3 Vision Language Models: Smaller, Faster, Stronger</a>, by Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, Paul Voigtlaender, Basil Mustafa, Sebastian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski, Daniel Salz, Xi Xiong, Daniel Vlasic, Filip Pavetic, Keran Rong, Tianli Yu, Daniel Keysers, Xiaohua Zhai, Radu Soricut.</li>
<li><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.19596">LocCa</a></strong>, by Bo Wan, Michael Tschannen, Yongqin Xian, Filip Pavetic, Ibrahim Alabdulmohsin, Xiao Wang, André Susano Pinto, Andreas Steiner, Lucas Beyer, Xiaohua Zhai.</li>
<li><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.07726">PaliGemma</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.03555">PaliGemma 2</a></strong>, by wow many authors.<ul>
<li>资源: <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/blob/main/big_vision/proj/paligemma/README.md">readme</a>, <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/blob/main/big_vision/models/proj/paligemma/paligemma.py">model</a>, <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/tree/main/big_vision/configs/proj/paligemma">transfer configs</a>, <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/blob/main/big_vision/proj/paligemma/datasets.md">datasets</a>, <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/blob/main/big_vision/proj/paligemma/countbench.md">CountBenchQA</a>.</li>
</ul>
</li>
<li><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.18318">SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features</a></strong>, by wow many authors.<ul>
<li>资源: <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/blob/main/big_vision/proj/siglip/README.md">readme (with checkpoints)</a>, <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/blob/main/big_vision/proj/siglip/colab.ipynb">colab</a>.</li>
</ul>
</li>
</ul>
<p><strong>训练</strong></p>
<ul>
<li><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.05237">Knowledge distillation: A good teacher is patient and consistent</a></strong>, by Lucas Beyer*, Xiaohua Zhai*, Amélie Royer*, Larisa Markeeva*, Rohan Anil, and Alexander Kolesnikov*<ul>
<li>资源: <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/blob/main/big_vision/proj/distill/README.md">README</a>, <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/blob/main/big_vision/trainers/proj/distill/distill.py">trainer</a>, <a target="_blank" rel="noopener" href="https://colab.research.google.com/github/google-research/big_vision/blob/main/big_vision/proj/distill/distill_colab.ipynb">colab</a>.</li>
</ul>
</li>
<li><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.01412">Sharpness-Aware Minimization for Efficiently Improving Generalization</a></strong>, by Pierre Foret, Ariel Kleiner, Hossein Mobahi, Behnam Neyshabur</li>
<li><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.08065">Surrogate Gap Minimization Improves Sharpness-Aware Training</a></strong>, by Juntang Zhuang, Boqing Gong, Liangzhe Yuan, Yin Cui, Hartwig Adam, Nicha Dvornek, Sekhar Tatikonda, James Duncan and Ting Liu<ul>
<li>资源: <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/blob/main/big_vision/trainers/sam.py">trainer</a>, <a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/blob/main/big_vision/configs/sam.py">config</a> reproduced results</li>
</ul>
</li>
<li><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2302.08242">Tuning computer vision models with task rewards</a></strong>, by André Susano Pinto*, Alexander Kolesnikov*, Yuge Shi, Lucas Beyer, Xiaohua Zhai.</li>
<li><strong>(partial)</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2211.09760">VeLO: Training Versatile Learned Optimizers by Scaling Up</a> by Luke Metz, James Harrison, C. Daniel Freeman, Amil Merchant, Lucas Beyer, James Bradbury, Naman Agrawal, Ben Poole, Igor Mordatch, Adam Roberts, Jascha Sohl-Dickstein.</li>
</ul>
<p><strong>其他</strong></p>
<ul>
<li><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.07159">Are we done with ImageNet?</a></strong>, by Lucas Beyer*, Olivier J. Hénaff*, Alexander Kolesnikov*, Xiaohua Zhai*, Aäron van den Oord*.</li>
<li><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.14680">No Filter: Cultural and Socioeconomic Diversity in Contrastive Vision-Language Models</a></strong>, by Angéline Pouget, Lucas Beyer, Emanuele Bugliarello, Xiao Wang, Andreas Peter Steiner, Xiaohua Zhai, Ibrahim Alabdulmohsin.</li>
</ul>
<p><strong>代码库高层级组织与原则简述</strong></p>
<ul>
<li><p>主要入口点是 <code>trainer</code> 模块（通常在根目录下的 <code>train.py</code>），它处理创建模型和优化器、加载数据、保存检查点以及在循环中训练&#x2F;评估模型的所有样板代码。通常，<code>big_vision</code> 中的各个项目会分叉并定制此训练器。</p>
</li>
<li><p>所有模型、评估器和预处理操作都位于相应的子目录中，通常可以在不同项目之间复用。我们鼓励在这些目录中使用兼容的 API 以促进复用性，但并未严格执行，因为个别项目可能需要引入其自定义 API。</p>
</li>
<li><p>我们有一个强大的配置系统，配置文件位于 <code>configs/</code> 目录中。自定义训练器和模块可以直接扩展&#x2F;修改配置选项。</p>
</li>
<li><p>项目特定代码位于 <code>.../proj/...</code> 命名空间中。保持项目特定代码与核心 <code>big_vision</code> 库同步并不总是可行的。我们在下方提供了一个表格，列出了每个项目已知可工作的最后提交。</p>
</li>
<li><p>训练作业鲁棒地应对中断，并将从最后保存的检查点无缝恢复（假设用户提供了正确的 <code>--workdir</code> 路径）。</p>
</li>
<li><p>每个配置文件顶部都包含一个带有 <code>COMMAND</code> 片段的注释，用于运行它，以及一些关于预期运行时间和结果的提示。更多细节见下文，但一般来说，在 GPU 机器上运行涉及调用 <code>python -m COMMAND</code>，而在 TPU（包括多主机）上运行涉及：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gcloud compute tpus tpu-vm ssh <span class="variable">$NAME</span> --zone=<span class="variable">$ZONE</span> --worker=all</span><br><span class="line">  --<span class="built_in">command</span> <span class="string">&quot;bash big_vision/run_tpu.sh COMMAND&quot;</span></span><br></pre></td></tr></table></figure>

<p>有关如何在 GPU 机器或 Google Cloud TPU 上运行 <code>big_vision</code> 代码的更多详细信息，请参阅下文说明。</p>
</li>
<li><p>默认情况下，我们会写入检查点和日志文件。日志文件是 JSON 对象列表，我们提供了一个简短直接的 <a target="_blank" rel="noopener" href="https://colab.research.google.com/github/google-research/big_vision/blob/main/big_vision/tools/big_vision_logs.ipynb">colab 示例</a> 来读取和显示日志及检查点。</p>
</li>
</ul>
<p><strong>当前和未来内容</strong></p>
<ul>
<li>初始版本包含在 Cloud TPU VM 上大规模预训练、迁移和评估分类模型的核心部分。</li>
<li>此后我们添加了以下关键功能和项目：<ul>
<li>如 LiT 和 CLIP 中的对比式图文模型训练和评估。</li>
<li>Patient and consistent 蒸馏。</li>
<li>扩展 ViT。</li>
<li>MLP-Mixer。</li>
<li>UViM。</li>
</ul>
</li>
<li>我们计划在不久的将来发布的功能和项目（无特定顺序）：<ul>
<li>TFDS 中的 ImageNet-21k。</li>
<li>加载我们出版物中使用的其他公共模型（NFNet, MoCov3, DINO）。</li>
<li>内存高效的 Polyak 平均实现。</li>
<li>高级 JAX 计算和内存分析。我们目前使用内部工具，但最终可能会增加对公开可用工具的支持。</li>
</ul>
</li>
<li>我们将继续在此发布我们在 <code>big_vision</code> 中开发的未来出版物的代码。</li>
</ul>
<p><strong>非包含内容</strong></p>
<ul>
<li>以下存在于该代码库的内部版本中，目前没有发布计划：<ul>
<li>质量和速度的常规回归测试。它们严重依赖内部基础设施。</li>
<li>高级实验日志记录、监控和绘图。这也严重依赖内部基础设施。然而，我们对此持开放态度，未来可能会添加一些，特别是如果以自包含的方式实现。</li>
<li>尚未发表的、正在进行的研究项目。</li>
</ul>
</li>
</ul>
<p><strong>GPU 设置</strong><br>我们首先讨论如何在（本地）GPU 机器上设置和运行 <code>big_vision</code>，然后讨论 Cloud TPU 的设置。请注意，（本地）GPU 设置的数据准备步骤在很大程度上可以重用于 Cloud TPU 设置。虽然说明为了简洁而跳过了这一点，但我们强烈建议在安装 Python 依赖项时使用虚拟环境。</p>
<p><strong>设置 Python 包</strong><br>第一步是检出 <code>big_vision</code> 并安装相关的 Python 依赖项：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/google-research/big_vision</span><br><span class="line"><span class="built_in">cd</span> big_vision/</span><br><span class="line">pip3 install --upgrade pip</span><br><span class="line">pip3 install -r big_vision/requirements.txt</span><br></pre></td></tr></table></figure>

<p>最新版本的 <code>jax</code> 库可以通过以下方式获取：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install --upgrade <span class="string">&quot;jax[cuda]&quot;</span> -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html</span><br></pre></td></tr></table></figure>

<p>根据机器上安装的 CUDA 和 cuDNN 库，您可能需要不同的 jax 包。请查阅官方 <a target="_blank" rel="noopener" href="https://github.com/google/jax#installation">jax 文档</a> 获取更多信息。</p>
<p><strong>准备 tfds 数据</strong><br>为了统一且可复现地访问标准数据集，我们选择使用 tensorflow_datasets (tfds) 库。它要求每个数据集被下载、预处理，然后存储在硬盘上（或者，如果您使用“Google Cloud”，最好存储在“GCP 存储桶”中）。</p>
<p>许多数据集在首次使用时可以自动下载和预处理。尽管如此，我们特意禁用此功能，并建议在首次运行前单独进行数据集准备步骤。如果出现问题，这将使调试更容易，并且一些数据集（如 imagenet2012）需要手动下载数据。</p>
<p>大多数数据集，例如 cifar100、oxford_iiit_pet 或 imagenet_v2，可以通过运行以下命令完全自动下载和准备：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> big_vision/</span><br><span class="line">python3 -m big_vision.tools.download_tfds_datasets cifar100 oxford_iiit_pet imagenet_v2</span><br></pre></td></tr></table></figure>

<p>完整的数据集列表可在此<a target="_blank" rel="noopener" href="https://www.tensorflow.org/datasets/catalog/overview">链接</a>找到。</p>
<p>一些数据集，如 imagenet2012 或 imagenet2012_real，需要手动下载数据并放置在 <code>$TFDS_DATA_DIR/downloads/manual/</code> 下（默认为 <code>~/tensorflow_datasets/downloads/manual/</code>）。例如，对于 imagenet2012 和 imagenet2012_real，需要将官方的 <code>ILSVRC2012_img_train.tar</code> 和 <code>ILSVRC2012_img_val.tar</code> 文件放在该目录中，然后运行 <code>python3 -m big_vision.tools.download_tfds_datasets imagenet2012 imagenet2012_real</code>（这可能需要约 1 小时）。</p>
<p>如果您使用 Google Cloud，特别是 TPU，您可以将预处理后的数据（存储在 <code>$TFDS_DATA_DIR</code> 中）上传到“Google Cloud 存储桶”，然后在任何（TPU）虚拟机上使用该存储桶来访问数据。</p>
<p><strong>在 GPU 机器上运行</strong><br>最后，在安装完所有 Python 依赖项并准备好 tfds 数据后，用户可以使用他们选择的配置运行任务，例如，要在 ImageNet 数据上训练 ViT-S&#x2F;16 模型，应运行以下命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 -m big_vision.train --config big_vision/configs/vit_s16_i1k.py --workdir workdirs/`<span class="built_in">date</span> <span class="string">&#x27;+%m-%d_%H%M&#x27;</span>`</span><br></pre></td></tr></table></figure>

<p>或者训练 MLP-Mixer-B&#x2F;16，运行（注意 <code>gpu8</code> 配置参数，它减少了默认的批大小和 epoch 数）：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 -m big_vision.train --config big_vision/configs/mlp_mixer_i1k.py:gpu8 --workdir workdirs/`<span class="built_in">date</span> <span class="string">&#x27;+%m-%d_%H%M&#x27;</span>`</span><br></pre></td></tr></table></figure>

<p><strong>Cloud TPU VM 设置</strong><br><strong>创建 TPU VM</strong><br>要创建具有 8 个 TPU 核心的单台机器，请遵循以下 Cloud TPU JAX 文档：<a target="_blank" rel="noopener" href="https://cloud.google.com/tpu/docs/run-calculation-jax">https://cloud.google.com/tpu/docs/run-calculation-jax</a></p>
<p>为了支持大规模视觉研究，建议使用更多核心和多个主机。下面我们提供如何操作的说明。</p>
<p>首先，创建一些有用的变量，这些变量将被重用：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> NAME=&lt;TPU 部署的名称，例如 my-tpu-machine&gt;</span><br><span class="line"><span class="built_in">export</span> ZONE=&lt;GCP 地理区域，例如 europe-west4-a&gt;</span><br><span class="line"><span class="built_in">export</span> GS_BUCKET_NAME=&lt;存储桶的名称，例如 my_bucket&gt;</span><br></pre></td></tr></table></figure>

<p>以下命令行将创建具有 32 个核心、4 个主机的 TPU VM：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcloud compute tpus tpu-vm create <span class="variable">$NAME</span> --zone <span class="variable">$ZONE</span> --accelerator-type v3-32 --version tpu-ubuntu2204-base</span><br></pre></td></tr></table></figure>

<p><strong>在 TPU VM 上安装 big_vision</strong><br>获取 <code>big_vision</code> 仓库，将其复制到所有 TPU VM 主机，并安装依赖项。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/google-research/big_vision</span><br><span class="line">gcloud compute tpus tpu-vm scp --recurse big_vision/big_vision $NAME: --zone=$ZONE --worker=all</span><br><span class="line">gcloud compute tpus tpu-vm ssh $NAME --zone=$ZONE --worker=all --command &quot;bash big_vision/run_tpu.sh&quot;</span><br></pre></td></tr></table></figure>

<p><strong>下载并准备 TFDS 数据集</strong>bash<br>我们建议按照上述说明在本地准备 tfds 数据，然后将数据上传到 Google Cloud 存储桶。但是，如果您愿意，不需要手动下载的数据集可以使用 TPU 机器自动准备，如下所述。请注意，TPU 机器只有 100 GB 磁盘空间，并且多主机 TPU 切片不允许以写入模式挂载外部磁盘，因此以下说明可能不适用于准备大型数据集。作为另一个替代方案，我们提供了如何在仅 CPU 的 GCP 机器上准备 tfds 数据的说明。</p>
<p>具体来说，评估期间使用的七个 TFDS 数据集将通过此命令在 TPU 机器的 <code>~/tensorflow_datasets</code> 下生成：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcloud compute tpus tpu-vm ssh <span class="variable">$NAME</span> --zone=<span class="variable">$ZONE</span> --worker=0 --<span class="built_in">command</span> <span class="string">&quot;TFDS_DATA_DIR=~/tensorflow_datasets bash big_vision/run_tpu.sh big_vision.tools.download_tfds_datasets cifar10 cifar100 oxford_iiit_pet oxford_flowers102 cars196 dtd uc_merced&quot;</span></span><br></pre></td></tr></table></figure>

<p>然后您可以将数据集复制到 GS 存储桶，使所有 TPU 工作节点都可以访问它们。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcloud compute tpus tpu-vm ssh <span class="variable">$NAME</span> --zone=<span class="variable">$ZONE</span> --worker=0 --<span class="built_in">command</span> <span class="string">&quot;rm -r ~/tensorflow_datasets/downloads &amp;&amp; gsutil cp -r ~/tensorflow_datasets gs://<span class="variable">$GS_BUCKET_NAME</span>&quot;</span></span><br></pre></td></tr></table></figure>

<p>如果您想集成其他公共或自定义数据集，例如 imagenet2012，请遵循<a target="_blank" rel="noopener" href="https://www.tensorflow.org/datasets/catalog/imagenet2012">官方指南</a>。</p>
<p><strong>预训练模型</strong><br>有关预训练模型的完整列表，请查看与模型代码位于同一模块中的 <code>load</code> 函数。有关如何使用这些模型的示例配置，请参阅 <code>configs/transfer.py</code>。</p>
<p><strong>在 TPU VM 上运行迁移脚本</strong><br>以下命令行在 cifar10 数据集上微调预训练的 <code>vit-i21k-augreg-b/32</code> 模型。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcloud compute tpus tpu-vm ssh <span class="variable">$NAME</span> --zone=<span class="variable">$ZONE</span> --worker=all --<span class="built_in">command</span> <span class="string">&quot;TFDS_DATA_DIR=gs://<span class="variable">$GS_BUCKET_NAME</span>/tensorflow_datasets bash big_vision/run_tpu.sh big_vision.train --config big_vision/configs/transfer.py:model=vit-i21k-augreg-b/32,dataset=cifar10,crop=resmall_crop --workdir gs://<span class="variable">$GS_BUCKET_NAME</span>/big_vision/workdir/`date &#x27;+%m-%d_%H%M&#x27;` --config.lr=0.03&quot;</span></span><br></pre></td></tr></table></figure>

<p><strong>在 TPU VM 上运行训练脚本</strong><br>要在大型数据集（例如 imagenet2012，请先准备 TFDS 数据集）上训练您自己的 <code>big_vision</code> 模型，请运行以下命令行。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcloud compute tpus tpu-vm ssh <span class="variable">$NAME</span> --zone=<span class="variable">$ZONE</span> --worker=all --<span class="built_in">command</span> <span class="string">&quot;TFDS_DATA_DIR=gs://<span class="variable">$GS_BUCKET_NAME</span>/tensorflow_datasets bash big_vision/run_tpu.sh big_vision.train --config big_vision/configs/bit_i1k.py  --workdir gs://<span class="variable">$GS_BUCKET_NAME</span>/big_vision/workdir/`date &#x27;+%m-%d_%H%M&#x27;`&quot;</span></span><br></pre></td></tr></table></figure>

<p><strong>FSDP 训练</strong><br><code>big_vision</code> 支持灵活的参数和模型分片策略。目前，我们通过简单的配置更改支持流行的 FSDP 分片，请参阅此<a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/blob/main/big_vision/configs/transfer_fsdp.py">配置示例</a>。例如，要运行预训练 ViT-L 模型的 FSDP 微调，请运行以下命令（可能需要根据您的硬件调整批大小）：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcloud compute tpus tpu-vm ssh <span class="variable">$NAME</span> --zone=<span class="variable">$ZONE</span> --worker=all --<span class="built_in">command</span> <span class="string">&quot;TFDS_DATA_DIR=gs://<span class="variable">$GS_BUCKET_NAME</span>/tensorflow_datasets bash big_vision/run_tpu.sh big_vision.train --config big_vision/configs/transfer.py:model=vit-i21k-augreg-l/16,dataset=oxford_iiit_pet,crop=resmall_crop,fsdp=True,batch_size=256 --workdir gs://<span class="variable">$GS_BUCKET_NAME</span>/big_vision/workdir/`date &#x27;+%m-%d_%H%M&#x27;` --config.lr=0.03&quot;</span></span><br></pre></td></tr></table></figure>

<p><strong>使用 SigLIP 进行图文训练</strong><br>一个使用公共 coco captions 数据的最小示例：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcloud compute tpus tpu-vm ssh <span class="variable">$NAME</span> --zone=<span class="variable">$ZONE</span> --worker=all --<span class="built_in">command</span> <span class="string">&quot;TFDS_DATA_DIR=gs://<span class="variable">$GS_BUCKET_NAME</span>/tensorflow_datasets bash big_vision/run_tpu.sh big_vision.trainers.proj.image_text.siglip --config big_vision/configs/proj/image_text/siglip_lit_coco.py --workdir gs://<span class="variable">$GS_BUCKET_NAME</span>/big_vision/`date &#x27;+%Y-%m-%d_%H%M&#x27;`&quot;</span></span><br></pre></td></tr></table></figure>

<p><strong>有时有用的 gcloud 命令</strong></p>
<ul>
<li>销毁 TPU 机器： <code>gcloud compute tpus tpu-vm delete $NAME --zone $ZONE</code></li>
<li>删除所有主机上与 big_vision 相关的文件夹： <code>gcloud compute tpus tpu-vm ssh $NAME --zone $ZONE --worker=all --command &#39;rm -rf ~/big_vision ~/bv_venv&#39;</code></li>
</ul>
<p><strong>在独立的 GCP CPU 机器上准备 tfds 数据。</strong><br>首先创建一台新机器和一个磁盘（请随意调整确切的机器类型和磁盘设置&#x2F;容量）：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> NAME_CPU_HOST=&lt;仅 CPU 机器的名称&gt;</span><br><span class="line"><span class="built_in">export</span> NAME_DISK=&lt;磁盘的名称&gt;</span><br><span class="line">gcloud compute instances create <span class="variable">$NAME_CPU_HOST</span> --machine-type c3-standard-22 --zone <span class="variable">$ZONE</span> --image-family ubuntu-2204-lts --image-project ubuntu-os-cloud</span><br><span class="line">gcloud compute disks create <span class="variable">$NAME_DISK</span> --size 1000GB --zone <span class="variable">$ZONE</span> --<span class="built_in">type</span> pd-balanced</span><br></pre></td></tr></table></figure>

<p>现在将磁盘附加到新创建的机器：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcloud compute instances attach-disk <span class="variable">$NAME_CPU_HOST</span> --disk <span class="variable">$NAME_DISK</span> --zone <span class="variable">$ZONE</span></span><br></pre></td></tr></table></figure>

<p>接下来，ssh 到机器 <code>gcloud compute ssh $NAME_CPU_HOST --zone=$ZONE</code> 并按照说明格式化并挂载磁盘。假设它挂载到了 <code>/mnt/disks/tfds</code>。</p>
<p>快完成了，现在克隆并设置 <code>big_vision</code>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcloud compute ssh <span class="variable">$NAME_CPU_HOST</span> --zone=<span class="variable">$ZONE</span> --<span class="built_in">command</span> <span class="string">&quot;git clone https://github.com/google-research/big_vision.git &amp;&amp; cd big_vision &amp;&amp; sh big_vision/run_tpu.sh&quot;</span></span><br></pre></td></tr></table></figure>

<p>最后，使用实用程序脚本准备数据集（例如 coco_captions），并将结果复制到您的 Google Cloud 存储桶：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gcloud compute ssh <span class="variable">$NAME_CPU_HOST</span> --zone=<span class="variable">$ZONE</span> --<span class="built_in">command</span> <span class="string">&quot;cd big_vision &amp;&amp; TFDS_DATA_DIR=/mnt/disks/tfds/tensorflow_datasets bash big_vision/run_tpu.sh big_vision.tools.download_tfds_datasets coco_captions&quot;</span></span><br><span class="line">gcloud compute ssh <span class="variable">$NAME_CPU_HOST</span> --zone=<span class="variable">$ZONE</span> --<span class="built_in">command</span> <span class="string">&quot;rm -rf /mnt/disks/tfds/tensorflow_datasets/downloads &amp;&amp; gsutil cp -r /mnt/disks/tfds/tensorflow_datasets gs://<span class="variable">$GS_BUCKET_NAME</span>&quot;</span></span><br></pre></td></tr></table></figure>

<p><strong>ViT 基线</strong><br>我们在名为 <code>vit_s16_i1k.py</code> 的配置文件中提供了一个经过良好调优的 ViT-S&#x2F;16 基线。它在 ImageNet 验证集上经过 90 个 epoch 的训练达到 76.5% 的准确率，是 ViT 模型研究的强大而简单的起点。</p>
<p>更多细节请参阅我们的 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.01580">arXiv 笔记</a>，如果此基线对您的研究有用，请考虑引用：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">@article&#123;vit_baseline,</span><br><span class="line">  url = &#123;https://arxiv.org/abs/2205.01580&#125;,</span><br><span class="line">  author = &#123;Beyer, Lucas and Zhai, Xiaohua and Kolesnikov, Alexander&#125;,</span><br><span class="line">  title = &#123;Better plain ViT baselines for ImageNet-1k&#125;,</span><br><span class="line">  journal=&#123;arXiv preprint arXiv:2205.01580&#125;,</span><br><span class="line">  year = &#123;2022&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>项目特定提交</strong><br>已知项目代码预期可工作的最后提交。核心代码和配置预计在最新提交 (head) 上工作。</p>
<table>
<thead>
<tr>
<th align="left">项目</th>
<th align="left">提交</th>
</tr>
</thead>
<tbody><tr>
<td align="left">UViM</td>
<td align="left"><a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/commit/21bd6ebe253f070f584d8b777ad76f4abce51bef">https://github.com/google-research/big_vision/commit/21bd6ebe253f070f584d8b777ad76f4abce51bef</a></td>
</tr>
<tr>
<td align="left">image_text</td>
<td align="left"><a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/commit/8921d5141504390a8a4f7b2dacb3b3c042237290">https://github.com/google-research/big_vision/commit/8921d5141504390a8a4f7b2dacb3b3c042237290</a></td>
</tr>
<tr>
<td align="left">distill</td>
<td align="left"><a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/commit/2f3f493af048dbfd97555ff6060f31a0e686f17f">https://github.com/google-research/big_vision/commit/2f3f493af048dbfd97555ff6060f31a0e686f17f</a></td>
</tr>
<tr>
<td align="left">GSAM</td>
<td align="left">WIP</td>
</tr>
<tr>
<td align="left">CLIPPO</td>
<td align="left"><a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/commit/fd2d3bd2efc9d89ea959f16cd2f58ae8a495cd44">https://github.com/google-research/big_vision/commit/fd2d3bd2efc9d89ea959f16cd2f58ae8a495cd44</a></td>
</tr>
<tr>
<td align="left">CapPa</td>
<td align="left"><a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/commit/7ace659452dee4b68547575352c022a2eef587a5">https://github.com/google-research/big_vision/commit/7ace659452dee4b68547575352c022a2eef587a5</a></td>
</tr>
<tr>
<td align="left">GIVT</td>
<td align="left"><a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision/commit/0cb70881dd33b3343b769347dc19793c4994b8cb">https://github.com/google-research/big_vision/commit/0cb70881dd33b3343b769347dc19793c4994b8cb</a></td>
</tr>
</tbody></table>
<p><strong>引用代码库</strong><br>如果您发现此代码库对您的研究有用，请考虑使用以下 BibTEX 引用它：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">@misc&#123;big_vision,</span><br><span class="line">  author = &#123;Beyer, Lucas and Zhai, Xiaohua and Kolesnikov, Alexander&#125;,</span><br><span class="line">  title = &#123;Big Vision&#125;,</span><br><span class="line">  year = &#123;2022&#125;,</span><br><span class="line">  publisher = &#123;GitHub&#125;,</span><br><span class="line">  journal = &#123;GitHub repository&#125;,</span><br><span class="line">  howpublished = &#123;\url&#123;https://github.com/google-research/big_vision&#125;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>免责声明</strong><br>这不是官方的 Google 产品。</p>
<p><strong>许可证</strong><br>除非另有明确说明，<code>big_vision</code> 代码库中的所有内容（包括模型和 colab）均根据 Apache2 许可证发布。完整许可证文本请参见 LICENSE 文件。</p>
<hr>
<hr>
<h1 id="架构研究"><a href="#架构研究" class="headerlink" title="架构研究"></a>架构研究</h1><h2 id="An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale"><a href="#An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale" class="headerlink" title="An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"></a>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</h2><h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><p>传统计算机视觉依赖<strong>卷积神经网络（CNN）</strong>，但本文提出 <strong>Vision Transformer（ViT）</strong>，首次证明：</p>
<blockquote>
<p><strong>纯Transformer模型</strong>（无需CNN）可直接处理图像，并在大规模数据训练下<strong>超越CNN性能</strong>，同时<strong>大幅降低计算成本</strong>。</p>
</blockquote>
<hr>
<h3 id="关键创新点"><a href="#关键创新点" class="headerlink" title="关键创新点"></a>关键创新点</h3><ol>
<li><strong>图像分块处理</strong><ul>
<li>将图像分割为固定大小的块（如 <code>16×16</code> 像素），每个块视为一个”视觉单词”。</li>
<li>示例：<code>224×224</code> 图像 → 拆分成 <code>196</code> 个块（<code>224÷16=14</code>，<code>14×14=196</code>）。</li>
</ul>
</li>
<li><strong>类NLP的输入序列</strong><ul>
<li>线性投影每个块为向量（类似单词嵌入）。</li>
<li>添加<strong>可学习的位置编码</strong>（保留空间信息）。</li>
<li>添加<strong>额外可学习的 [class] 标记</strong>（用于最终图像分类）。</li>
</ul>
</li>
<li><strong>纯Transformer架构</strong><ul>
<li>直接使用标准Transformer编码器（与NLP相同），无需修改结构。</li>
<li>无卷积操作，仅靠自注意力机制学习全局关系。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="重要发现"><a href="#重要发现" class="headerlink" title="重要发现"></a>重要发现</h3><table>
<thead>
<tr>
<th align="left">场景</th>
<th align="left">表现</th>
<th align="left">原因分析</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>小数据集训练</strong></td>
<td align="left">ViT 弱于 ResNet（如 ImageNet 上低 2-4%）</td>
<td align="left">ViT 缺乏 CNN 的<strong>局部性偏置</strong>（平移不变性、局部感知）</td>
</tr>
<tr>
<td align="left"><strong>大数据集训练</strong></td>
<td align="left">ViT <strong>显著超越</strong> ResNet（如 JFT-300M 上训练）</td>
<td align="left">大数据<strong>弥补归纳偏置不足</strong></td>
</tr>
<tr>
<td align="left"><strong>计算效率</strong></td>
<td align="left">ViT 训练成本<strong>降低 2-4倍</strong>（相同性能下）</td>
<td align="left">Transformer 并行化优势</td>
</tr>
<tr>
<td align="left"><strong>高分辨率微调</strong></td>
<td align="left">微调时提升分辨率（如 384×384），ViT 性能进一步提升</td>
<td align="left">保持块大小，增加序列长度</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>关键结论</strong>：<br><strong>当数据量足够大（&gt;1400万图像）时，ViT 的泛化能力碾压CNN，证明“大数据训练”比“内置视觉偏置”更重要。</strong></p>
</blockquote>
<hr>
<h3 id="性能对比（大数据预训练）"><a href="#性能对比（大数据预训练）" class="headerlink" title="性能对比（大数据预训练）"></a>性能对比（大数据预训练）</h3><table>
<thead>
<tr>
<th align="left">模型</th>
<th align="left">ImageNet 精度</th>
<th align="left">训练成本（TPU days）</th>
<th align="left">优势</th>
</tr>
</thead>
<tbody><tr>
<td align="left">ViT-H&#x2F;14 (JFT)</td>
<td align="left"><strong>88.55%</strong></td>
<td align="left"><strong>2500</strong></td>
<td align="left">当前最佳，计算成本最低</td>
</tr>
<tr>
<td align="left">ResNet152x4 (BiT)</td>
<td align="left">87.54%</td>
<td align="left">9900</td>
<td align="left">传统CNN SOTA</td>
</tr>
<tr>
<td align="left">ViT-L&#x2F;16 (JFT)</td>
<td align="left">87.76%</td>
<td align="left">680</td>
<td align="left">性能接近ResNet，成本仅1&#x2F;15</td>
</tr>
</tbody></table>
<hr>
<h3 id="对初学者的启发"><a href="#对初学者的启发" class="headerlink" title="对初学者的启发"></a>对初学者的启发</h3><ol>
<li><strong>Transformer的通用性</strong>：<br>不仅限于NLP，<strong>图像可视为“块的序列”</strong>，用相同架构处理。</li>
<li><strong>大数据是关键</strong>：<br>ViT 在小数据下表现差，但<strong>大数据训练能突破模型固有偏置的限制</strong>。</li>
<li><strong>计算效率优势</strong>：<br>ViT 的并行设计更适合硬件加速，未来大模型方向明确。</li>
<li><strong>简单性的力量</strong>：<br>无需复杂结构修改，标准Transformer + 分块策略即可实现SOTA。</li>
</ol>
<blockquote>
<p>论文开源：<a target="_blank" rel="noopener" href="https://github.com/google-research/vision_transformer">ViT代码与模型</a></p>
</blockquote>
<hr>
<h3 id="图解ViT流程（简化版）"><a href="#图解ViT流程（简化版）" class="headerlink" title="图解ViT流程（简化版）"></a>图解ViT流程（简化版）</h3><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">输入图像 → 分块 → 块嵌入 → +位置编码 → [class]标记 → Transformer编码器 → [class]输出 → 分类头</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-01-26_at_9.40.31_PM_uI4jjMq.png">https://production-media.paperswithcode.com/methods/Screen_Shot_2021-01-26_at_9.40.31_PM_uI4jjMq.png</a></p>
<hr>
<p><strong>总结</strong>：ViT 证明了 Transformer 在视觉领域的强大潜力，”大数据+简单架构”可颠覆传统CNN的主导地位，为后续多模态模型（如 CLIP、DALL·E）奠定基础。</p>
<hr>
<hr>
<h2 id="Scaling-Vision-Transformer"><a href="#Scaling-Vision-Transformer" class="headerlink" title="Scaling Vision Transformer"></a>Scaling Vision Transformer</h2><h3 id="1-研究目标"><a href="#1-研究目标" class="headerlink" title="1. 研究目标"></a><strong>1. 研究目标</strong></h3><ul>
<li><strong>核心问题</strong>：探索视觉Transformer（ViT）模型在<strong>模型规模（参数量）、训练数据量、计算资源</strong>三个维度的扩展规律（Scaling Laws）。</li>
<li><strong>动机</strong>：此前缩放定律在NLP领域已被研究，但视觉领域（尤其是ViT）尚未明确。理解这些规律可指导高效设计大模型。</li>
</ul>
<hr>
<h3 id="2-关键发现"><a href="#2-关键发现" class="headerlink" title="2. 关键发现"></a><strong>2. 关键发现</strong></h3><h4 id="1-缩放定律（Scaling-Laws）"><a href="#1-缩放定律（Scaling-Laws）" class="headerlink" title="(1) 缩放定律（Scaling Laws）"></a><strong>(1) 缩放定律（Scaling Laws）</strong></h4><ul>
<li><strong>三要素需同步扩展</strong>：同时增大<strong>模型参数量、数据量、计算资源</strong>，模型性能（准确率）才能持续提升。<ul>
<li><strong>瓶颈现象</strong>：<ul>
<li><strong>小模型</strong>：参数量不足时，增加数据或计算资源无效（性能饱和）。</li>
<li><strong>大模型</strong>：数据量不足时（&lt;10亿图像），性能无法充分释放。</li>
</ul>
</li>
</ul>
</li>
<li><strong>幂律关系</strong>：性能与计算量之间呈<code>E = aC^&#123;-b&#125; + c</code>的<strong>饱和幂律关系</strong>：<ul>
<li><strong>低计算量时</strong>：小模型受限于基础性能（如随机猜测准确率）。</li>
<li><strong>高计算量时</strong>：大模型逼近任务的理论上限（如ImageNet存在精度天花板）。</li>
</ul>
</li>
</ul>
<h4 id="2-大模型的优势"><a href="#2-大模型的优势" class="headerlink" title="(2) 大模型的优势"></a><strong>(2) 大模型的优势</strong></h4><ul>
<li><strong>样本效率更高</strong>：大模型用<strong>更少的数据</strong>达到与小模型相同的性能。<ul>
<li><em>例</em>：ViT-L&#x2F;16 比 ViT-Ti&#x2F;16 的样本效率高 <strong>100倍</strong>（少样本任务）。</li>
</ul>
</li>
<li><strong>少样本学习能力强</strong>：<ul>
<li><strong>1-shot</strong>：每类仅1张示例，ImageNet准确率达 <strong>69.52%</strong>。</li>
<li><strong>10-shot</strong>：每类10张示例，准确率达 <strong>84.86%</strong>（SOTA）。</li>
</ul>
</li>
</ul>
<h4 id="3-扩展极限"><a href="#3-扩展极限" class="headerlink" title="(3) 扩展极限"></a><strong>(3) 扩展极限</strong></h4><ul>
<li>训练 <strong>20亿参数</strong>的ViT-G模型（ViT-G&#x2F;14），刷新ImageNet记录：<ul>
<li><strong>Top-1准确率</strong>：<strong>90.45%</strong>（此前SOTA为88.3%）。</li>
<li>在鲁棒性测试集（ObjectNet等）和迁移学习基准（VTAB）也达到SOTA。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="3-技术创新"><a href="#3-技术创新" class="headerlink" title="3. 技术创新"></a><strong>3. 技术创新</strong></h3><h4 id="1-训练优化"><a href="#1-训练优化" class="headerlink" title="(1) 训练优化"></a><strong>(1) 训练优化</strong></h4><ul>
<li><strong>解耦权重衰减</strong>：<ul>
<li>对<strong>分类头（head）</strong> 施加<strong>强L2正则化</strong>（权重衰减值3.0），对<strong>主干（body）</strong> 用弱正则化（0.03）。</li>
<li><strong>效果</strong>：显著提升少样本泛化能力（类似SVM的间隔最大化思想）。</li>
</ul>
</li>
<li><strong>学习率调度</strong>：<ul>
<li>采用<strong>平方根倒数衰减</strong>（<code>reciprocal square-root</code>） + <strong>冷却期（cooldown）</strong>。</li>
<li><strong>优势</strong>：单次训练可评估多阶段模型，节省计算资源。</li>
</ul>
</li>
</ul>
<h4 id="2-内存优化"><a href="#2-内存优化" class="headerlink" title="(2) 内存优化"></a><strong>(2) 内存优化</strong></h4><ul>
<li><strong>移除[class]令牌</strong>：<ul>
<li>用<strong>多头注意力池化（MAP）</strong> 或<strong>全局平均池化（GAP）</strong> 替代原ViT的[class]令牌。</li>
<li><strong>效果</strong>：减少TPU内存占用50%（避免填充对齐开销）。</li>
</ul>
</li>
<li><strong>高效优化器</strong>：<ul>
<li><strong>半精度动量Adam</strong>：存储动量用bfloat16，内存开销降至1.5倍。</li>
<li><strong>改进版Adafactor</strong>：因子化二阶动量，内存开销仅1.5倍（原Adam需2倍）。</li>
</ul>
</li>
</ul>
<h4 id="3-数据扩展"><a href="#3-数据扩展" class="headerlink" title="(3) 数据扩展"></a><strong>(3) 数据扩展</strong></h4><ul>
<li>使用 <strong>JFT-3B数据集</strong>（30亿弱标注图像）：<ul>
<li><strong>去重</strong>：移除92.7万与测试集重复的图像。</li>
<li><strong>效果</strong>：即使不增加模型规模，性能提升约1%。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="4-对初学者的启发"><a href="#4-对初学者的启发" class="headerlink" title="4. 对初学者的启发"></a><strong>4. 对初学者的启发</strong></h3><ul>
<li><strong>设计模型的原则</strong>：<ul>
<li>小模型盲目增加数据或计算是浪费，大模型需配比海量数据。</li>
<li>少样本能力依赖<strong>强正则化的分类头</strong>。</li>
</ul>
</li>
<li><strong>工程实践</strong>：<ul>
<li>内存优化是训练大模型的关键（如优化器选择、移除[class]令牌）。</li>
<li>幂律关系可预测模型性能，避免盲目试错。</li>
</ul>
</li>
<li><strong>ViT的潜力</strong>：<ul>
<li>视觉Transformer可扩展至<strong>十亿级参数</strong>，性能超越CNN（如ResNet、EfficientNet）。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="5-重要图表速览"><a href="#5-重要图表速览" class="headerlink" title="5. 重要图表速览"></a><strong>5. 重要图表速览</strong></h3><ul>
<li><strong>图2</strong>：模型规模&#x2F;数据量&#x2F;计算量对性能的影响（饱和曲线）。</li>
<li><strong>图3</strong>：大模型样本效率更高（相同数据量下误差更低）。</li>
<li><strong>表1</strong>：ViT-G&#x2F;14在多项基准测试中刷新SOTA记录。</li>
<li><strong>图8</strong>：内存优化技术显著扩展可训练模型规模（绿色&#x2F;蓝色区域）。</li>
</ul>
<blockquote>
<p><strong>提示</strong>：初学者可重点理解 <strong>“三要素同步扩展”</strong> 和 <strong>“大模型样本高效”</strong> 两个核心结论，这是论文对ViT发展的核心贡献。</p>
</blockquote>
<hr>
<hr>
<h2 id="How-to-train-your-ViT-Data-Augmentation-and-Regularization-in-Vision-Transformers"><a href="#How-to-train-your-ViT-Data-Augmentation-and-Regularization-in-Vision-Transformers" class="headerlink" title="How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers"></a>How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers</h2><h3 id="核心问题"><a href="#核心问题" class="headerlink" title="核心问题"></a>核心问题</h3><ul>
<li>ViT 模型（一种用于图像的Transformer模型）不像传统的卷积神经网络（CNN）那样有内置的“图像理解先验知识”。</li>
<li>这导致 ViT 在<strong>小数据集</strong>上训练时，很容易<strong>过拟合</strong>（就是模型只记住了训练数据，没学到通用规律，在新数据上表现差）。</li>
<li>为了避免过拟合，需要依赖<strong>数据增强 (Augmentation)</strong> 和 <strong>模型正则化 (Regularization)</strong>，合称 <strong>AugReg</strong>。</li>
<li>但问题是：<strong>数据量、AugReg强度、模型大小、训练时间（算力）</strong> 这些因素之间如何权衡？怎么搭配才能用有限的资源训练出最好的模型？</li>
</ul>
<h3 id="主要发现（对初学者最重要）"><a href="#主要发现（对初学者最重要）" class="headerlink" title="主要发现（对初学者最重要）"></a>主要发现（对初学者最重要）</h3><ol>
<li><strong>AugReg 可以 “替代” 大量数据！</strong><ul>
<li>精心选择的数据增强（如 RandAugment, Mixup）和模型正则化（如 Dropout, Stochastic Depth）<strong>效果惊人</strong>。</li>
<li>作者发现：在 ImageNet-1k (130万张图) 上使用强 AugReg 训练出的 ViT 模型，其性能可以媲美在 <strong>10倍大</strong>的 ImageNet-21k (1300万张图) 上训练（但不用强AugReg）的<strong>相同模型</strong>。</li>
<li>更进一步，在 ImageNet-21k 上用强 AugReg 并增加训练时间，其性能甚至能<strong>匹配或超过</strong>在超大规模私有数据集 JFT-300M (3亿张图！) 上训练的模型。</li>
<li><strong>结论：用好 AugReg，就能用相对较小的公开数据集训练出接近使用超大私有数据集才能达到的效果！</strong> (见图1)</li>
</ul>
</li>
<li><strong>迁移学习几乎总是更好的选择！</strong><ul>
<li>问题：如果你有一个自己的中小型数据集（比如几千或几万张图），是应该从头训练一个 ViT 模型，还是找一个在大型数据集（如 ImageNet-21k）上预训练好的 ViT 模型，然后在你的数据上微调（迁移学习）？</li>
<li>答案非常明确：<strong>优先选择迁移学习！</strong></li>
<li>原因：<ul>
<li><strong>效果更好：</strong> 即使花很多计算资源和时间从头训练（并精心调 AugReg），也很难达到迁移学习模型的性能（尤其在很小的数据集上，几乎不可能）。见图2左。</li>
<li><strong>成本更低：</strong> 预训练好的优秀模型可以免费下载（作者开源了5万多个模型！）。你只需要花费计算资源在你的小数据集上微调，这比从头训练一个强大的 ViT 模型便宜得多。</li>
<li><strong>更简单：</strong> 找到一个好的预训练模型并在其上微调，比从头开始调参（找最佳AugReg、学习率等）要容易和可靠得多。见图2右（随机搜索效果不稳定）。</li>
</ul>
</li>
</ul>
</li>
<li><strong>预训练数据越多，模型越通用！</strong><ul>
<li>在更大的数据集（如 ImageNet-21k）上预训练的模型，即使在本数据集上的精度提升不明显，但在<strong>迁移到各种不同类型的新任务</strong>（自然图像、卫星图像、几何任务等，用 VTAB 基准测试）时，表现<strong>显著更好</strong>。</li>
<li><strong>结论：</strong> 在算力允许的情况下，<strong>优先选择在更大、更多样化的数据集上预训练的模型</strong>，因为它学到的特征更通用，迁移能力更强。 (见图3)</li>
</ul>
</li>
<li><strong>AugReg 不是万能的，需谨慎使用！</strong><ul>
<li><strong>在中小数据集上：</strong> AugReg (无论是数据增强还是模型正则化) 通常<strong>很有帮助</strong>，能有效防止过拟合。</li>
<li><strong>在大型数据集上（如 ImageNet-21k）：</strong><ul>
<li>如果<strong>训练时间很短</strong>，加 AugReg <strong>反而可能损害性能</strong>（尤其是对小模型）。模型可能还没学够，就被约束住了。</li>
<li>只有当你同时<strong>增加训练时间</strong>（给模型更多机会学习）并使用<strong>足够大的模型</strong>时，AugReg 才重新变得有益。</li>
</ul>
</li>
<li><strong>数据增强 vs 模型正则化：</strong> 总的来说，<strong>数据增强</strong>比模型正则化更常带来好处。模型正则化（如 Dropout）在大型数据集上尤其需要小心使用，容易伤性能。(见图4, 图7)</li>
</ul>
</li>
<li><strong>如何选择预训练模型进行迁移？</strong><ul>
<li><strong>简单有效的方法：</strong> 直接选择在预训练任务（如 ImageNet-21k）上<strong>验证集精度最高</strong>的模型进行迁移微调。作者实验证明，这在绝大多数情况下效果非常好，与微调所有模型再选最好的成本效益比最高。</li>
<li><strong>注意坑：</strong> 如果要把在 ImageNet-21k 上预训练的模型迁移到 ImageNet-1k 任务，直接用预训练时的验证精度选模型会<strong>不准</strong>（因为数据有重叠）。这时需要用独立的测试集（如 ImageNetV2）来选模型。</li>
</ul>
</li>
<li><strong>模型设计小技巧：Patch Size 很重要！</strong><ul>
<li>对于需要<strong>快速推理</strong>（小模型）的场景，与其把模型变“瘦”（参数更少），不如<strong>增大 Patch Size</strong>（例如用 &#x2F;32 代替 &#x2F;16）。</li>
<li>增大 Patch Size 减少了模型需要处理的“图像块”数量，降低了计算量，但对模型能力的伤害比单纯减少网络宽度（参数）要小。 (见图6右)</li>
</ul>
</li>
</ol>
<h3 id="给初学者的实用建议总结"><a href="#给初学者的实用建议总结" class="headerlink" title="给初学者的实用建议总结"></a>给初学者的实用建议总结</h3><ol>
<li><strong>别从头训！</strong> 优先下载在 <strong>ImageNet-21k</strong> 这样的大数据集上预训练好的 <strong>ViT 模型</strong>（作者开源了很多）。GitHub链接在论文里。</li>
<li><strong>微调它！</strong> 在你的任务数据上，对这个预训练模型进行<strong>微调 (Fine-tuning)</strong>。这比从头训练效果好得多、快得多、便宜得多。</li>
<li><strong>选对模型：</strong> 选择预训练任务上<strong>验证精度最高</strong>的模型进行迁移（作者有推荐列表）。</li>
<li><strong>理解 AugReg：</strong> 如果你<em>必须</em>在<strong>中小型数据集</strong>上从头训练，<strong>一定要用 AugReg</strong>（数据增强如 RandAugment&#x2F;Mixup + 适度的正则化如Dropout&#x2F;Stochastic Depth），它能极大提升效果，相当于给你“变”出更多数据。</li>
<li><strong>大型数据集训练需谨慎：</strong> 如果你在非常大的数据集上训练，<strong>训练时间要足够长</strong>，并且<strong>大模型</strong>才适合用较强的 AugReg。否则 AugReg 可能帮倒忙。</li>
<li><strong>关注 Patch Size：</strong> 需要小模型时，优先考虑<strong>增大 Patch Size (如 &#x2F;32)</strong>，而不是单纯减少网络参数。</li>
</ol>
<h3 id="一句话核心"><a href="#一句话核心" class="headerlink" title="一句话核心"></a>一句话核心</h3><p><strong>用好数据增强和正则化 (AugReg)，结合迁移学习（优先用大数据集预训练模型+微调），是资源有限时训练高性能 Vision Transformer 的关键！</strong></p>
<p>这篇论文通过海量实验（训练了5万多个模型！）给出了非常实用的指导，告诉你在数据、算力、模型大小、AugReg之间如何做最优选择。对于初学者，最重要的是理解 <strong>迁移学习优先</strong> 和 <strong>AugReg 在小数据上的威力</strong> 这两个核心点。</p>
<hr>
<hr>
<h2 id="MLP-Mixer-An-all-MLP-Architecture-for-Vision"><a href="#MLP-Mixer-An-all-MLP-Architecture-for-Vision" class="headerlink" title="MLP-Mixer: An all-MLP Architecture for Vision"></a>MLP-Mixer: An all-MLP Architecture for Vision</h2><h3 id="🧠-核心思想"><a href="#🧠-核心思想" class="headerlink" title="🧠 核心思想"></a>🧠 <strong>核心思想</strong></h3><p>传统视觉模型依赖 <strong>卷积（CNN）</strong> 或 <strong>自注意力（如Transformer）</strong>，但本文提出 <strong>MLP-Mixer</strong>，仅用<strong>多层感知机（MLP）</strong> 构建视觉模型，证明了：</p>
<blockquote>
<p><strong>卷积和注意力都不是必需的！</strong><br>纯MLP也能在图像分类任务中达到接近SOTA的性能。</p>
</blockquote>
<hr>
<h3 id="🧩-模型结构"><a href="#🧩-模型结构" class="headerlink" title="🧩 模型结构"></a>🧩 <strong>模型结构</strong></h3><h4 id="1-输入处理"><a href="#1-输入处理" class="headerlink" title="1. 输入处理"></a>1. <strong>输入处理</strong></h4><ul>
<li>将图像分割为多个<strong>非重叠小块</strong>（如16×16像素），每个块展平为向量。</li>
<li>所有块通过<strong>相同线性投影</strong>转换为特征向量，形成 <code>S×C</code> 矩阵（<code>S</code>&#x3D;块数, <code>C</code>&#x3D;特征维度）。</li>
</ul>
<h4 id="2-核心模块：Mixer层"><a href="#2-核心模块：Mixer层" class="headerlink" title="2. 核心模块：Mixer层"></a>2. <strong>核心模块：Mixer层</strong></h4><p>每层包含两种交替的MLP：</p>
<ul>
<li><strong>Token-Mixing MLP</strong><ul>
<li><strong>作用</strong>：混合<strong>不同空间位置</strong>（块与块之间）的信息。</li>
<li><strong>操作</strong>：对矩阵的<strong>每一列</strong>独立应用MLP（跨空间位置）。</li>
</ul>
</li>
<li><strong>Channel-Mixing MLP</strong><ul>
<li><strong>作用</strong>：混合<strong>同一位置内不同通道</strong>的特征。</li>
<li><strong>操作</strong>：对矩阵的<strong>每一行</strong>独立应用MLP（跨特征通道）。</li>
</ul>
</li>
</ul>
<blockquote>
<p>✅ <strong>关键设计</strong>：两种MLP分离了<strong>空间交互</strong>和<strong>通道交互</strong>，替代了CNN的卷积操作。</p>
</blockquote>
<h4 id="3-辅助组件"><a href="#3-辅助组件" class="headerlink" title="3. 辅助组件"></a>3. <strong>辅助组件</strong></h4><ul>
<li><strong>残差连接</strong>（Skip-connections）</li>
<li><strong>层归一化</strong>（Layer Normalization）</li>
<li><strong>分类头</strong>：全局平均池化 + 线性分类器。</li>
</ul>
<hr>
<h3 id="⚡️-优势与特性"><a href="#⚡️-优势与特性" class="headerlink" title="⚡️ 优势与特性"></a>⚡️ <strong>优势与特性</strong></h3><ol>
<li><strong>极简实现</strong><br>仅需矩阵乘法、reshape操作和激活函数（如GELU），无需卷积或注意力机制。</li>
<li><strong>计算高效</strong><br>复杂度与图像像素数成<strong>线性关系</strong>（ViT的自注意力是二次复杂度）。</li>
<li><strong>排列不变性</strong><br>对输入块的顺序不敏感（见图4实验），与CNN的局部结构偏好不同。</li>
<li><strong>可扩展性</strong><br>在大规模数据（如JFT-300M）上预训练后，性能逼近ViT和ResNet（ImageNet <strong>87.94%</strong> top-1）。</li>
</ol>
<hr>
<h3 id="📊-性能对比"><a href="#📊-性能对比" class="headerlink" title="📊 性能对比"></a>📊 <strong>性能对比</strong></h3><table>
<thead>
<tr>
<th align="left">模型</th>
<th align="left">ImageNet精度</th>
<th align="left">推理速度 (img&#x2F;sec&#x2F;core)</th>
<th align="left">预训练成本 (TPUv3 core-days)</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>MLP-Mixer-H&#x2F;14</strong></td>
<td align="left">87.94%</td>
<td align="left"><strong>40</strong></td>
<td align="left">1.01k</td>
</tr>
<tr>
<td align="left">ViT-H&#x2F;14</td>
<td align="left">88.55%</td>
<td align="left">15</td>
<td align="left">2.30k</td>
</tr>
<tr>
<td align="left">ResNet152x4</td>
<td align="left">87.54%</td>
<td align="left">26</td>
<td align="left">9.90k</td>
</tr>
</tbody></table>
<blockquote>
<p>✅ <strong>结论</strong>：<br>MLP-Mixer在<strong>精度-计算效率权衡</strong>上媲美CNN和Transformer，且推理速度更快（比ViT快2.5倍）。</p>
</blockquote>
<hr>
<h3 id="🌟-意义与启示"><a href="#🌟-意义与启示" class="headerlink" title="🌟 意义与启示"></a>🌟 <strong>意义与启示</strong></h3><ol>
<li><strong>挑战传统</strong>：证明纯MLP结构可有效处理视觉任务。</li>
<li><strong>启发研究</strong>：鼓励探索超越CNN&#x2F;Transformer的新架构（如MLP在其他领域NLP的应用）。</li>
<li><strong>开源代码</strong>：仅20行核心逻辑（JAX&#x2F;Flax实现见附录E）。</li>
</ol>
<hr>
<h3 id="🔍-初学者理解要点"><a href="#🔍-初学者理解要点" class="headerlink" title="🔍 初学者理解要点"></a>🔍 <strong>初学者理解要点</strong></h3><ul>
<li><strong>Token-Mixing</strong> ≈ 学习<strong>块与块之间的关系</strong>（类似注意力但更简单）。</li>
<li><strong>Channel-Mixing</strong> ≈ 学习<strong>每个块内部特征的组合</strong>（类似1×1卷积）。</li>
<li><strong>核心突破</strong>：用<strong>矩阵乘法+分治策略</strong>（行列分离处理）替代复杂操作。</li>
</ul>
<blockquote>
<p>💡 <strong>一句话总结</strong>：<br>MLP-Mixer像用“乐高基础积木”搭出视觉模型，证明<strong>简单组件</strong>也能组合出强大功能！</p>
</blockquote>
<p>建议结合论文中的 <strong>图1（架构图）</strong> 和 <strong>图5（权重可视化）</strong> 直观理解。代码已开源，动手实践能加深认知！</p>
<hr>
<hr>
<h2 id="Better-plain-ViT-baselines-for-ImageNet-1k"><a href="#Better-plain-ViT-baselines-for-ImageNet-1k" class="headerlink" title="Better plain ViT baselines for ImageNet-1k"></a>Better plain ViT baselines for ImageNet-1k</h2><ol>
<li><strong>核心发现：打破一个常见误解</strong><ul>
<li>之前大家都觉得 Vision Transformer (ViT) 模型在 ImageNet-1k（一个标准的图片分类数据集）上表现好，<strong>必须</strong>用非常复杂的“技巧”（正则化技术）来防止模型学得太死板（过拟合）。</li>
<li>这篇论文的作者们发现：<strong>这其实是不对的！</strong></li>
<li>他们证明，只要对原始的 ViT 训练方法做一些<strong>非常简单的小改动</strong>，再配合<strong>标准的数据增强技术</strong>（让模型看到更多样化的图片），就能让普通的 ViT 模型在 ImageNet-1k 上表现得非常好。</li>
</ul>
</li>
<li><strong>他们做了什么改动？（简单有效的小窍门）</strong><ul>
<li>主要就是调整了几个训练设置：<ul>
<li><strong>位置编码 (Positional Embedding)：</strong> 使用了固定的 <code>sincos2d</code> 编码，而不是让模型自己去学 (<code>learned</code>)。</li>
<li><strong>特征提取方式：</strong> 使用 <strong>全局平均池化 (Global Average Pooling - GAP)</strong> 来总结图片信息，而不是使用 ViT 原始的 <code>[cls]</code> token。</li>
<li><strong>分类头 (Head)：</strong> 使用一个简单的 <strong>线性层 (Linear)</strong> 来做最终分类，而不是一个更复杂的小多层感知机 (MLP)。</li>
<li><strong>批量大小 (Batch Size)：</strong> 使用了一个相对较小的批量大小 (<code>1024</code>)。</li>
</ul>
</li>
<li><strong>数据增强：</strong> 关键！使用了标准的组合：裁剪 (<code>inception crop</code>)、水平翻转 (<code>flip_lr</code>)、<code>RandAugment</code>（随机组合多种图片变换）和 <code>MixUp</code>（混合两张图片和它们的标签）。</li>
<li><strong>训练时长：</strong> 强调训练更久（300个周期）效果更好。</li>
</ul>
</li>
<li><strong>效果有多好？（结果显著提升）</strong><ul>
<li><strong>对比原始 ViT：</strong> 改动后效果提升巨大！原始 ViT 训练 90 个周期 (epoch) 在 ImageNet 上准确率只有约 67%，他们的方法在同样时间内达到了 <strong>76.5%</strong>。</li>
<li><strong>对比经典 ResNet50：</strong> 他们的 ViT-S&#x2F;16（一个较小的 ViT 型号）训练 <strong>90 个周期</strong>（约 <strong>7 小时</strong> 在 TPUv3-8 上）就能达到和经典 ResNet50 模型差不多的 <strong>76%+</strong> 的准确率。这是最重要的成果之一。</li>
<li><strong>训练更久效果更好：</strong> 训练 <strong>300 个周期</strong>（约 <strong>1 天</strong>），准确率能进一步提升到 <strong>80%</strong>。</li>
<li><strong>效率高：</strong> 在普通的硬件（TPUv3-8）上训练速度快，效果好。</li>
</ul>
</li>
<li><strong>为什么重要？（意义）</strong><ul>
<li><strong>简化流程：</strong> 提供了一个 <strong>更简单、更容易复现</strong> 的 ViT 训练基准方法。初学者想尝试 ViT 在 ImageNet 上，不再需要找一堆复杂的技巧，用这个方法就行。</li>
<li><strong>破除迷信：</strong> 证明了 ViT 本身在中等规模数据集（如 ImageNet-1k）上，配合标准数据增强，就能有很强的性能，不一定需要特别复杂的正则化。</li>
<li><strong>实用基准：</strong> 为后续的 ViT 研究提供了一个新的、性能更强的、且依然简单的比较基准。</li>
<li><strong>代码开源：</strong> 他们使用的代码库 (<code>big_vision</code>) 是开源的，里面包含了实现这个方法的完整配置（论文附录里有示例代码），方便大家直接使用和复现结果。</li>
</ul>
</li>
<li><strong>给初学者的关键启示：</strong><ul>
<li><strong>数据增强是王道：</strong> 对于 ViT 在 ImageNet 这样的任务上，<code>RandAugment</code> + <code>MixUp</code> 这些<strong>标准数据增强技术至关重要</strong>（见表1，去掉它们效果下降很多）。</li>
<li><strong>小改动大提升：</strong> 一些看起来很小的训练设置调整（位置编码方式、池化方式、分类头结构、批量大小）对最终结果影响很大。</li>
<li><strong>ViT 也能高效训练：</strong> ViT 不再是只能在超大算力和数据上训练的神秘模型，用合适的方法，在普通硬件上也能快速训练出好模型（对标 ResNet50）。</li>
<li><strong>训练久点更好：</strong> 如果算力允许，多训练一些时间（epoch），效果会更好。</li>
</ul>
</li>
</ol>
<p><strong>总结成一句话：</strong> 这篇论文告诉大家，用一些简单的小技巧和标准的数据增强方法，就能让普通的 Vision Transformer (ViT) 在 ImageNet 图片分类任务上又快又好地训练出来，效果能媲美经典的 ResNet50，而且这个方法简单、开源、容易复现。</p>
<hr>
<hr>
<h2 id="UViM-A-Unified-Modeling-Approach-for-Vision-with-Learned-Guiding-Codes"><a href="#UViM-A-Unified-Modeling-Approach-for-Vision-with-Learned-Guiding-Codes" class="headerlink" title="UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes"></a>UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes</h2><h3 id="研究背景与问题"><a href="#研究背景与问题" class="headerlink" title="研究背景与问题"></a><strong>研究背景与问题</strong></h3><ol>
<li><strong>量子神经网络（QNN）的潜力</strong><br>量子计算有望利用量子特性（如叠加、纠缠）提升机器学习性能，但当前量子设备存在局限：<ul>
<li>噪声干扰（NISQ时代）</li>
<li>量子比特数量有限</li>
<li>梯度消失（Barren Plateaus）：参数增多时，梯度趋近于零，导致训练停滞。</li>
</ul>
</li>
<li><strong>传统训练方法的瓶颈</strong><ul>
<li><strong>参数平移规则（PSR）</strong>：计算量子层梯度需对每个参数评估代价函数两次。</li>
<li><strong>问题</strong>：参数量大时，评估次数线性增长，计算成本过高。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="解决方案：进化策略（ES）"><a href="#解决方案：进化策略（ES）" class="headerlink" title="解决方案：进化策略（ES）"></a><strong>解决方案：进化策略（ES）</strong></h3><ol>
<li><strong>核心思想</strong><br>ES是一种<strong>黑盒优化算法</strong>，通过随机采样估计梯度，避免直接计算量子层的解析梯度。<ul>
<li><strong>步骤</strong>：<ul>
<li>在参数空间随机采样（如高斯分布）。</li>
<li>用采样点评估代价函数。</li>
<li>根据函数值加权平均，估计梯度方向（公式见论文算法1）。</li>
</ul>
</li>
</ul>
</li>
<li><strong>优势</strong><ul>
<li><strong>评估次数可控</strong>：采样数 λ<em>λ</em> 可自由设定（与参数量无关）。</li>
<li><strong>并行化潜力</strong>：采样点相互独立，适合并行计算（但受当前量子设备限制）。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="实验设计与关键发现"><a href="#实验设计与关键发现" class="headerlink" title="实验设计与关键发现"></a><strong>实验设计与关键发现</strong></h3><ol>
<li><strong>模型结构</strong><ul>
<li><strong>Model 1</strong>：纯量子层（2层，5+3量子比特）。</li>
<li><strong>Model 2</strong>：经典-量子混合（经典层+量子层+经典层）。</li>
<li><strong>任务</strong>：MNIST数据集二进制分类（识别数字0和1）。</li>
</ul>
</li>
<li><strong>训练结果</strong><ul>
<li><strong>ES可行性</strong>：ES成功优化了两种模型，验证其作为PSR替代方案的潜力。</li>
<li><strong>超参数敏感性</strong>：<ul>
<li><strong>学习率 η*η*</strong>：过大（如0.01）易陷入局部极小值；过小（如0.0001）收敛慢。</li>
<li><strong>采样数 λ*λ*</strong>：增加λ<em>λ</em>提升梯度估计精度，但计算成本上升。</li>
<li><strong>模型差异</strong>：混合模型（Model 2）对超参数更敏感（图9,11）。</li>
</ul>
</li>
<li><strong>梯度消失问题</strong>：ES同样受量子比特数增加导致的梯度消失影响（图7）。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="结论与展望"><a href="#结论与展望" class="headerlink" title="结论与展望"></a><strong>结论与展望</strong></h3><ol>
<li><strong>ES的适用性</strong><ul>
<li>ES是训练量子-经典混合网络的有效工具，尤其适合参数量大的场景。</li>
<li><strong>局限</strong>：性能依赖超参数调优，且无法避免梯度消失。</li>
</ul>
</li>
<li><strong>未来方向</strong><ul>
<li>结合超参数优化（如贝叶斯优化）。</li>
<li>探索自然进化策略（Natural ES）。</li>
<li>研究逐层训练等新方法缓解梯度消失。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="关键概念解释"><a href="#关键概念解释" class="headerlink" title="关键概念解释"></a><strong>关键概念解释</strong></h3><table>
<thead>
<tr>
<th align="left"><strong>术语</strong></th>
<th align="left"><strong>解释</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>NISQ时代</strong></td>
<td align="left">当前量子设备的”嘈杂中等规模量子”阶段，受限于噪声和量子比特数。</td>
</tr>
<tr>
<td align="left"><strong>梯度消失</strong></td>
<td align="left">量子比特数或层数增加时，梯度趋近于零，导致优化停滞。</td>
</tr>
<tr>
<td align="left"><strong>参数平移规则</strong></td>
<td align="left">传统量子梯度计算方法，评估次数随参数量线性增长。</td>
</tr>
<tr>
<td align="left"><strong>黑盒优化</strong></td>
<td align="left">无需知道系统内部结构，仅通过输入&#x2F;输出优化参数（ES属于此类）。</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>初学者提示</strong>：可将ES理解为一种”智能随机搜索”——通过多次试探性评估，找到代价函数下降的方向，避免直接计算复杂梯度。</p>
</blockquote>
<p>论文代码基于<strong>Qiskit（量子）</strong> 和 <strong>PyTorch（经典）</strong> 实现，开源地址可向作者索取。</p>
<hr>
<hr>
<h2 id="FlexiViT-One-Model-for-All-Patch-Sizes"><a href="#FlexiViT-One-Model-for-All-Patch-Sizes" class="headerlink" title="FlexiViT: One Model for All Patch Sizes"></a>FlexiViT: One Model for All Patch Sizes</h2><h3 id="核心问题-1"><a href="#核心问题-1" class="headerlink" title="核心问题"></a><strong>核心问题</strong></h3><p>Vision Transformer (ViT) 将图像分割为固定大小的**图像块（Patch）**进行处理。传统方法中：</p>
<ul>
<li><strong>小图像块</strong>（如 8×8）→ 计算量大、精度高</li>
<li><strong>大图像块</strong>（如 32×32）→ 计算量小、精度低<br>但切换图像块大小时需<strong>重新训练模型</strong>，效率低下。</li>
</ul>
<hr>
<h3 id="FlexiViT-的解决方案"><a href="#FlexiViT-的解决方案" class="headerlink" title="FlexiViT 的解决方案"></a><strong>FlexiViT 的解决方案</strong></h3><h4 id="✅-核心思路"><a href="#✅-核心思路" class="headerlink" title="✅ 核心思路"></a>✅ 核心思路</h4><p>训练时<strong>随机采样不同图像块大小</strong>，使单个模型适配多种尺寸，无需重新训练。<br><a target="_blank" rel="noopener" href="https://example.com/flexivit-patch-sampling.png">https://example.com/flexivit-patch-sampling.png</a><br><em>（图：训练时随机选择不同 Patch 尺寸）</em></p>
<h4 id="✅-关键技术"><a href="#✅-关键技术" class="headerlink" title="✅ 关键技术"></a>✅ 关键技术</h4><ol>
<li><strong>PI-Resize</strong><br>自适应调整位置编码和图像块嵌入权重，解决不同尺寸的兼容性问题（公式推导见原文第4页）。</li>
<li><strong>知识蒸馏</strong><br>用预训练 ViT 作为教师模型初始化 FlexiViT，提升训练效率（损失函数见公式5）。</li>
</ol>
<hr>
<h3 id="关键优势"><a href="#关键优势" class="headerlink" title="关键优势"></a><strong>关键优势</strong></h3><ol>
<li><strong>性能无损</strong><ul>
<li>在分类、分割、检索等任务中，匹配或超越单尺寸 ViT（图7）。</li>
</ul>
</li>
<li><strong>资源高效</strong><ul>
<li><strong>廉价微调</strong>：用大图像块微调（省资源），部署时用小图像块推理（高性能）（图8）。</li>
<li><strong>加速预训练</strong>：课程学习先大后小图像块，减少 20% 训练时间（图12）。</li>
</ul>
</li>
<li><strong>即插即用</strong><br>兼容现有 ViT 架构（如 LiT、CLIP），无需修改模型（图10）。</li>
</ol>
<hr>
<h3 id="实验验证"><a href="#实验验证" class="headerlink" title="实验验证"></a><strong>实验验证</strong></h3><table>
<thead>
<tr>
<th align="left">任务</th>
<th align="left">结果</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>图像分类</strong></td>
<td align="left">在 ImageNet 上，FlexiViT-B&#x2F;8 精度 85.1%，接近 ViT-B&#x2F;8 (85.6%)（表1）</td>
</tr>
<tr>
<td align="left"><strong>开放词汇检测</strong></td>
<td align="left">OWL-ViT + FlexiViT 在 LVIS 上 AP 提升 0.6%（图11）</td>
</tr>
<tr>
<td align="left"><strong>语义分割</strong></td>
<td align="left">在 Cityscapes 上 mIoU 达 70.0%，优于固定 ViT（表7）</td>
</tr>
</tbody></table>
<hr>
<h3 id="初学者要点"><a href="#初学者要点" class="headerlink" title="初学者要点"></a><strong>初学者要点</strong></h3><ol>
<li><strong>为何重要？</strong><br>解决 ViT 部署时计算资源动态调整的痛点，一模型多用途。</li>
<li><strong>如何实现？</strong><br>训练时随机 Patch 大小 + PI-Resize 兼容不同尺寸。</li>
<li><strong>哪里用？</strong><br>适用于需平衡计算资源与精度的场景（如移动端&#x2F;云端部署）。</li>
</ol>
<blockquote>
<p><strong>论文资源</strong>：</p>
<ul>
<li>代码：<a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision">GitHub</a></li>
<li>核心图示：图3（标准 ViT vs FlexiViT）、图8（资源高效微调）</li>
</ul>
</blockquote>
<hr>
<p>✅ <strong>一句话总结</strong>：<br>FlexiViT 通过 <strong>“训练时随机 Patch 大小 + 自适应权重调整”</strong>，实现单一模型动态适配不同计算需求，突破 ViT 的灵活性瓶颈。</p>
<hr>
<hr>
<h2 id="Dual-PatchNorm"><a href="#Dual-PatchNorm" class="headerlink" title="Dual PatchNorm"></a>Dual PatchNorm</h2><h3 id="1-核心问题"><a href="#1-核心问题" class="headerlink" title="1. 核心问题"></a><strong>1. 核心问题</strong></h3><ul>
<li><strong>背景</strong>：Vision Transformers (ViT) 依赖 LayerNorm（层归一化）稳定训练，但传统方法只在 Transformer 块内部使用 LayerNorm（如 self-attention 前）。</li>
<li><strong>疑问</strong>：能否通过调整 LayerNorm 的位置提升 ViT 性能？例如在 patch embedding 层（将图像分割为小块的核心层）附近添加 LayerNorm。</li>
</ul>
<hr>
<h3 id="2-关键发现-1"><a href="#2-关键发现-1" class="headerlink" title="2. 关键发现"></a><strong>2. 关键发现</strong></h3><ul>
<li><p><strong>Transformer 块内调整无效</strong>：<br>尝试在 self-attention 和 MLP 层前后不同位置添加 LayerNorm（共 9 种组合），<strong>均未显著提升性能</strong>（图 1）。</p>
</li>
<li><p><strong>突破性方案：Dual PatchNorm (DPN)</strong></p>
<ul>
<li><p><strong>方法</strong>：在 patch embedding 层 <strong>前</strong> 和 <strong>后</strong> 各加一个 LayerNorm（图 2）。</p>
<p>python</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 原始：x = PatchEmbed(x)</span><br><span class="line"># DPN：x = LayerNorm(PatchEmbed(LayerNorm(x)))</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>效果</strong>：显著提升 ViT 的准确率，且 <strong>不增加计算开销</strong>。</p>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="3-实验结果"><a href="#3-实验结果" class="headerlink" title="3. 实验结果"></a><strong>3. 实验结果</strong></h3><h4 id="图像分类（ImageNet）"><a href="#图像分类（ImageNet）" class="headerlink" title="图像分类（ImageNet）"></a><strong>图像分类（ImageNet）</strong></h4><ul>
<li><strong>提升明显</strong>：在 5 种 ViT 架构上平均提升 <strong>+1.4%</strong> 准确率（表 1）。<ul>
<li>最大增益：ViT-S&#x2F;32 模型 <strong>+1.9%</strong></li>
<li>大模型（如 ViT-B&#x2F;16）<strong>+0.7%</strong></li>
</ul>
</li>
<li><strong>其他场景有效</strong>：<ul>
<li>大数据集（ImageNet-21k、JFT）：短训练周期下平均 <strong>+1.7%</strong>。</li>
<li>高分辨率微调：<strong>+0.6~1.0%</strong>。</li>
</ul>
</li>
</ul>
<h4 id="下游任务"><a href="#下游任务" class="headerlink" title="下游任务"></a><strong>下游任务</strong></h4><ul>
<li><strong>VTAB 微调</strong>（19 个数据集）：<ul>
<li>自然图像任务（7 个）显著提升（如 ViT-B&#x2F;32 <strong>+4.2%</strong>）。</li>
<li>其他任务不降低性能（表 3）。</li>
</ul>
</li>
<li><strong>语义分割（ADE20K）</strong>：<ul>
<li>所有数据量下均提升 IoU（表 5），如全量数据 <strong>+0.5%</strong>。</li>
</ul>
</li>
<li><strong>对比学习（LiT）</strong>：<ul>
<li>零样本 ImageNet 准确率最高 <strong>+1.1%</strong>（表 4）。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="4-消融实验（关键验证）"><a href="#4-消融实验（关键验证）" class="headerlink" title="4. 消融实验（关键验证）"></a><strong>4. 消融实验（关键验证）</strong></h3><ul>
<li><strong>DPN 必须同时作用于 embedding 层前后</strong>（表 6）：<ul>
<li>仅加在前（Pre）或后（Post）会降低性能。</li>
<li>移除可学习参数（<code>γ</code>, <code>β</code>）或改用 RMSNorm 也损害性能。</li>
</ul>
</li>
<li><strong>作用机制</strong>：<ul>
<li><strong>稳定训练</strong>：DPN 显著降低 embedding 层的梯度幅值（图 2）。</li>
<li><strong>可视化</strong>：第一个 LayerNorm 的缩放参数（<code>γ</code>）加强图像块中心和角落的权重（图 3）。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="5-初学者要点"><a href="#5-初学者要点" class="headerlink" title="5. 初学者要点"></a><strong>5. 初学者要点</strong></h3><ul>
<li><strong>DPN 是什么？</strong><br>在 ViT 的 <strong>图像分块层（patch embedding）前后各加一个 LayerNorm</strong>。</li>
<li><strong>为什么简单却有效？</strong><ul>
<li>规范输入数据的分布，提升训练稳定性。</li>
<li>引导模型关注图像块的关键区域（如中心和角落）。</li>
</ul>
</li>
<li><strong>如何应用？</strong><br>可直接插入现有 ViT 代码（<a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision">参考附录 B</a>），无需调整超参数。</li>
<li><strong>核心结论</strong>：<br>DPN 是一种 <strong>零成本、通用性强</strong> 的改进，适用于分类、分割、对比学习等任务，<strong>永不损害性能</strong>。</li>
</ul>
<hr>
<h3 id="图示总结"><a href="#图示总结" class="headerlink" title="图示总结"></a><strong>图示总结</strong></h3><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">传统 ViT: </span><br><span class="line">  图像 → Patch Embedding → [Transformer 块（含 LayerNorm）] → 输出</span><br><span class="line"></span><br><span class="line">DPN ViT: </span><br><span class="line">  图像 → LayerNorm → Patch Embedding → LayerNorm → [Transformer 块] → 输出</span><br></pre></td></tr></table></figure>

<hr>
<hr>
<h2 id="Getting-ViT-in-Shape-Scaling-Laws-for-Compute-Optimal-Model-Design"><a href="#Getting-ViT-in-Shape-Scaling-Laws-for-Compute-Optimal-Model-Design" class="headerlink" title="Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design"></a>Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design</h2><h3 id="核心目标"><a href="#核心目标" class="headerlink" title="核心目标"></a><strong>核心目标</strong></h3><p>提出统一框架 <strong>Fiber</strong>（<strong>F</strong>usion-<strong>I</strong>n-the-<strong>B</strong>ackbone transform<strong>ER</strong>），同时解决两类任务：</p>
<ol>
<li><strong>图像级任务</strong>：视觉问答（VQA）、图像描述、图文检索（理解整体内容）。</li>
<li><strong>区域级任务</strong>：短语定位（Phrase Grounding）、目标检测（OD）、指代理解（REC）（需精确定位物体位置）。</li>
</ol>
<hr>
<h3 id="关键技术"><a href="#关键技术" class="headerlink" title="关键技术"></a><strong>关键技术</strong></h3><h4 id="1-模型架构创新：Fiber"><a href="#1-模型架构创新：Fiber" class="headerlink" title="1. 模型架构创新：Fiber"></a>1. <strong>模型架构创新：Fiber</strong></h4><ul>
<li><strong>问题</strong>：传统模型将多模态融合层堆叠在单模态编码器顶部，无法兼顾高效性和任务多样性。</li>
<li><strong>解决方案</strong>：<ul>
<li>将<strong>跨模态注意力模块</strong>直接插入图像（Swin Transformer）和文本（RoBERTa）主干网络中（而非顶部）。</li>
<li><strong>门控机制</strong>：通过可学习参数 <code>α</code> 控制融合强度（初始为0，逐步激活）。</li>
</ul>
</li>
<li><strong>优势</strong>：<ul>
<li><strong>灵活切换模式</strong>：开启跨注意力 → 融合编码器（VQA&#x2F;描述）；关闭 → 双编码器（快速检索）。</li>
<li><strong>高效省内存</strong>：参数增量仅26M（Base模型），比METER（110M）减少76%融合计算量。</li>
</ul>
</li>
</ul>
<h4 id="2-两阶段预训练策略"><a href="#2-两阶段预训练策略" class="headerlink" title="2. 两阶段预训练策略"></a>2. <strong>两阶段预训练策略</strong></h4><ul>
<li><strong>粗粒度预训练</strong>：<ul>
<li><strong>输入</strong>：低分辨率图像（384×384）。</li>
<li><strong>数据</strong>：400万图文对（无框标注）。</li>
<li><strong>任务</strong>：图文对比（ITC）、图文匹配（ITM）、掩码语言建模（MLM）。</li>
</ul>
</li>
<li><strong>细粒度预训练</strong>：<ul>
<li><strong>输入</strong>：高分辨率图像（800×1333）。</li>
<li><strong>数据</strong>：80万图文框数据（带物体框标注）。</li>
<li><strong>任务</strong>：边界框定位损失 + 文本-区域对齐损失。</li>
</ul>
</li>
<li><strong>关键思想</strong>：粗阶段提供通用表征，细阶段复用参数，减少对昂贵框标注数据的依赖。</li>
</ul>
<hr>
<h3 id="实验结果（亮点）"><a href="#实验结果（亮点）" class="headerlink" title="实验结果（亮点）"></a><strong>实验结果（亮点）</strong></h3><table>
<thead>
<tr>
<th align="left">任务类型</th>
<th align="left">数据集</th>
<th align="left">关键结果</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>图像级任务</strong></td>
<td align="left">VQAv2</td>
<td align="left">78.55%（4M数据），优于BLIP（129M数据）和SimVLM（1.8B数据）</td>
</tr>
<tr>
<td align="left">图文检索</td>
<td align="left">COCO</td>
<td align="left">双编码器比融合编码器快2500倍；融合重排名后TR@1达<strong>80.10%</strong>（SOTA）</td>
</tr>
<tr>
<td align="left">图像描述</td>
<td align="left">COCO</td>
<td align="left">CIDEr <strong>144.4</strong>（Base模型最高）</td>
</tr>
<tr>
<td align="left"><strong>区域级任务</strong></td>
<td align="left">Flickr30k</td>
<td align="left">短语定位R@1 <strong>87.4%</strong>，优于GLIP-L（多用25倍框数据）</td>
</tr>
<tr>
<td align="left">目标检测</td>
<td align="left">LVIS（稀有物体）</td>
<td align="left">APᵣ <strong>35.8</strong>，优于GLIP-L（更大模型+25倍数据）</td>
</tr>
<tr>
<td align="left">少样本检测</td>
<td align="left">ODinW-13数据集</td>
<td align="left">平均AP <strong>65.9</strong>，数据效率显著优于GLIP</td>
</tr>
</tbody></table>
<hr>
<h3 id="为什么重要？"><a href="#为什么重要？" class="headerlink" title="为什么重要？"></a><strong>为什么重要？</strong></h3><ol>
<li><strong>统一性</strong>：首个支持图文检索（高效）、VQA、描述、定位、检测的端到端模型。</li>
<li><strong>高效性</strong>：融合模块计算量减半，高分辨率检测训练提速1.5倍（表2）。</li>
<li><strong>数据高效</strong>：粗粒度预训练迁移性强，细粒度阶段仅需3%框数据即超越GLIP-L。</li>
</ol>
<hr>
<h3 id="初学者要点-1"><a href="#初学者要点-1" class="headerlink" title="初学者要点"></a><strong>初学者要点</strong></h3><ul>
<li><strong>跨模态融合</strong>：让图像和文本在主干网络深层交互（非仅顶层），提升细粒度理解。</li>
<li><strong>门控机制</strong>：像“开关”控制图文交互强度，平衡单&#x2F;多模态任务需求。</li>
<li><strong>粗→细预训练</strong>：先学整体语义（低分辨率），再学物体定位（高分辨率），模仿人类认知。</li>
<li><strong>核心创新</strong>：用同一模型+两阶段训练，解决多粒度任务，减少标注成本。</li>
</ul>
<blockquote>
<p><strong>论文链接</strong>：<a target="_blank" rel="noopener" href="https://github.com/microsoft/FIBER">FIBER GitHub</a><br><strong>图示辅助</strong>：图1（训练流程）、图2（模型架构）、图3（任务适配）对理解至关重要。</p>
</blockquote>
<hr>
<hr>
<h2 id="Scaling-Vision-Transformers-to-22-Billion-Parameters"><a href="#Scaling-Vision-Transformers-to-22-Billion-Parameters" class="headerlink" title="Scaling Vision Transformers to 22 Billion Parameters"></a>Scaling Vision Transformers to 22 Billion Parameters</h2><h3 id="1-研究目标-1"><a href="#1-研究目标-1" class="headerlink" title="1. 研究目标"></a><strong>1. 研究目标</strong></h3><ul>
<li><strong>问题</strong>：语言模型（如GPT-3）已成功扩展到千亿参数，但视觉模型（ViT）此前最大仅40亿参数，规模受限。</li>
<li><strong>目标</strong>：首次将视觉Transformer（ViT）扩展到<strong>220亿参数</strong>（ViT-22B），探索大规模视觉模型的潜力。</li>
</ul>
<hr>
<h3 id="2-关键技术创新"><a href="#2-关键技术创新" class="headerlink" title="2. 关键技术创新"></a><strong>2. 关键技术创新</strong></h3><h4 id="1-模型架构改进"><a href="#1-模型架构改进" class="headerlink" title="(1) 模型架构改进"></a><strong>(1) 模型架构改进</strong></h4><ul>
<li><strong>并行层（Parallel Layers）</strong>：<br>将Transformer中的注意力（Attention）和多层感知机（MLP）模块并行计算（而非串行），提升训练效率。</li>
<li><strong>QK归一化（QK Normalization）</strong>：<br>对注意力模块的Query&#x2F;Key进行层归一化，解决训练不稳定问题（防止注意力分数过大导致梯度爆炸）。</li>
<li><strong>移除偏置项（Omitting Biases）</strong>：<br>删除QKV投影和LayerNorm中的偏置项，提升硬件利用率3%且不影响性能。</li>
</ul>
<h4 id="2-高效训练策略"><a href="#2-高效训练策略" class="headerlink" title="(2) 高效训练策略"></a><strong>(2) 高效训练策略</strong></h4><ul>
<li><strong>异步并行计算</strong>：<br>重叠计算与通信（见图3），最大化TPU利用率。</li>
<li><strong>参数分片（Parameter Sharding）</strong>：<br>将模型参数分散到多个TPU芯片，支持超大规模模型训练。</li>
<li><strong>性能指标</strong>：<br>训练效率达<strong>54.9% MFU</strong>（模型浮点利用率），高于语言模型PaLM（46.2%）。</li>
</ul>
<hr>
<h3 id="3-实验结果-1"><a href="#3-实验结果-1" class="headerlink" title="3. 实验结果"></a><strong>3. 实验结果</strong></h3><h4 id="1-核心性能"><a href="#1-核心性能" class="headerlink" title="(1) 核心性能"></a><strong>(1) 核心性能</strong></h4><ul>
<li><strong>ImageNet分类</strong>（冻结特征 + 线性层）：<br><strong>89.51%</strong> 准确率，超越此前最佳模型（ViT-e&#x2F;14的89.26%）。</li>
<li><strong>零样本分类</strong>（LiT文本对齐）：<br><strong>85.9%</strong> 准确率，优于CLIP、ALIGN等模型。</li>
<li><strong>少样本学习</strong>：<br>仅用1&#x2F;16标注数据，语义分割性能超小模型+8.6 mIoU。</li>
</ul>
<h4 id="2-规模带来的优势"><a href="#2-规模带来的优势" class="headerlink" title="(2) 规模带来的优势"></a><strong>(2) 规模带来的优势</strong></h4><ul>
<li><strong>鲁棒性提升</strong>：<br>在分布外（OOD）数据（如ObjectNet）上表现更好（图5）。</li>
<li><strong>人类感知对齐</strong>：<br><strong>形状偏置达87%</strong>（此前模型仅20-30%，人类为96%），更接近人类视觉决策（图8）。</li>
<li><strong>公平性优化</strong>：<br>大模型减少性别偏见误差，不同群体性能差异更小（图7）。</li>
</ul>
<h4 id="3-下游任务泛化"><a href="#3-下游任务泛化" class="headerlink" title="(3) 下游任务泛化"></a><strong>(3) 下游任务泛化</strong></h4><ul>
<li><strong>密集预测</strong>：<br>语义分割（ADE20K）和深度估计（Waymo）性能显著提升（表4、表5）。</li>
<li><strong>视频分类</strong>：<br>冻结ViT-22B特征 + 轻量时序模型，在Kinetics 400达到<strong>88.0%</strong> 准确率（表6）。</li>
<li><strong>模型蒸馏</strong>：<br>将ViT-22B知识压缩到ViT-B，ImageNet准确率<strong>88.6%</strong>（SOTA，表8）。</li>
</ul>
<hr>
<h3 id="4-资源与数据"><a href="#4-资源与数据" class="headerlink" title="4. 资源与数据"></a><strong>4. 资源与数据</strong></h3><ul>
<li><strong>训练数据</strong>：40亿图像的扩展版JFT数据集。</li>
<li><strong>硬件</strong>：1024个TPU v4芯片，训练17.7万步（约3轮）。</li>
<li><strong>代码库</strong>：基于JAX&#x2F;FLAX&#x2F;Scenic实现。</li>
</ul>
<hr>
<h3 id="5-意义与启示"><a href="#5-意义与启示" class="headerlink" title="5. 意义与启示"></a><strong>5. 意义与启示</strong></h3><ul>
<li><strong>证明视觉模型可规模化</strong>：首次将ViT扩展到220亿参数，填补视觉与语言模型的规模差距。</li>
<li><strong>超越精度的收益</strong>：大模型提升公平性、鲁棒性和人类对齐性，为可靠AI提供新方向。</li>
<li><strong>开源生态</strong>：提供模型卡（Appendix C），促进可复现性和伦理评估。</li>
</ul>
<hr>
<h3 id="图示总结-1"><a href="#图示总结-1" class="headerlink" title="图示总结"></a>图示总结</h3><table>
<thead>
<tr>
<th align="left"><strong>关键创新</strong></th>
<th align="left"><strong>核心结果</strong></th>
<th align="left"><strong>应用场景</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left">并行层 + QK归一化</td>
<td align="left">ImageNet 89.51%</td>
<td align="left">图像分类</td>
</tr>
<tr>
<td align="left">异步计算优化</td>
<td align="left">形状偏置87% (接近人类)</td>
<td align="left">医疗&#x2F;自动驾驶</td>
</tr>
<tr>
<td align="left">参数分片策略</td>
<td align="left">OOD鲁棒性↑</td>
<td align="left">安全关键系统</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>初学者提示</strong>：ViT-22B的核心突破是证明<strong>视觉模型也能像语言模型一样通过扩大规模获得全面性能提升</strong>，而不仅是准确率提高。</p>
</blockquote>
<hr>
<hr>
<h2 id="Finite-Scalar-Quantization-VQ-VAE-Made-Simple"><a href="#Finite-Scalar-Quantization-VQ-VAE-Made-Simple" class="headerlink" title="Finite Scalar Quantization: VQ-VAE Made Simple"></a>Finite Scalar Quantization: VQ-VAE Made Simple</h2><h3 id="核心问题-2"><a href="#核心问题-2" class="headerlink" title="核心问题"></a><strong>核心问题</strong></h3><p>传统 <strong>VQ-VAE</strong>（向量量化变分自编码器）使用向量量化（VQ）将连续特征映射到离散码本，但存在两大痛点：</p>
<ol>
<li><strong>码本坍塌</strong>：码本中许多向量从未被使用。</li>
<li><strong>训练复杂</strong>：需额外损失函数、码本重置等技巧稳定训练。</li>
</ol>
<hr>
<h3 id="解决方案：FSQ（有限标量量化）"><a href="#解决方案：FSQ（有限标量量化）" class="headerlink" title="解决方案：FSQ（有限标量量化）"></a><strong>解决方案：FSQ（有限标量量化）</strong></h3><h4 id="核心思想-1"><a href="#核心思想-1" class="headerlink" title="核心思想"></a><strong>核心思想</strong></h4><ol>
<li><strong>降维+标量量化</strong>：<ul>
<li>将高维特征（如512维）<strong>投影到极低维度</strong>（如3-10维）。</li>
<li>对每一维<strong>独立离散化</strong>（例如将值域划分为3档：<code>&#123;-1, 0, 1&#125;</code>）。</li>
</ul>
</li>
<li><strong>隐式码本</strong>：<ul>
<li>若维度数 <code>d=3</code>，每维取3个值，则码本大小为 <code>3³=27</code>（如图1）。</li>
<li><strong>无需显式存储码本向量</strong>，直接通过离散值组合生成。</li>
</ul>
</li>
</ol>
<h4 id="关键优势-1"><a href="#关键优势-1" class="headerlink" title="关键优势"></a><strong>关键优势</strong></h4><table>
<thead>
<tr>
<th align="left"><strong>特性</strong></th>
<th align="left"><strong>VQ</strong></th>
<th align="left"><strong>FSQ</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left">码本利用率</td>
<td align="left">低（需额外技巧提升）</td>
<td align="left"><strong>近100%</strong>（天然避免坍塌）</td>
</tr>
<tr>
<td align="left">训练复杂度</td>
<td align="left">高（需承诺损失、熵正则等）</td>
<td align="left"><strong>极简</strong>（仅需STE梯度）</td>
</tr>
<tr>
<td align="left">参数量</td>
<td align="left">高（存储大码本）</td>
<td align="left"><strong>低</strong>（无显式码本）</td>
</tr>
<tr>
<td align="left">可扩展性</td>
<td align="left">大码本时性能下降</td>
<td align="left"><strong>大码本时性能更优</strong></td>
</tr>
</tbody></table>
<hr>
<h3 id="实验验证-1"><a href="#实验验证-1" class="headerlink" title="实验验证"></a><strong>实验验证</strong></h3><h4 id="任务表现"><a href="#任务表现" class="headerlink" title="任务表现"></a><strong>任务表现</strong></h4><p>FSQ 在以下任务中与 VQ 性能相当（差距仅 <strong>0.5-3%</strong>）：</p>
<ol>
<li><strong>图像生成</strong>（MaskGIT + ImageNet）<ul>
<li>生成质量接近（FID、精度&#x2F;召回率相似），见图4、图5。</li>
</ul>
</li>
<li><strong>密集预测任务</strong>（UViM）<ul>
<li>深度估计（RMSE）、全景分割（PQ）、图像上色（FID）均达到SOTA水平（表2）。</li>
</ul>
</li>
</ol>
<h4 id="核心发现"><a href="#核心发现" class="headerlink" title="核心发现"></a><strong>核心发现</strong></h4><ol>
<li><strong>码本利用率</strong>：<br>FSQ 码本利用率始终 ≈100%，而 VQ 在码本增大时利用率骤降（图3c）。</li>
<li><strong>参数量更少</strong>：<br>FSQ 省去码本参数（例如：VQ 需 4096×512≈200万参数，FSQ 仅需5维）。</li>
<li><strong>无依赖技巧</strong>：<br>FSQ 无需码本分裂（图6）、熵正则等复杂操作，训练更稳定。</li>
</ol>
<hr>
<h3 id="为什么FSQ有效？"><a href="#为什么FSQ有效？" class="headerlink" title="为什么FSQ有效？"></a><strong>为什么FSQ有效？</strong></h3><ul>
<li><strong>低维离散化</strong> 迫使编码器将信息<strong>均匀分布</strong>到各维度（图2左）。</li>
<li>解码器<strong>强大的拟合能力</strong>（如Transformer）可弥补量化损失，保持重建质量。</li>
<li><strong>简单即有效</strong>：固定网格划分比VQ的自适应分区更易优化。</li>
</ul>
<hr>
<h3 id="初学者要点-2"><a href="#初学者要点-2" class="headerlink" title="初学者要点"></a><strong>初学者要点</strong></h3><ol>
<li><strong>FSQ本质</strong>：<br>用 <code>d</code> 个离散变量（如 <code>[颜色, 纹理, 形状]</code>）代替高维向量，每变量取有限值（如红&#x2F;黄&#x2F;蓝）。</li>
<li><strong>颠覆认知</strong>：<br>即使码本是<strong>固定网格</strong>（非学习所得），配合强大解码器仍能匹敌VQ。</li>
<li><strong>实践意义</strong>：<ul>
<li>代码极简（论文附录提供20行实现）。</li>
<li>可直接替换现有VQ-VAE，无需调整下游模型。</li>
</ul>
</li>
</ol>
<blockquote>
<p><strong>论文标题的深意</strong>：<br>VQ-VAE 的复杂性主要源于向量量化（VQ），而 FSQ 用标量量化将其大幅简化，故名 <strong>“VQ-VAE Made Simple”</strong>。</p>
</blockquote>
<hr>
<p><strong>附：关键图示速览</strong></p>
<ul>
<li><strong>图1</strong>：FSQ（左）在3维网格中量化，VQ（右）在高维空间搜索最近邻。</li>
<li><strong>图3</strong>：FSQ 码本增大时性能持续提升，VQ 则下降。</li>
<li><strong>图5</strong>：FSQ 与 VQ 生成的图像视觉质量高度相似。</li>
</ul>
<p>此设计使 FSQ 成为<strong>更简单、更鲁棒</strong>的VQ替代方案，尤其适合需要离散表示的生成任务（图像&#x2F;音频&#x2F;多模态）。代码已在GitHub开源。</p>
<hr>
<hr>
<h2 id="GIVT-Generative-Infinite-Vocabulary-Transformers"><a href="#GIVT-Generative-Infinite-Vocabulary-Transformers" class="headerlink" title="GIVT: Generative Infinite-Vocabulary Transformers"></a>GIVT: Generative Infinite-Vocabulary Transformers</h2><h3 id="核心问题-3"><a href="#核心问题-3" class="headerlink" title="核心问题"></a><strong>核心问题</strong></h3><p>传统研究聚焦动物群体的 <strong>空间决策</strong>（如迁徙方向、觅食地点），但忽略了同等重要的 <strong>时间决策</strong>（何时行动）。本文提出：<strong>时间决策的集体智慧机制可能与空间决策截然不同</strong>，需独立研究。</p>
<hr>
<h3 id="关键概念"><a href="#关键概念" class="headerlink" title="关键概念"></a><strong>关键概念</strong></h3><ol>
<li><strong>集体智慧（Collective Intelligence）</strong><br>群体通过协作（如信息共享、求平均值）做出比个体更优的决策。例：鱼群集体躲避天敌。</li>
<li><strong>时间决策（Temporal Decisions）</strong><br>决定 <strong>何时行动</strong>（如何时迁徙、何时逃离天敌），而非去哪里。</li>
</ol>
<hr>
<h3 id="时间-vs-空间决策的四大差异"><a href="#时间-vs-空间决策的四大差异" class="headerlink" title="时间 vs. 空间决策的四大差异"></a><strong>时间 vs. 空间决策的四大差异</strong></h3><ol>
<li><strong>时间不可逆（单向性）</strong><ul>
<li><strong>空间</strong>：可反复考察多个选项（如比较多个巢穴位置）。</li>
<li><strong>时间</strong>：选项按顺序出现，错过无法回头（如迁徙时机稍纵即逝）。<br><em>→ 时间决策需预测未来，无法直接验证。</em></li>
</ul>
</li>
<li><strong>错误成本不对称</strong><ul>
<li><strong>太早行动</strong>：损失机会（如过早迁徙错过食物）。<br><strong>太晚行动</strong>：致命风险（如过晚逃离被天敌捕食）。<br><em>→ 简单“求平均值”策略在时间决策中可能失效。</em></li>
</ul>
</li>
<li><strong>信息收集的悖论</strong><ul>
<li>花更多时间收集信息可提高准确性，但会减少可用选项（时间流逝导致机会消失）。<br><em>→ 存在“最优信息收集时长”，过度犹豫适得其反。</em></li>
</ul>
</li>
<li><strong>群体动态的博弈性</strong><ul>
<li>个体最优时间可能冲突（如强壮个体想早迁徙，虚弱个体需延迟）。</li>
<li>群体需平衡 <strong>同步性</strong>（维持群体）和 <strong>精准性</strong>（个体最优）。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="为什么重要？-1"><a href="#为什么重要？-1" class="headerlink" title="为什么重要？"></a><strong>为什么重要？</strong></h3><ul>
<li><strong>生态意义</strong>：时间决策影响生存（如迁徙时机错误导致繁殖失败）。</li>
<li><strong>气候变化</strong>：全球变暖扰乱环境节律（如植物开花提前），动物需调整时间决策机制。</li>
<li><strong>人类启示</strong>：政策制定也需权衡“过早行动”（经济成本）和“过晚行动”（气候灾难）。</li>
</ul>
<hr>
<h3 id="未来研究方向"><a href="#未来研究方向" class="headerlink" title="未来研究方向"></a><strong>未来研究方向</strong></h3><ul>
<li>群体规模如何影响时间决策精度？</li>
<li>新机制探索：时间决策是否需要全新的集体智慧模型？</li>
<li>实验设计：如何测量野外动物的“最优行动时间”？</li>
</ul>
<hr>
<h3 id="一句话总结"><a href="#一句话总结" class="headerlink" title="一句话总结"></a><strong>一句话总结</strong></h3><blockquote>
<p><strong>时间决策是集体智慧的下一个前沿：其不可逆性、成本不对称性等特征，要求突破传统空间决策理论，建立全新研究框架。</strong></p>
</blockquote>
<hr>
<hr>
<h2 id="Unified-Auto-Encoding-with-Masked-Diffusion"><a href="#Unified-Auto-Encoding-with-Masked-Diffusion" class="headerlink" title="Unified Auto-Encoding with Masked Diffusion"></a>Unified Auto-Encoding with Masked Diffusion</h2><h3 id="1-研究目标-2"><a href="#1-研究目标-2" class="headerlink" title="1. 研究目标"></a><strong>1. 研究目标</strong></h3><ul>
<li><strong>核心问题</strong>：生成模型（如扩散模型）和表示学习模型（如MAE）通常独立设计，但二者都依赖“破坏输入→重建原始数据”的机制。</li>
<li><strong>目标</strong>：提出一种<strong>统一框架</strong>，同时实现高质量的图像生成（如扩散模型）和有效的特征表示（如MAE），且<strong>降低计算成本</strong>。</li>
</ul>
<hr>
<h3 id="2-现有方法的不足"><a href="#2-现有方法的不足" class="headerlink" title="2. 现有方法的不足"></a><strong>2. 现有方法的不足</strong></h3><table>
<thead>
<tr>
<th align="left"><strong>方法</strong></th>
<th align="left"><strong>优点</strong></th>
<th align="left"><strong>缺点</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>扩散模型</strong></td>
<td align="left">生成质量高（如清晰图像）</td>
<td align="left">计算开销大；需额外编码器</td>
</tr>
<tr>
<td align="left"><strong>MAE</strong></td>
<td align="left">计算高效；表示学习能力强</td>
<td align="left">生成图像模糊；生成能力弱</td>
</tr>
<tr>
<td align="left"><strong>混合模型</strong></td>
<td align="left">尝试结合两者</td>
<td align="left">生成与表示能力难以兼顾</td>
</tr>
</tbody></table>
<hr>
<h3 id="3-核心创新：UMD（统一掩码扩散）"><a href="#3-核心创新：UMD（统一掩码扩散）" class="headerlink" title="3. 核心创新：UMD（统一掩码扩散）"></a><strong>3. 核心创新：UMD（统一掩码扩散）</strong></h3><h4 id="关键设计"><a href="#关键设计" class="headerlink" title="关键设计"></a><strong>关键设计</strong></h4><ol>
<li><strong>融合两种破坏机制</strong>：<ul>
<li><strong>扩散模型</strong>：添加高斯噪声（细粒度破坏）。</li>
<li><strong>MAE</strong>：随机遮盖图像块（粗粒度破坏）。</li>
</ul>
</li>
<li><strong>改进训练流程</strong>：<ul>
<li>在扩散噪声计划中增加 <strong>无噪声掩码步骤</strong>（t&#x3D;0<em>t</em>&#x3D;0），遮盖比例高达75%。</li>
<li>后续步骤使用 <strong>混合破坏</strong>（噪声+掩码），遮盖比例降至37.5%。</li>
</ul>
</li>
<li><strong>双重建目标</strong>：<ul>
<li>预测原始图像（x0<em>x</em>0）→ 针对<strong>被遮盖区域</strong>。</li>
<li>预测噪声（ϵ<em>ϵ</em>）→ 针对<strong>可见区域</strong>。</li>
<li>公式：<br>L(θ)UMD&#x3D;rt&#x3D;0⋅Lt&#x3D;0+(1−rt&#x3D;0)⋅Lt≥1L(<em>θ</em>)UMD​&#x3D;<em>r**t</em>&#x3D;0​⋅L<em>t</em>&#x3D;0​+(1−<em>r**t</em>&#x3D;0​)⋅L<em>t</em>≥1​</li>
</ul>
</li>
</ol>
<h4 id="优势"><a href="#优势" class="headerlink" title="优势"></a><strong>优势</strong></h4><ul>
<li><strong>计算高效</strong>：高掩码比例减少计算量（比扩散模型快42%）。</li>
<li><strong>无需额外组件</strong>：不依赖数据增强、多视图或额外编码器。</li>
<li><strong>灵活适配</strong>：<ul>
<li>当 rt&#x3D;0&#x3D;1<em>r**t</em>&#x3D;0&#x3D;1 → 退化为MAE</li>
<li>当 rt&#x3D;0&#x3D;0<em>r**t</em>&#x3D;0&#x3D;0 → 退化为扩散模型</li>
</ul>
</li>
</ul>
<hr>
<h3 id="4-实验结果"><a href="#4-实验结果" class="headerlink" title="4. 实验结果"></a><strong>4. 实验结果</strong></h3><h4 id="性能对比（ImageNet-1K）"><a href="#性能对比（ImageNet-1K）" class="headerlink" title="性能对比（ImageNet-1K）"></a><strong>性能对比（ImageNet-1K）</strong></h4><table>
<thead>
<tr>
<th align="left"><strong>方法</strong></th>
<th align="left">训练耗时（TPU小时↓）</th>
<th align="left">表示学习（准确率↑）</th>
<th align="left">生成质量（FID↓&#x2F;IS↑）</th>
</tr>
</thead>
<tbody><tr>
<td align="left">MAE</td>
<td align="left">45</td>
<td align="left"><strong>36.6%</strong></td>
<td align="left">34.1 &#x2F; 13.4</td>
</tr>
<tr>
<td align="left">DiT</td>
<td align="left">105</td>
<td align="left">25.7%</td>
<td align="left"><strong>21.2</strong> &#x2F; <strong>23.0</strong></td>
</tr>
<tr>
<td align="left">UMD</td>
<td align="left"><strong>60</strong></td>
<td align="left">31.8%</td>
<td align="left">23.2 &#x2F; 20.2</td>
</tr>
</tbody></table>
<h4 id="关键结论"><a href="#关键结论" class="headerlink" title="关键结论"></a><strong>关键结论</strong></h4><ul>
<li><strong>表示学习</strong>：UMD接近MAE，显著优于扩散模型。</li>
<li><strong>图像生成</strong>：UMD接近DiT，远超MAE（生成图像更清晰）。</li>
<li><strong>迁移学习</strong>：在8个数据集上，UMD的少样本分类性能与MAE相当，优于扩散模型。</li>
</ul>
<hr>
<h3 id="5-重要图表解析"><a href="#5-重要图表解析" class="headerlink" title="5. 重要图表解析"></a><strong>5. 重要图表解析</strong></h3><ul>
<li><strong>图1（UMD流程）</strong>：<br>输入→添加噪声→掩码→编码器（处理可见块）→解码器（重建全图）。</li>
<li><strong>图3（生成样本）</strong>：<br>UMD和DiT生成清晰图像，MAE生成模糊且不连贯。</li>
<li><strong>表2（消融实验）</strong>：<br>同时预测 x0<em>x</em>0​ 和 ϵ<em>ϵ</em>、高 t&#x3D;0<em>t</em>&#x3D;0 采样比例、中等噪声掩码比例时性能最优。</li>
</ul>
<hr>
<h3 id="6-对初学者的启示"><a href="#6-对初学者的启示" class="headerlink" title="6. 对初学者的启示"></a><strong>6. 对初学者的启示</strong></h3><ul>
<li><strong>核心思想</strong>：通过<strong>可控破坏</strong>（噪声+掩码）让模型同时学习“生成”和“理解”图像。</li>
<li><strong>实践意义</strong>：<ul>
<li>单一模型解决两类任务，降低部署复杂度。</li>
<li>代码已开源（<a target="_blank" rel="noopener" href="https://github.com/philippe-eecs/small-vision">GitHub链接</a>）。</li>
</ul>
</li>
<li><strong>局限与未来</strong>：<br>破坏机制仍依赖人工设计，未来可探索自适应破坏策略。</li>
</ul>
<hr>
<h3 id="7-关键术语速查"><a href="#7-关键术语速查" class="headerlink" title="7. 关键术语速查"></a><strong>7. 关键术语速查</strong></h3><table>
<thead>
<tr>
<th align="left"><strong>术语</strong></th>
<th align="left"><strong>解释</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>表示学习</strong></td>
<td align="left">模型学习数据的特征（用于分类、检测等任务）</td>
</tr>
<tr>
<td align="left"><strong>生成模型</strong></td>
<td align="left">模型生成新数据（如图像、文本）</td>
</tr>
<tr>
<td align="left"><strong>FID&#x2F;IS</strong></td>
<td align="left">评估生成质量的指标（值越低&#x2F;越高越好）</td>
</tr>
<tr>
<td align="left"><strong>掩码比例</strong></td>
<td align="left">图像中被遮盖块的比例（如75% &#x3D; 保留25%）</td>
</tr>
</tbody></table>
<p>通过UMD，研究者成功弥合了生成模型与表示学习的鸿沟，为多任务学习提供了新思路。</p>
<hr>
<hr>
<h2 id="Jet-A-Modern-Transformer-Based-Normalizing-Flow"><a href="#Jet-A-Modern-Transformer-Based-Normalizing-Flow" class="headerlink" title="Jet: A Modern Transformer-Based Normalizing Flow"></a>Jet: A Modern Transformer-Based Normalizing Flow</h2><h3 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a><strong>问题背景</strong></h3><ol>
<li><strong>一维字符串的重复结构</strong><ul>
<li><strong>平方（Square）</strong>：形如 UU<em>UU</em> 的子串（如 “abab” 是 “ab” 的平方）。</li>
<li><strong>已知结论</strong>：长度为 n<em>n</em> 的字符串最多有 O(n)<em>O</em>(<em>n</em>) 个<strong>不同的平方</strong>，且可在 O(n)<em>O</em>(<em>n</em>) 时间内找到它们。</li>
</ul>
</li>
<li><strong>二维字符串的挑战</strong><ul>
<li>二维字符串是 n×n<em>n</em>×<em>n</em> 的字符矩阵。</li>
<li><strong>四元重复（Quartic）</strong>：由相同子块 W<em>W</em> 组成的 2×22×2 块（即 W2,2<em>W</em>2,2），是二维的“平方”类比。</li>
<li><strong>核心问题</strong>：一个 n×n<em>n</em>×<em>n</em> 二维字符串中，<strong>不同四元重复的最大数量</strong>是多少？如何高效计算它们？</li>
</ul>
</li>
</ol>
<hr>
<h3 id="先前工作"><a href="#先前工作" class="headerlink" title="先前工作"></a><strong>先前工作</strong></h3><ul>
<li><strong>上界</strong>：Charalampopoulos 等 (2020) 证明上界为 O(n2log⁡2n)<em>O</em>(<em>n</em>2log2<em>n</em>)。</li>
<li><strong>下界</strong>：Gawrychowski 等 (2021) 构造了含 Ω(n2log⁡n)Ω(<em>n</em>2log<em>n</em>) 个不同四元重复的字符串。</li>
<li><strong>悬而未决</strong>：上界与下界之间存在 log⁡nlog<em>n</em> 的差距，且算法复杂度未优化。</li>
</ul>
<hr>
<h3 id="本文贡献"><a href="#本文贡献" class="headerlink" title="本文贡献"></a><strong>本文贡献</strong></h3><ol>
<li><strong>紧确的组合上界</strong><ul>
<li>证明不同四元重复的数量为 O(n2log⁡n)<em>O</em>(<em>n</em>2log<em>n</em>)，与下界匹配，<strong>终结了最优界问题</strong>。</li>
<li><strong>关键思想</strong>：<ul>
<li><strong>分类</strong>：将四元重复分为 <strong>Thin</strong>（如 P2,2<em>P</em>2,2）和 <strong>Thick</strong>（如 Px,y,x,y≥5<em>P**x</em>,<em>y</em>,<em>x</em>,<em>y</em>≥5)。</li>
<li><strong>Thin 上界</strong>：通过分析“极端出现位置”和周期性，证明其数量为 O(n2log⁡n)<em>O</em>(<em>n</em>2log<em>n</em>)。</li>
<li><strong>Thick 上界</strong>：将问题转化为几何点集分析，利用 <strong>Marcus-Tardos 定理</strong>（置换矩阵避免）证明点集大小为 O(log⁡n)<em>O</em>(log<em>n</em>)，从而得界。</li>
</ul>
</li>
</ul>
</li>
<li><strong>最优算法</strong><ul>
<li>设计 O(n2log⁡n)<em>O</em>(<em>n</em>2log<em>n</em>) 时间算法枚举所有不同四元重复，与组合上界匹配。</li>
<li><strong>关键技术</strong>：<ul>
<li><strong>元字符串（Metastring）</strong>：将行&#x2F;列视为超级字符，转化成一维问题。</li>
<li><strong>周期性查询</strong>：预处理快速检测周期性结构（如 2-period 查询）。</li>
<li><strong>分层处理</strong>：按尺寸分组（正则化高度&#x2F;宽度为 2a×2b2<em>a</em>×2<em>b</em>），避免冗余计算。</li>
<li><strong>天空线字符串（Skyline）</strong>：筛选“非支配”基元串，减少候选集。</li>
</ul>
</li>
</ul>
</li>
</ol>
<hr>
<h3 id="核心证明图示"><a href="#核心证明图示" class="headerlink" title="核心证明图示"></a><strong>核心证明图示</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">四元重复 Q = W²,²</span><br><span class="line">    [W | W]</span><br><span class="line">    [---]</span><br><span class="line">    [W | W]</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>极端出现（Extreme Occurrence）</strong>：Q 的右下角位置无更靠右下的其他出现。</li>
<li><strong>锚点（Anchor）</strong>：将 Thick 四元重复关联到位置 (i+2h,j+2w)(<em>i</em>+2<em>h</em>,<em>j</em>+2<em>w</em>)，约束其数量。</li>
<li><strong>点集 PP</strong>：用坐标 (log⁡h,log⁡w)(log<em>h</em>,log<em>w</em>) 表示基元串，通过链分解和 Marcus-Tardos 定理限制其大小。</li>
</ul>
<hr>
<h3 id="开放问题"><a href="#开放问题" class="headerlink" title="开放问题"></a><strong>开放问题</strong></h3><ul>
<li><strong>二维游程（2D Runs）</strong>：当前上界 O(n2log⁡2n)<em>O</em>(<em>n</em>2log2<em>n</em>) 与下界 Ω(n2log⁡n)Ω(<em>n</em>2log<em>n</em>) 仍存在 log⁡nlog<em>n</em> 差距，尚未解决。</li>
</ul>
<hr>
<h3 id="初学者要点-3"><a href="#初学者要点-3" class="headerlink" title="初学者要点"></a><strong>初学者要点</strong></h3><ul>
<li><strong>四元重复</strong>：二维中由 4 个相同块构成的 2×22×2 结构。</li>
<li><strong>维度跃升</strong>：从一维（平方）到二维（四元重复），复杂度从 O(n)<em>O</em>(<em>n</em>) 增至 O(n2log⁡n)<em>O</em>(<em>n</em>2log<em>n</em>)，体现“维度诅咒”。</li>
<li><strong>方法论</strong>：组合证明依赖<strong>分类+周期性+几何</strong>；算法依赖<strong>元字符串+分层预处理</strong>。</li>
<li><strong>意义</strong>：首次给出二维重复结构的紧确界与最优算法，为更高维重复分析提供模板。</li>
</ul>
<blockquote>
<p><strong>注</strong>：原文技术细节较深（如 Marcus-Tardos 定理、元字符串操作）。初学者可先掌握问题框架与结论，再逐步深入证明。</p>
</blockquote>
<hr>
<hr>
<h2 id="JetFormer-An-autoregressive-generative-model-of-raw-images-and-text"><a href="#JetFormer-An-autoregressive-generative-model-of-raw-images-and-text" class="headerlink" title="JetFormer: An autoregressive generative model of raw images and text"></a>JetFormer: An autoregressive generative model of raw images and text</h2><h3 id="核心问题-4"><a href="#核心问题-4" class="headerlink" title="核心问题"></a><strong>核心问题</strong></h3><p>自监督语音模型（如HuBERT、WavLM）不同层捕获的信息类型不同，但现有方法（如加权求和层特征）可能造成信息损失。论文提出：<strong>在预训练模型（上游）和任务模型（下游）之间需设计专门的“接口模块”</strong>，以更高效融合多层信息。</p>
<hr>
<h3 id="关键创新：接口设计"><a href="#关键创新：接口设计" class="headerlink" title="关键创新：接口设计"></a><strong>关键创新：接口设计</strong></h3><p>提出5种接口方案，核心思想是避免层间特征直接相加导致的信息冲突：</p>
<ol>
<li><strong>分组加权求和</strong><br>将层分组后分别加权求和，拼接结果再投影。</li>
<li><strong>拼接+投影</strong><br>所有层特征拼接后，用线性层压缩维度。</li>
<li><strong>分层卷积（效果最佳）</strong><br>在层维度做一维卷积（固定核大小5，步长3），堆叠 ⌊log₃L⌋ 层逐步融合信息（图2a）。</li>
<li><strong>CLS池化</strong><br>添加可学习CLS标记，用Transformer聚合层信息（图2b）。</li>
<li><strong>PCA降维+拼接</strong><br>每层PCA降维后拼接。</li>
</ol>
<hr>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><strong>实验结果</strong></h3><ul>
<li><strong>测试基准</strong>：ML-SUPERB（多语言ASR）、SUPERB（语音识别&#x2F;情感分析等任务）。</li>
<li><strong>上游模型</strong>：HuBERT、WavLM、XLSR-53的Base&#x2F;Large版本。</li>
<li><strong>核心发现</strong>：<ul>
<li><strong>分层卷积接口全面最优</strong>（表1,2）：<ul>
<li>在ASR任务上，CER比加权求和降低1-4点（如HuBERT Base：35.1→33.9）。</li>
<li>在音素识别（PR）任务，PER从5.41→2.93（HuBERT Base）。</li>
</ul>
</li>
<li><strong>参数量不是主因</strong>（表3）：<br>即使增加下游模型参数量，仍不如分层卷积接口。</li>
<li><strong>微调时仍有效</strong>（表4）：<br>上游模型可训练时，分层卷积仍优于加权求和（CER 31.1 vs 31.5）。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="初学者要点-4"><a href="#初学者要点-4" class="headerlink" title="初学者要点"></a><strong>初学者要点</strong></h3><ol>
<li><strong>为什么需要接口？</strong><br>语音模型不同层学到不同特征（如浅层声学、深层语义），直接相加可能混合冲突信息。</li>
<li><strong>分层卷积为何有效？</strong><br>通过局部卷积逐步融合相邻层特征，保留层次化信息（类似渐进式汇总）。</li>
<li><strong>实践意义</strong>：<ul>
<li>替换加权求和接口可显著提升性能（尤其深层模型）。</li>
<li>代码已开源（GitHub链接见论文）。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="图示辅助理解"><a href="#图示辅助理解" class="headerlink" title="图示辅助理解"></a><strong>图示辅助理解</strong></h3><ul>
<li><strong>图1</strong>：框架图（上游 → 接口 → 下游）。</li>
<li><strong>图2</strong>：分层卷积（a）与CLS池化（b）的结构对比。</li>
</ul>
<blockquote>
<p><strong>注</strong>：初学者可重点关注摘要、实验结论（表2）及分层卷积设计（2.2.3节）。论文通过大量实验验证了接口设计的重要性，为自监督语音模型应用提供了新思路。</p>
</blockquote>
<hr>
<hr>
<h1 id="多模态研究"><a href="#多模态研究" class="headerlink" title="多模态研究"></a>多模态研究</h1><h2 id="LiT-Zero-Shot-Transfer-with-Locked-image-Text-Tuning"><a href="#LiT-Zero-Shot-Transfer-with-Locked-image-Text-Tuning" class="headerlink" title="LiT: Zero-Shot Transfer with Locked-image Text Tuning,"></a><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2111.07991">LiT: Zero-Shot Transfer with Locked-image Text Tuning</a></strong>,</h2><h3 id="1-研究目标-3"><a href="#1-研究目标-3" class="headerlink" title="1. 研究目标"></a><strong>1. 研究目标</strong></h3><ul>
<li><strong>解决什么问题？</strong><br>传统视觉模型需要针对新任务微调（需标注数据），而本文提出一种<strong>零样本迁移（Zero-Shot Transfer）</strong> 方法，无需额外训练即可适应新任务（如图像分类、检索）。</li>
<li><strong>关键挑战</strong><br>如何让预训练的视觉模型直接理解自然语言描述的类别（如“狗”、“汽车”），而无需重新训练。</li>
</ul>
<hr>
<h3 id="2-核心方法：LiT（Locked-image-Tuning）"><a href="#2-核心方法：LiT（Locked-image-Tuning）" class="headerlink" title="2. 核心方法：LiT（Locked-image Tuning）"></a><strong>2. 核心方法：LiT（Locked-image Tuning）</strong></h3><h4 id="核心思想-2"><a href="#核心思想-2" class="headerlink" title="核心思想"></a><strong>核心思想</strong></h4><ul>
<li><strong>锁定图像模型，只训练文本模型</strong><ul>
<li>使用<strong>预训练好的视觉模型</strong>（如ViT、ResNet）提取图像特征，<strong>固定其权重（不更新）</strong>。</li>
<li>仅训练一个<strong>文本模型</strong>，使其输出的文本特征与图像特征在共享空间中对齐。</li>
</ul>
</li>
<li><strong>训练方式：对比学习（Contrastive Learning）</strong><ul>
<li>目标：让匹配的“图像-文本对”特征相似，不匹配的特征远离。</li>
<li>例如：一张狗的图像和文本“狗”的特征应接近，而和“汽车”的特征应远离。</li>
</ul>
</li>
</ul>
<h4 id="为什么有效？"><a href="#为什么有效？" class="headerlink" title="为什么有效？"></a><strong>为什么有效？</strong></h4><ul>
<li>预训练视觉模型已学习高质量的通用图像表示，冻结可避免破坏这些表示。</li>
<li>文本模型只需学会“读取”视觉模型的特征，无需从头学习视觉概念，更高效。</li>
</ul>
<hr>
<h3 id="3-关键优势"><a href="#3-关键优势" class="headerlink" title="3. 关键优势"></a><strong>3. 关键优势</strong></h3><ol>
<li><strong>零样本性能强</strong><ul>
<li>在ImageNet上达到<strong>85.2%</strong> 零样本分类准确率（ViT-g&#x2F;14模型），超越之前方法（如CLIP的76.2%）。</li>
<li>在鲁棒性测试（如ObjectNet）上达到<strong>82.5%</strong>，提升显著。</li>
</ul>
</li>
<li><strong>计算高效</strong><ul>
<li>只需训练轻量文本模型，复用现有视觉模型，<strong>训练速度提升40倍</strong>（图1右）。</li>
<li>仅需<strong>3亿图文对</strong>即可达到81.7%准确率，而CLIP需128亿（图1左）。</li>
</ul>
</li>
<li><strong>通用性强</strong><ul>
<li>支持多种视觉架构（ViT、ResNet、MLP-Mixer）。</li>
<li>兼容监督&#x2F;无监督预训练模型（如DINO、MoCo）。</li>
<li>在多语言任务中表现优异（图5-6），无需翻译数据。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="4-重要结论"><a href="#4-重要结论" class="headerlink" title="4. 重要结论"></a><strong>4. 重要结论</strong></h3><ul>
<li><strong>图像模型应冻结</strong>：解锁图像模型会导致过拟合图文数据，损害泛化能力（图4）。</li>
<li><strong>文本模型选择</strong>：BERT表现最好，但ViT+SentencePiece更稳定（表4）。</li>
<li><strong>数据效率</strong>：小规模公开数据（YFCC100M+CC12M）即可达到75.7% ImageNet准确率（表1）。</li>
</ul>
<hr>
<h3 id="5-实用价值"><a href="#5-实用价值" class="headerlink" title="5. 实用价值"></a><strong>5. 实用价值</strong></h3><ul>
<li><strong>开源模型</strong>：提供预训练模型和代码（<a target="_blank" rel="noopener" href="https://github.com/google-research/vision_transformerflit-models">GitHub链接</a>）。</li>
<li><strong>低成本应用</strong>：仅需中等算力即可训练高性能零样本模型，促进广泛研究。</li>
</ul>
<hr>
<h3 id="类比理解"><a href="#类比理解" class="headerlink" title="类比理解"></a><strong>类比理解</strong></h3><blockquote>
<p>想象预训练视觉模型是一本百科全书，LiT只需训练一个“翻译官”（文本模型），让它学会用自然语言查询百科全书中的知识，无需修改百科全书本身。</p>
</blockquote>
<hr>
<h3 id="附录：关键图表速览"><a href="#附录：关键图表速览" class="headerlink" title="附录：关键图表速览"></a><strong>附录：关键图表速览</strong></h3><table>
<thead>
<tr>
<th align="left"><strong>内容</strong></th>
<th align="left"><strong>位置</strong></th>
<th align="left"><strong>结论</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left">LiT vs CLIP 性能对比</td>
<td align="left">图1</td>
<td align="left">LiT数据效率更高，性能更强</td>
</tr>
<tr>
<td align="left">不同训练策略（Lu&#x2F;Uu）</td>
<td align="left">图3、表2</td>
<td align="left">图像锁定（Lu）效果最佳</td>
</tr>
<tr>
<td align="left">多语言零样本分类</td>
<td align="left">图5-6</td>
<td align="left">支持100+语言，长尾语言性能显著提升</td>
</tr>
<tr>
<td align="left">模型架构对比</td>
<td align="left">表6、附录A</td>
<td align="left">ViT &gt; MLP-Mixer &gt; ResNet</td>
</tr>
</tbody></table>
<p>初学者只需掌握：<strong>LiT通过冻结图像模型 + 对比学习训练文本模型，实现了高效、强大的零样本视觉任务迁移。</strong></p>
<hr>
<hr>
<h2 id="CLIPPO-Image-and-Language-Understanding-from-Pixels-Only"><a href="#CLIPPO-Image-and-Language-Understanding-from-Pixels-Only" class="headerlink" title="CLIPPO: Image-and-Language Understanding from Pixels Only"></a><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2212.08037">CLIPPO: Image-and-Language Understanding from Pixels Only</a></strong></h2><h3 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a><strong>研究背景</strong></h3><ul>
<li><strong>问题</strong>：大语言模型（LLM）在生成答案时缺乏<strong>可归因性</strong>（即无法提供支持答案的证据来源），降低了可信度。</li>
<li><strong>目标</strong>：提出**可归因问答（Attributed QA）**任务，要求模型生成答案时附带证据片段（如维基百科段落），提升可解释性。</li>
</ul>
<hr>
<h3 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a><strong>任务定义</strong></h3><ul>
<li><strong>输入</strong>：问题（<em>e.g., “恐龙最早出现在何时？”</em>）</li>
<li><strong>输出</strong>：答案 + 证据来源（<code>(答案, 证据段落)</code>）<ul>
<li>示例：<br><code>答案 = &quot;2.3亿年前&quot;</code><br><code>证据 = &quot;恐龙出现在三叠纪中期（约2.3亿年前）...&quot; [链接]</code></li>
</ul>
</li>
</ul>
<hr>
<h3 id="关键贡献"><a href="#关键贡献" class="headerlink" title="关键贡献"></a><strong>关键贡献</strong></h3><ol>
<li><p><strong>评估框架</strong></p>
<ul>
<li><strong>人工评估（AIS）</strong>：标注员判断答案是否完全由证据段落支持（双条件：信息可理解 + 证据充分支持）。</li>
<li><strong>自动评估（AutoAIS）</strong>：基于自然语言推理（NLI）的自动指标，与人工评分强相关（Pearson&#x3D;0.96），适合开发阶段使用。</li>
</ul>
</li>
<li><p><strong>系统架构对比</strong><br>测试了四类模型，性能如下（AIS分数越高越好）：</p>
<table>
<thead>
<tr>
<th align="left"><strong>架构</strong></th>
<th align="left"><strong>特点</strong></th>
<th align="left"><strong>AIS分数</strong></th>
<th align="left"><strong>优缺点</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>检索-阅读（RTR）</strong></td>
<td align="left">先检索证据，再生成答案</td>
<td align="left">65.5%</td>
<td align="left">性能最佳，但需大量标注数据训练</td>
</tr>
<tr>
<td align="left"><strong>后验归因（Post-hoc）</strong></td>
<td align="left">先生成答案，再检索证据</td>
<td align="left">55.6%</td>
<td align="left">只需少量样本提示，但归因效果较弱</td>
</tr>
<tr>
<td align="left"><strong>低资源系统</strong></td>
<td align="left">仅用64个样本提示 + 稀疏检索</td>
<td align="left">48.6%</td>
<td align="left">资源效率高，适合标注稀缺场景</td>
</tr>
<tr>
<td align="left"><strong>端到端LLM归因</strong></td>
<td align="left">直接生成答案+来源（如URL）</td>
<td align="left">46.0%</td>
<td align="left">无需检索模块，但依赖超大模型（540B参数）</td>
</tr>
</tbody></table>
</li>
<li><p><strong>核心发现</strong></p>
<ul>
<li><strong>归因 ≠ 答案正确性</strong>：传统精确匹配（EM）指标与归因质量相关性低（Pearson&#x3D;0.45），EM会错误惩罚合理答案（<em>例：”Julie Kavner” vs 标准答案”Julie Deborah Kavner”</em>）。</li>
<li><strong>证据多样性挑战</strong>：同一问题可能存在多个有效证据段落（平均1.57个&#x2F;问题），需支持答案多样性。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="未来方向"><a href="#未来方向" class="headerlink" title="未来方向"></a><strong>未来方向</strong></h3><ul>
<li><strong>模型设计</strong>：降低RTR系统的训练资源消耗；提升后验归因的准确性。</li>
<li><strong>评估优化</strong>：改进AutoAIS在实例级别的可靠性；构建多语言&#x2F;多模态归因数据集。</li>
<li><strong>任务扩展</strong>：从问答延伸至对话、长文本生成等复杂场景。</li>
</ul>
<hr>
<h3 id="资源与伦理"><a href="#资源与伦理" class="headerlink" title="资源与伦理"></a><strong>资源与伦理</strong></h3><ul>
<li><strong>开源数据</strong>：发布所有模型输出和人工评分（<a target="_blank" rel="noopener" href="https://github.com/google-research-datasets/Attributed-QA">GitHub链接</a>）。</li>
<li><strong>伦理考量</strong>：归因不保证事实正确性，但使用户能自主验证来源，缓解LLM的”幻觉”问题。</li>
</ul>
<hr>
<h3 id="初学者要点-5"><a href="#初学者要点-5" class="headerlink" title="初学者要点"></a><strong>初学者要点</strong></h3><ol>
<li><strong>可归因性</strong> &#x3D; 答案 + 证据来源，提升可信度。</li>
<li><strong>评估重点</strong>：人工AIS是黄金标准，AutoAIS可作为开发替代。</li>
<li><strong>最佳架构</strong>：RTR效果最好但耗资源；后验归因适合低资源场景。</li>
<li><strong>传统EM的局限</strong>：可能误判合理答案，需结合归因评估。</li>
</ol>
<blockquote>
<p>论文通过系统实验回答了三个核心问题：<br><strong>如何评估归因？</strong> → AIS + AutoAIS<br><strong>当前方法效果？</strong> → RTR最优但需改进<br><strong>如何构建可归因LLM？</strong> → 融合检索与生成，探索端到端设计</p>
</blockquote>
<hr>
<hr>
<h2 id="Sigmoid-Loss-for-Language-Image-Pre-Training"><a href="#Sigmoid-Loss-for-Language-Image-Pre-Training" class="headerlink" title="Sigmoid Loss for Language Image Pre-Training"></a><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.15343">Sigmoid Loss for Language Image Pre-Training</a></strong></h2><h3 id="核心问题-5"><a href="#核心问题-5" class="headerlink" title="核心问题"></a><strong>核心问题</strong></h3><p>传统图文对比学习（如CLIP）使用 <strong>Softmax Loss</strong>，需计算整个批次的相似度矩阵进行归一化，导致：</p>
<ol>
<li><strong>内存消耗大</strong>（存储 <code>batch_size × batch_size</code> 矩阵）</li>
<li><strong>难以扩展批次大小</strong>（尤其资源有限时）</li>
<li><strong>小批次性能较差</strong></li>
</ol>
<hr>
<h3 id="解决方案：Sigmoid-Loss"><a href="#解决方案：Sigmoid-Loss" class="headerlink" title="解决方案：Sigmoid Loss"></a><strong>解决方案：Sigmoid Loss</strong></h3><p>作者提出 <strong>Sigmoid Loss</strong>，将图文匹配视为<strong>二元分类任务</strong>（正样本&#x3D;匹配对，负样本&#x3D;不匹配对）：</p>
<ul>
<li><strong>核心公式</strong>：<br>Lij&#x3D;−log⁡11+e−zij(t⋅xiTyj+b)<em>L**ij</em>​&#x3D;−log1+<em>e</em>−<em>z**ij</em>​(<em>t</em>⋅<em>x<strong>i</strong>T</em>​<em>y**j</em>​+<em>b</em>)1​<br>（zij&#x3D;1<em>z**ij</em>​&#x3D;1 表示匹配，否则为 −1−1；t,b<em>t</em>,<em>b</em> 为可学习参数）</li>
</ul>
<h4 id="关键优势-2"><a href="#关键优势-2" class="headerlink" title="关键优势"></a><strong>关键优势</strong></h4><ol>
<li><strong>内存高效</strong><ul>
<li>无需全局相似度矩阵，仅需局部计算（图1展示分块优化）。</li>
<li>支持<strong>超大批次</strong>（最高达 <strong>100万</strong>），仅需少量设备（如4 TPUv4芯片）。</li>
</ul>
</li>
<li><strong>性能更优</strong><ul>
<li><strong>小批次</strong>（&lt;16k）时显著优于Softmax（图2左）。</li>
<li><strong>32k批次</strong>即达饱和性能，更大批次收益微弱（图2）。</li>
</ul>
</li>
<li><strong>训练稳定</strong><ul>
<li>对噪声数据更鲁棒（图7）。</li>
<li>引入偏置项 b<em>b</em> 避免初始过校正（表4）。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="关键实验结果"><a href="#关键实验结果" class="headerlink" title="关键实验结果"></a><strong>关键实验结果</strong></h3><table>
<thead>
<tr>
<th align="left"><strong>模型</strong></th>
<th align="left"><strong>资源</strong></th>
<th align="left"><strong>训练时间</strong></th>
<th align="left"><strong>ImageNet 零样本精度</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left">SigLiT（ViT-g&#x2F;14）</td>
<td align="left">4 TPUv4芯片</td>
<td align="left">2天</td>
<td align="left"><strong>84.5%</strong></td>
</tr>
<tr>
<td align="left">SigLIP（ViT-B）</td>
<td align="left">32 TPUv4芯片</td>
<td align="left">2天</td>
<td align="left"><strong>72.1%</strong></td>
</tr>
<tr>
<td align="left">mSigLIP（多语言）</td>
<td align="left">100+语言数据</td>
<td align="left">-</td>
<td align="left"><strong>跨模态检索SOTA</strong></td>
</tr>
</tbody></table>
<ol>
<li><strong>批次大小影响</strong><ul>
<li>32k批次为<strong>性价比最优</strong>（图2），更大批次收益递减。</li>
</ul>
</li>
<li><strong>多语言训练</strong><ul>
<li>32k批次在100+语言上足够，更大批次反降性能（图2右）。</li>
</ul>
</li>
<li><strong>微调技巧</strong><ul>
<li>微调预训练视觉模型时，<strong>关闭权重衰减</strong>可保留表征质量（图4）。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="实用价值"><a href="#实用价值" class="headerlink" title="实用价值"></a><strong>实用价值</strong></h3><ol>
<li><strong>低资源训练</strong><ul>
<li>仅需 <strong>4 TPUv4芯片 + 1天</strong> 达到79.7% ImageNet精度（表1）。</li>
</ul>
</li>
<li><strong>开源模型</strong><ul>
<li>代码与模型公开：<strong><a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision">big_vision项目</a></strong>。</li>
<li>性能超越CLIP、OpenCLIP等（表3）。</li>
</ul>
</li>
<li><strong>研究方向</strong><ul>
<li>探索高效负样本挖掘（图6）。</li>
<li>扩展分辨率与多语言场景（第4.6节）。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="总结一句话"><a href="#总结一句话" class="headerlink" title="总结一句话"></a><strong>总结一句话</strong></h3><p><strong>Sigmoid Loss 以更低资源、更高效率实现媲美Softmax的图文表征学习，尤其适合小规模实验与多语言扩展。</strong></p>
<blockquote>
<p>初学者可重点关注：</p>
<ul>
<li><strong>Sigmoid vs Softmax</strong> 的差异（二元分类 vs 全局对比）</li>
<li><strong>批次大小</strong>的选择（32k性价比最高）</li>
<li><strong>开源代码</strong>实践（直接复现实验）</li>
</ul>
</blockquote>
<hr>
<hr>
<h2 id="A-Study-of-Autoregressive-Decoders-for-Multi-Tasking-in-Computer-Vision"><a href="#A-Study-of-Autoregressive-Decoders-for-Multi-Tasking-in-Computer-Vision" class="headerlink" title="A Study of Autoregressive Decoders for Multi-Tasking in Computer Vision"></a><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2304.08479">A Study of Autoregressive Decoders for Multi-Tasking in Computer Vision</a></strong></h2><h3 id="研究目标"><a href="#研究目标" class="headerlink" title="研究目标"></a><strong>研究目标</strong></h3><ul>
<li><strong>核心问题</strong>：如何设计视觉语言模型（VLMs）的提示（Prompts），使其在面对<strong>分布偏移</strong>（测试数据与训练数据差异）和<strong>新类别</strong>（训练未见的类别）时仍保持鲁棒性？</li>
</ul>
<hr>
<h3 id="关键概念-1"><a href="#关键概念-1" class="headerlink" title="关键概念"></a><strong>关键概念</strong></h3><ol>
<li><strong>两种鲁棒性</strong>：<ul>
<li><strong>基础类别鲁棒性</strong>：测试图像与训练集类别相同，但数据分布不同（如训练用正常猫图，测试用模糊猫图）。</li>
<li><strong>新类别鲁棒性</strong>：测试图像属于训练未见的类别（如训练只有“猫&#x2F;狗”，测试出现“老虎”）。</li>
</ul>
</li>
<li><strong>两类方法</strong>：<ul>
<li><strong>上下文学习（IcoL）</strong>：从支持集中选取示例作为提示（如选5张猫图+描述）。<br><em>问题</em>：对示例选择敏感，鲁棒性差。</li>
<li><strong>提示学习（ProL）</strong>：用支持集训练生成“虚拟提示嵌入”（无需手动设计）。<br><em>问题</em>：在基础类别上鲁棒，但难以泛化到新类别。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="核心发现-1"><a href="#核心发现-1" class="headerlink" title="核心发现"></a><strong>核心发现</strong></h3><ol>
<li><strong>现有方法对比</strong>：<ul>
<li><strong>ProL</strong>：在基础类别上鲁棒性<strong>优于</strong> IcoL（因利用所有训练信息）。</li>
<li><strong>IcoL</strong>：在新类别上泛化能力<strong>优于</strong> ProL（因ProL过度拟合训练类别）。</li>
</ul>
</li>
<li><strong>创新方案：Robust-ProL</strong><ul>
<li><strong>思路</strong>：将同一图像的<strong>多尺度特征</strong>（如224×224, 384×384）融入提示。</li>
<li><strong>优势</strong>：<ul>
<li>多尺度特征捕捉不同层次信息（如局部细节+全局结构）。</li>
<li>减少模型对单一尺度的依赖，避免过拟合伪特征。</li>
</ul>
</li>
<li><strong>效果</strong>：在基础类别<strong>和新类别</strong>上鲁棒性均显著提升（见下图）。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="实验结果-1"><a href="#实验结果-1" class="headerlink" title="实验结果"></a><strong>实验结果</strong></h3><ul>
<li><strong>数据集</strong>：ImageNet及5个分布偏移数据集（如ImageNet-R艺术画风、ImageNet-C噪声干扰等）。</li>
<li><strong>结论</strong>：<ul>
<li><strong>Robust-ProL</strong> 在基础类别上接近全监督模型，在新类别上准确率比传统ProL <strong>提高超20%</strong>。</li>
<li>多尺度特征比单尺度或传统集成方法更高效（计算量相同，性能更优）。</li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th align="left">方法</th>
<th align="left">基础类别鲁棒性</th>
<th align="left">新类别鲁棒性</th>
</tr>
</thead>
<tbody><tr>
<td align="left">IcoL-Random</td>
<td align="left">低</td>
<td align="left">中</td>
</tr>
<tr>
<td align="left">IcoL-RICE</td>
<td align="left">中</td>
<td align="left">中</td>
</tr>
<tr>
<td align="left">ProL</td>
<td align="left"><strong>高</strong></td>
<td align="left">低</td>
</tr>
<tr>
<td align="left"><strong>Robust-ProL</strong></td>
<td align="left"><strong>最高</strong></td>
<td align="left"><strong>最高</strong></td>
</tr>
</tbody></table>
<hr>
<h3 id="研究意义"><a href="#研究意义" class="headerlink" title="研究意义"></a><strong>研究意义</strong></h3><ol>
<li><strong>实践价值</strong>：提供一种简单有效的提示设计方法，提升VLMs在开放环境中的泛化能力。</li>
<li><strong>理论启示</strong>：多尺度特征能增强模型对视觉变化的适应性，减少过拟合。</li>
</ol>
<hr>
<h3 id="初学者要点-6"><a href="#初学者要点-6" class="headerlink" title="初学者要点"></a><strong>初学者要点</strong></h3><ul>
<li><strong>提示（Prompt）</strong>：引导模型完成任务的指令或示例。</li>
<li><strong>分布偏移</strong>：测试数据与训练数据不一致（如不同光照、风格）。</li>
<li><strong>多尺度特征</strong>：同一图像的不同分辨率版本，帮助模型全面理解内容。</li>
</ul>
<blockquote>
<p><strong>通俗类比</strong>：<br>传统ProL像“死记硬背考试重点”，遇到新题型易挂科；<br>Robust-ProL像“掌握多角度解题思路”，无论题型变化都能应对。</p>
</blockquote>
<p>此工作为VLMs的实用化提供了重要思路，后续可探索更多模态融合与动态提示设计。</p>
<hr>
<hr>
<h2 id="Image-Captioners-Are-Scalable-Vision-Learners-Too"><a href="#Image-Captioners-Are-Scalable-Vision-Learners-Too" class="headerlink" title="Image Captioners Are Scalable Vision Learners Too"></a><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.04639">Image Captioners Are Scalable Vision Learners Too</a></strong></h2><h3 id="核心目标：算术化导向（Arithmetization-Oriented-AO）"><a href="#核心目标：算术化导向（Arithmetization-Oriented-AO）" class="headerlink" title="核心目标：算术化导向（Arithmetization-Oriented, AO）"></a>核心目标：算术化导向（Arithmetization-Oriented, AO）</h3><ul>
<li><strong>问题：</strong> 像 zkSNARKs、MPC、FHE 这样的高级密码协议中，传统的对称密码原语（如 AES, SHA）效率很低。原因是这些协议需要在有限域上进行大量计算，而传统密码设计并非为此优化。</li>
<li><strong>解决：</strong> 设计新的置换（Permutation）和哈希函数，其核心操作（尤其是乘法）在零知识证明电路中非常“便宜”（约束少、深度浅）。这就是“算术化导向”的含义。</li>
</ul>
<h3 id="Arion-的核心创新：GTDS"><a href="#Arion-的核心创新：GTDS" class="headerlink" title="Arion 的核心创新：GTDS"></a>Arion 的核心创新：GTDS</h3><ul>
<li><strong>基础：</strong> Arion 基于一种新的数学框架——<strong>广义三角动力系统（Generalized Triangular Dynamical System, GTDS）</strong>。</li>
<li><strong>结构特点：</strong><ul>
<li><strong>分支混合：</strong> 在一个轮函数中，结合了 <strong>SPN（替换置换网络）</strong> 和 <strong>Feistel 网络</strong> 的优点。</li>
<li><strong>分支策略：</strong> <code>n</code> 个分支中，<code>n-1</code> 个分支使用<strong>低次多项式</strong>（如 <code>x^3</code>, <code>x^5</code>），<strong>最后一个分支</strong>使用一个<strong>高次逆多项式</strong>（如 <code>x^&#123;121&#125;</code>, <code>x^&#123;257&#125;</code> 的逆 <code>x^e</code>）。</li>
<li><strong>线性层：</strong> 使用一个<strong>循环矩阵</strong>（<code>circ(1,2,...,n)</code>）来充分混合所有分支的状态。该矩阵被证明在常用参数下是 MDS（最大距离可分），提供良好的扩散性。</li>
</ul>
</li>
<li><strong>为什么有效？</strong> 这种结构设计巧妙：<ul>
<li>低次分支保证了基础效率。</li>
<li>高次分支（在最后一个位置）能<strong>极快地提升整体代数次数</strong>（一轮就可能超过域大小 <code>p</code>），这能有效抵抗多种密码分析（如插值攻击）。</li>
<li>GTDS 结构本身便于进行系统的安全分析。</li>
</ul>
</li>
</ul>
<h3 id="ArionHash：基于-Arion-的哈希函数"><a href="#ArionHash：基于-Arion-的哈希函数" class="headerlink" title="ArionHash：基于 Arion 的哈希函数"></a>ArionHash：基于 Arion 的哈希函数</h3><ul>
<li><strong>构建方式：</strong> 使用未加密钥的 Arion 置换 (<code>Arion-π</code>) 在 <strong>Sponge 结构</strong>（类似海绵吸水）中构造哈希函数。这是 AO 哈希函数的常见模式（如 Poseidon）。</li>
<li><strong>参数选择：</strong> 针对 zkSNARK 常用的椭圆曲线（BLS12, BN254）所对应的大素数域（约 250 位）进行优化。</li>
<li><strong>高效实现技巧（关键！）：</strong><ul>
<li><strong>CCZ-等价：</strong> 论文证明 Arion 的 GTDS 与另一个相关的 GTDS 是 <strong>CCZ-等价</strong>的。这个等价的 GTDS 用 <code>x^&#123;d2&#125;</code> (高次) 代替了原来的 <code>x^e</code> (逆高次)。</li>
<li><strong>电路优化：</strong> 在 zkSNARK 电路中，与其直接计算逆高次 <code>x^e</code>（代价高昂），不如计算其等价的<strong>高次幂 <code>x^&#123;d2&#125;</code></strong>（并验证 <code>(x^e)^&#123;d2&#125; = x</code>）。因为计算 <code>x^&#123;d2&#125;</code> 可以用优化的加法链实现（见表 1），比计算逆 <code>x^e</code> 所需的乘法约束少得多！这是 ArionHash 高性能的关键。</li>
</ul>
</li>
</ul>
<h3 id="安全分析（核心结论）"><a href="#安全分析（核心结论）" class="headerlink" title="安全分析（核心结论）"></a>安全分析（核心结论）</h3><p>论文进行了广泛分析，结论是对于推荐参数（轮数 <code>r</code> 和分支数 <code>n</code>，见表 3）和足够大的素数 <code>p</code>（如 BLS12&#x2F;BN254 对应的约 250 位素数），Arion 和 ArionHash 能提供 <strong>128 比特的安全级别</strong>，抵抗以下攻击：</p>
<ul>
<li><strong>差分密码分析 (DC)：</strong> 攻击概率被 GTDS 的高次数和 MDS 线性层有效压制。</li>
<li><strong>线性密码分析 (LC)：</strong> 类似 DC，攻击相关性被有效压制。</li>
<li><strong>截断差分 &amp; 反弹攻击：</strong> 概率极低。</li>
<li><strong>插值攻击 &amp; 积分攻击：</strong> 得益于极快的次数增长（常在第一轮就超过 <code>p</code>），目标函数在有限域上几乎等价于随机函数，插值或寻找积分区分器代价过高。</li>
<li><strong>代数攻击（Gröbner 基）：</strong> 通过理论模型和小规模实验，论文估算构造的方程系统的求解复杂度远超 2^128 操作。这是确定轮数 <code>r</code> 的主要依据（见表 3）。论文还区分了：<ul>
<li><strong>Arion &amp; ArionHash：</strong> 更保守，抵抗概率性和确定性 Gröbner 基求解算法。</li>
<li><strong>α-Arion &amp; α-ArionHash：</strong> 更激进（轮数更少），只抵抗确定性 Gröbner 基求解算法（假设概率性算法不奏效）。</li>
</ul>
</li>
</ul>
<h3 id="性能：碾压-Poseidon"><a href="#性能：碾压-Poseidon" class="headerlink" title="性能：碾压 Poseidon"></a>性能：碾压 Poseidon</h3><ul>
<li><strong>对比对象：</strong> Poseidon（当前 zkSNARK 中最主流的 AO 哈希）、Griffin、Anemoi（同期较新的 AO 哈希）。</li>
<li><strong>指标：</strong> 在 zkSNARK 电路中生成证明所需时间（约束数直接影响时间）。</li>
<li><strong>结果（核心亮点）：</strong><ul>
<li><strong>R1CS 约束数（表 5）：</strong> ArionHash 的约束数显著少于 Poseidon（约一半），与 Griffin 和 Anemoi 相当或更优。</li>
<li><strong>Plonk 约束数（表 7）：</strong> 同样显著优于 Poseidon。</li>
<li><strong>实际运行时间（表 21, 22）：</strong><ul>
<li><strong>在 libsnark (R1CS) 上：</strong> ArionHash 生成 Merkle Tree 成员证明的时间<strong>比 Poseidon 快约 2 倍</strong>。</li>
<li><strong>在 Dusk-Plonk 上：</strong> ArionHash 生成证明的时间也<strong>大约是 Poseidon 的一半</strong>。</li>
<li><strong>α-ArionHash：</strong> 在常用参数（如 <code>n=3,4</code> 的 Merkle Tree）下，甚至<strong>比 Griffin 更快</strong>。</li>
</ul>
</li>
</ul>
</li>
<li><strong>原因：</strong> 主要归功于利用 CCZ-等价实现的电路优化，避免了高代价的逆高次幂计算。</li>
</ul>
<h3 id="总结给初学者"><a href="#总结给初学者" class="headerlink" title="总结给初学者"></a>总结给初学者</h3><ol>
<li><strong>它是什么？</strong> Arion 是一对新的密码学工具：一个置换（Arion）和一个基于它的哈希函数（ArionHash）。</li>
<li><strong>为谁设计？</strong> 专门为零知识证明（zkSNARKs）等需要高效“算术化”的场景设计。</li>
<li><strong>核心创新点？</strong><ul>
<li>使用一种叫 <strong>GTDS</strong> 的新数学结构来构建轮函数。</li>
<li><strong>分支策略：</strong> 大部分分支用简单的低次运算，最后一个分支用复杂的高次运算（但巧妙避开了直接计算其逆）。</li>
<li>利用 <strong>CCZ-等价</strong> 关系，在电路实现中用计算高次幂 <code>x^&#123;d2&#125;</code> 代替计算逆高次幂 <code>x^e</code>，大幅减少约束。</li>
</ul>
</li>
<li><strong>安全吗？</strong> 论文做了全面分析，认为对于推荐的大参数（大素数、足够轮数），它能抵抗主流攻击，达到 128 比特安全。有标准版（更安全）和激进版（α版，更快）。</li>
<li><strong>有多快？</strong> <strong>非常快！</strong> 在实际的 zkSNARK 库（libsnark, Dusk-Plonk）测试中，ArionHash <strong>生成证明的速度大约是当前主流选择 Poseidon 的 2 倍</strong>。激进版 α-ArionHash 在常用场景下甚至比另一个新秀 Griffin 还快。</li>
<li><strong>资源：</strong> 代码开源在 GitHub: <code>https://github.com/sca-research/Arion</code> (SageMath, C++, Rust 实现)。</li>
</ol>
<p><strong>简单比喻：</strong> 想象 Arion 是一个特制的“密码搅拌机”（GTDS）。它大部分叶片（分支）转速慢但省力（低次运算），最后一个叶片转速极高（高次运算）确保东西瞬间被彻底搅匀（安全）。而 ArionHash 的魔法在于，它找到了一种方式，让电路只需要观察高速叶片正向转动的效果（算 <code>x^&#123;d2&#125;</code>），就能确认整个搅拌过程完成了（等价于验证了反向转动 <code>x^e</code>），从而大大节省了电路检查的成本（约束数少），最终让零知识证明跑得更快。</p>
<hr>
<hr>
<h2 id="Three-Towers-Flexible-Contrastive-Learning-with-Pretrained-Image-Models"><a href="#Three-Towers-Flexible-Contrastive-Learning-with-Pretrained-Image-Models" class="headerlink" title="Three Towers: Flexible Contrastive Learning with Pretrained Image Models"></a><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.16999">Three Towers: Flexible Contrastive Learning with Pretrained Image Models</a></strong></h2><h3 id="核心问题-6"><a href="#核心问题-6" class="headerlink" title="核心问题"></a><strong>核心问题</strong></h3><ul>
<li><strong>现有方法缺陷</strong>：<ul>
<li><strong>CLIP&#x2F;ALIGN</strong>：图像和文本塔从头训练，未利用预训练模型的知识。</li>
<li><strong>LiT</strong>：直接冻结预训练图像模型作为图像塔，但图像塔无法从对比学习中受益，性能受限于预训练模型的质量（例如预训练数据与下游任务不匹配时性能下降）。</li>
</ul>
</li>
<li><strong>目标</strong>：结合预训练模型的知识和对比学习的灵活性，提升视觉-语言模型的性能。</li>
</ul>
<hr>
<h3 id="解决方案：三塔模型（3T）"><a href="#解决方案：三塔模型（3T）" class="headerlink" title="解决方案：三塔模型（3T）"></a><strong>解决方案：三塔模型（3T）</strong></h3><h4 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a><strong>架构设计</strong></h4><ol>
<li><p><strong>三个塔</strong>：</p>
<ul>
<li><strong>主图像塔（可训练）</strong>：从头训练，学习图像特征。</li>
<li><strong>主文本塔（可训练）</strong>：从头训练，学习文本特征。</li>
<li><strong>第三塔（冻结）</strong>：加载预训练图像模型（如ImageNet或JFT训练的ViT），提供额外知识。</li>
<li><strong>关键</strong>：下游任务仅用主图像塔和主文本塔，推理成本与LiT&#x2F;CLIP相同。</li>
</ul>
</li>
<li><p><strong>损失函数</strong>：</p>
<ul>
<li>主图像塔与主文本塔的对比损失（对齐图文）。</li>
<li>主图像塔与第三塔的对比损失（对齐预训练知识）。</li>
<li>主文本塔与第三塔的对比损失（对齐文本与预训练图像特征）。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\mathcal&#123;L&#125;_&#123;\text&#123;3T&#125;&#125; = \frac&#123;1&#125;&#123;3&#125; \left( \mathcal&#123;L&#125;_&#123;f \leftrightarrow g&#125; + \mathcal&#123;L&#125;_&#123;f_h \leftrightarrow h_f&#125; + \mathcal&#123;L&#125;_&#123;g_h \leftrightarrow h_g&#125; \right)</span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="核心优势"><a href="#核心优势" class="headerlink" title="核心优势"></a><strong>核心优势</strong></h4><ul>
<li><strong>灵活性</strong>：主图像塔同时从对比学习和预训练知识中受益。</li>
<li><strong>鲁棒性</strong>：即使预训练模型质量差（如Places365），3T仍能超越基线。</li>
<li><strong>可扩展性</strong>：模型规模增大时，3T性能提升显著优于LiT。</li>
</ul>
<hr>
<h3 id="关键实验结果-1"><a href="#关键实验结果-1" class="headerlink" title="关键实验结果"></a><strong>关键实验结果</strong></h3><h4 id="1-检索任务（Image-Text-Retrieval）"><a href="#1-检索任务（Image-Text-Retrieval）" class="headerlink" title="1. 检索任务（Image-Text Retrieval）"></a><strong>1. 检索任务（Image-Text Retrieval）</strong></h4><ul>
<li><strong>结果</strong>：3T <strong>全面优于</strong> LiT和CLIP基线（见表1）。</li>
<li><strong>原因</strong>：<ul>
<li>LiT的冻结图像塔缺乏细粒度检索能力。</li>
<li>3T结合了预训练模型的泛化性和对比学习的任务适配性。</li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th align="left">方法</th>
<th align="left">Flickr img→txt</th>
<th align="left">Flickr txt→img</th>
<th align="left">COCO img→txt</th>
<th align="left">COCO txt→img</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>CLIP基线</strong></td>
<td align="left">85.0</td>
<td align="left">67.0</td>
<td align="left">60.0</td>
<td align="left">44.7</td>
</tr>
<tr>
<td align="left"><strong>LiT</strong></td>
<td align="left">83.9</td>
<td align="left">66.5</td>
<td align="left">59.5</td>
<td align="left">43.6</td>
</tr>
<tr>
<td align="left"><strong>3T</strong></td>
<td align="left"><strong>87.3</strong></td>
<td align="left"><strong>72.1</strong></td>
<td align="left"><strong>64.1</strong></td>
<td align="left"><strong>48.5</strong></td>
</tr>
</tbody></table>
<blockquote>
<p>表1：Top-1检索召回率（↑），JFT预训练，Text-Filtered WebLI数据集。</p>
</blockquote>
<h4 id="2-分类任务（Few-Zero-Shot-Classification）"><a href="#2-分类任务（Few-Zero-Shot-Classification）" class="headerlink" title="2. 分类任务（Few&#x2F;Zero-Shot Classification）"></a><strong>2. 分类任务（Few&#x2F;Zero-Shot Classification）</strong></h4><ul>
<li><strong>ImageNet-21k预训练</strong>：<ul>
<li>3T <strong>超越LiT和基线</strong>（平均准确率71.4% vs. LiT 68.3%）。</li>
<li>LiT在预训练未覆盖的任务上失效（如Stanford Cars准确率仅41.9%，3T达84.9%）。</li>
</ul>
</li>
<li><strong>JFT预训练</strong>：<ul>
<li>LiT分类略优（平均77.4%），但3T仍显著优于基线（72.4%）。</li>
<li>3T在冷门任务（如遥感图像RESISC）表现更鲁棒。</li>
</ul>
</li>
</ul>
<h4 id="3-鲁棒性验证"><a href="#3-鲁棒性验证" class="headerlink" title="3. 鲁棒性验证"></a><strong>3. 鲁棒性验证</strong></h4><ul>
<li><strong>预训练数据不匹配</strong>（如Places365）：<ul>
<li>LiT性能崩溃（平均准确率29.4%），3T保持稳定（49.3%）。</li>
</ul>
</li>
<li><strong>模型规模不匹配</strong>（小预训练模型+大文本塔）：<ul>
<li>3T受影响小，LiT性能显著下降。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="创新点与意义"><a href="#创新点与意义" class="headerlink" title="创新点与意义"></a><strong>创新点与意义</strong></h3><ol>
<li><strong>灵活的知识融合</strong>：第三塔作为”知识桥梁”，避免LiT的冻结缺陷。</li>
<li><strong>零额外推理成本</strong>：下游任务丢弃第三塔，效率与CLIP&#x2F;LiT相同。</li>
<li><strong>实用技巧</strong>：<ul>
<li><strong>测试时插值</strong>：融合主图像塔和第三塔的嵌入（<code>α·h(I) + (1-α)·f(I)</code>），可进一步提升检索和小样本分类性能（见图4）。</li>
</ul>
</li>
<li><strong>启发</strong>：预训练模型与对比学习结合时，<strong>保持图像塔可训练</strong>是关键。</li>
</ol>
<hr>
<h3 id="初学者要点-7"><a href="#初学者要点-7" class="headerlink" title="初学者要点"></a><strong>初学者要点</strong></h3><ul>
<li><strong>对比学习</strong>：通过拉近相关样本、推远不相关样本学习表征（如CLIP的图文匹配）。</li>
<li><strong>预训练模型</strong>：在大规模数据上预训练的模型（如ViT），提供通用知识。</li>
<li><strong>3T本质</strong>：<ul>
<li>用第三塔”指导”主塔学习，既吸收预训练知识，又通过对比学习适应新任务。</li>
<li><strong>相当于”教师-学生”蒸馏</strong>：第三塔是教师，主塔是学生，但学生同时接受对比学习训练。</li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>提示</strong>：3T的核心思想适用于任何需结合预训练知识和端到端训练的场景（如视频-文本模型）。</p>
</blockquote>
<hr>
<p><strong>总结</strong>：3T是一种高效融合预训练模型与对比学习的方法，在检索任务中全面领先，在分类任务中更鲁棒，且无推理开销，为视觉-语言模型提供了新范式。</p>
<hr>
<hr>
<h2 id="PaLI-A-Jointly-Scaled-Multilingual-Language-Image-Model"><a href="#PaLI-A-Jointly-Scaled-Multilingual-Language-Image-Model" class="headerlink" title="PaLI: A Jointly-Scaled Multilingual Language-Image Model"></a><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2209.06794">PaLI: A Jointly-Scaled Multilingual Language-Image Model</a></h2><h3 id="PaLI-是什么？"><a href="#PaLI-是什么？" class="headerlink" title="PaLI 是什么？"></a>PaLI 是什么？</h3><ol>
<li><strong>联合缩放的多语言图文模型：</strong> PaLI 是一个能够同时理解和生成图像和文本的巨型 AI 模型。它的独特之处在于“<strong>联合缩放</strong>”——同时、均衡地增大处理图像部分（Vision Transformer, ViT）和处理语言部分（Multilingual Text-to-Text Transfer Transformer, mT5）的规模。</li>
<li><strong>统一的任务接口：</strong> 它使用一个极其简单的接口：<strong>输入一张图片 + 一段文本（查询&#x2F;指令），输出一段文本（答案&#x2F;描述）</strong>。这个接口让它能完成各种任务（看图说话、视觉问答、文本识别理解等），并且支持<strong>超过 100 种语言</strong>。</li>
<li><strong>模块化设计：</strong> PaLI 的架构是模块化的，由两个预训练好的核心组件拼接而成：<ul>
<li><strong>视觉组件 (Vision Encoder)：</strong> 一个巨大的 Vision Transformer (ViT)，专门负责理解图片内容。论文特别训练了一个史无前例的 <strong>40 亿参数</strong>的 ViT，称为 <strong>ViT-e (enormous)</strong>。</li>
<li><strong>语言组件 (Language Encoder-Decoder)：</strong> 一个强大的多语言文本模型 <strong>mT5-XXL (130亿参数)</strong>，负责理解和生成文本。</li>
</ul>
</li>
</ol>
<h3 id="PaLI-的核心创新点"><a href="#PaLI-的核心创新点" class="headerlink" title="PaLI 的核心创新点"></a>PaLI 的核心创新点</h3><ol>
<li><strong>视觉组件的重大突破 (ViT-e)：</strong><ul>
<li>认识到以前的多模态模型（如 Flamingo）视觉部分（几亿参数）远小于语言部分（几百亿参数），限制了图像理解能力。</li>
<li>专门训练了一个 <strong>40亿参数</strong> 的 ViT-e，证明<strong>即使对于超大模型，提升视觉能力仍然能显著提升多模态性能</strong>（而不仅仅是提升纯图像分类任务）。</li>
</ul>
</li>
<li><strong>平衡的联合缩放：</strong><ul>
<li>在最大的 PaLI 模型 (<strong>PaLI-17B</strong>, 总参数约170亿) 中，视觉组件 ViT-e 占了约 <strong>40亿</strong> 参数，语言组件 mT5-XXL 占了 <strong>130亿</strong> 参数。这比之前模型的视觉&#x2F;语言比例<strong>均衡得多</strong>。</li>
<li><strong>关键发现：</strong> 增加视觉部分的参数规模（从 18亿参数的 ViT-G 到 40亿参数的 ViT-e）带来的性能提升（性价比）<strong>高于</strong> 同等参数增加在语言部分。这表明<strong>提升视觉能力对多模态模型至关重要</strong>。</li>
</ul>
</li>
<li><strong>超大规模多语言数据集 (WebLI)：</strong><ul>
<li>为了训练如此大的模型，作者构建了一个新的巨型数据集 <strong>WebLI</strong>。</li>
<li>包含约 <strong>100亿张图片</strong> 和 <strong>120亿条图文对</strong>。</li>
<li>覆盖 <strong>超过 100 种语言</strong>。</li>
<li>还额外使用公开 API 生成了 <strong>290亿条图片-OCR文本对</strong>。</li>
<li>数据经过严格去重和质量过滤（保留质量最高的前10%）。</li>
</ul>
</li>
<li><strong>混合多任务预训练：</strong> PaLI 在预训练时同时学习 8 种不同的任务，包括看图说话、视觉问答、OCR文本识别、仅文本学习、目标检测等，这使得它能获得广泛的能力。</li>
<li><strong>冻结视觉组件：</strong> 预训练时，只更新语言组件（mT5）的参数，而冻结视觉组件（ViT）的参数。实验证明这种策略效果更好。</li>
</ol>
<h3 id="PaLI-取得了什么成就？"><a href="#PaLI-取得了什么成就？" class="headerlink" title="PaLI 取得了什么成就？"></a>PaLI 取得了什么成就？</h3><ol>
<li><strong>多项任务达到世界领先水平 (SOTA)：</strong><ul>
<li><strong>看图说话 (Image Captioning)：</strong> 在权威的 COCO 数据集上创下了 <strong>149.1</strong> 的 CIDEr 分数新高（不使用 CIDEr 优化的情况下）。在包含更多罕见物体的 NoCaps 数据集上也表现优异。</li>
<li><strong>视觉问答 (Visual Question Answering, VQA)：</strong><ul>
<li>在 VQAv2 上达到 <strong>84.3%</strong> 准确率（使用<strong>开放式文本生成</strong>方式），超越了使用<strong>封闭式分类</strong>方式的模型（如 BEiT-3 的 84.0%）。</li>
<li>在需要外部知识的 OKVQA 上达到 <strong>64.5%</strong> 准确率，大幅超越之前的 SOTA (54.4%)。</li>
<li>在需要阅读图片中文字的 TextVQA 和 VizWiz-QA 上也取得了 SOTA。</li>
</ul>
</li>
<li><strong>多语言视觉问答：</strong> 在多语言的 xGQA 和 MaXM 基准测试上，PaLI-17B 相比之前模型在所有语言上都有显著提升。</li>
<li><strong>多语言看图说话：</strong> 在 Crossmodal-3600 基准测试（35种语言）上，PaLI-17B 的平均 CIDEr 分数（<strong>53.6</strong>）远超之前的 SOTA (28.9)。</li>
</ul>
</li>
<li><strong>保留强大的语言能力：</strong> 尽管主要用于图文任务，PaLI 在纯语言理解任务（如 SuperGLUE, XNLI, XQuAD）上的表现与其基础语言模型 mT5-XXL <strong>相当接近</strong>，说明它没有“遗忘”语言能力。</li>
<li><strong>零样本图像分类：</strong> PaLI 在不需要专门训练分类器的情况下，直接在 ImageNet 及其多个变体（ImageNet-R, ImageNet-A, ObjectNet 等）上进行零样本分类，也取得了非常不错的结果，甚至优于 Flamingo 的少样本结果。</li>
<li><strong>为未来模型指明方向：</strong> PaLI 证明了联合均衡地缩放视觉和语言组件、构建大规模多语言数据集、以及训练超大视觉骨干网络（ViT-e）是构建更强大多模态模型的有效路径。</li>
</ol>
<h3 id="总结给初学者-1"><a href="#总结给初学者-1" class="headerlink" title="总结给初学者"></a>总结给初学者</h3><ul>
<li><strong>PaLI 是一个“看图说话+看图答题”的超级AI模型。</strong> 你给它一张图和一个问题（比如“图片里有什么？”、“现在几点？”、“这个牌子是什么？”），它就能用文字回答你，而且能用100多种语言。</li>
<li><strong>它的核心突破是：</strong> 意识到要让模型“看”得更好（不仅仅是“说”得更好），于是造了一个巨大的“眼睛”（ViT-e, 40亿参数），并把它和一个巨大的“大脑”（mT5-XXL, 130亿参数）连接起来，让两者能力更平衡。</li>
<li><strong>为了训练它，</strong> 作者收集了海量的网络图片和文字（100亿图+120亿文，100+语言），并严格清洗。</li>
<li><strong>结果就是：</strong> PaLI 在很多看图说话、看图答题的比赛里都拿到了冠军，尤其是在多语言任务上表现突出，证明了“做大视觉”和“做多语言”的重要性。</li>
<li><strong>这个研究告诉我们：</strong> 未来要造更聪明的图文AI，不仅要继续做大语言模型，更要<strong>同步做大视觉模型</strong>，并且要用<strong>多语言大数据</strong>来训练。</li>
</ul>
<p>这篇论文展示了 Google Research 在多模态人工智能领域取得的一项重大进展，PaLI 模型在性能、多语言能力和模型设计理念上都具有里程碑意义。</p>
<hr>
<hr>
<h2 id="PaLI-3-Vision-Language-Models-Smaller-Faster-Stronger"><a href="#PaLI-3-Vision-Language-Models-Smaller-Faster-Stronger" class="headerlink" title="PaLI-3 Vision Language Models: Smaller, Faster, Stronger"></a><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.09199">PaLI-3 Vision Language Models: Smaller, Faster, Stronger</a></h2><h3 id="研究目标-1"><a href="#研究目标-1" class="headerlink" title="研究目标"></a><strong>研究目标</strong></h3><p>开发一个<strong>更小、更快、更强</strong>的多模态模型（视觉语言模型，VLM），在保持高性能的同时减少参数量（仅50亿参数），挑战传统“模型越大越好”的思维。</p>
<hr>
<h3 id="核心创新"><a href="#核心创新" class="headerlink" title="核心创新"></a><strong>核心创新</strong></h3><ol>
<li><strong>图像编码器改进</strong><ul>
<li>用 <strong>SigLIP 对比学习</strong> 替代传统的图像分类预训练（如ImageNet）。</li>
<li><strong>关键发现</strong>：SigLIP 虽在纯图像分类任务中稍弱，但在<strong>图文结合任务</strong>（如定位、文本理解）中显著优于分类预训练模型（见表1）。</li>
</ul>
</li>
<li><strong>模型架构</strong><ul>
<li><strong>图像编码器</strong>：20亿参数的SigLIP ViT（基于对比学习）。</li>
<li><strong>文本解码器</strong>：30亿参数的UL2语言模型。</li>
<li><strong>输入处理</strong>：图像→视觉标记 + 文本提示 → 生成答案（图1）。</li>
</ul>
</li>
<li><strong>训练策略优化</strong><ul>
<li><strong>三阶段训练</strong>：<ol>
<li><strong>单模态预训练</strong>：SigLIP学习图像-文本对齐。</li>
<li><strong>多模态训练</strong>：冻结图像编码器，混合数据训练图文任务。</li>
<li><strong>高分辨率微调</strong>：提升至1064×1064分辨率，增强细节理解。</li>
</ol>
</li>
<li><strong>数据增强</strong>：加入100+语言的PDF文档数据，强化文本理解。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="关键成果"><a href="#关键成果" class="headerlink" title="关键成果"></a><strong>关键成果</strong></h3><ol>
<li><strong>性能超越大模型</strong><ul>
<li>在10+项图文任务中达到<strong>SOTA</strong>，性能媲美10倍参数的模型（如PaLI-X 550亿参数）。</li>
<li><strong>显著优势任务</strong>：<ul>
<li><strong>视觉文本理解</strong>（TextVQA等）：无OCR辅助时领先SOTA 4.4分（表2）。</li>
<li><strong>指代表达分割</strong>（RefCOCO）：mIoU达77.33，刷新记录（表3）。</li>
</ul>
</li>
<li><strong>视频任务</strong>：未使用视频数据预训练，却在MSR-VTT-QA等任务中超越SOTA（表5）。</li>
</ul>
</li>
<li><strong>多语言能力</strong><ul>
<li>20亿参数SigLIP编码器在<strong>36种语言</strong>的图文检索任务（Crossmodal-3600）中达到SOTA（表6）。</li>
</ul>
</li>
<li><strong>效率与环保</strong><ul>
<li>小模型<strong>训练&#x2F;部署成本更低</strong>，适合实际应用。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="重要结论"><a href="#重要结论" class="headerlink" title="重要结论"></a><strong>重要结论</strong></h3><ol>
<li><strong>预训练方法决定能力</strong><ul>
<li>图文任务中，<strong>对比学习（SigLIP）</strong> 比分类预训练更有效，尤其擅长<strong>空间定位</strong>和<strong>文本理解</strong>。</li>
<li>纯图像评估（如ImageNet）无法反映模型在图文任务中的真实能力（表6）。</li>
</ul>
</li>
<li><strong>小模型的潜力</strong><ul>
<li>通过改进架构和训练策略，<strong>5B参数模型</strong>可超越<strong>50B+模型</strong>，推动高效VLM研究。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="模型局限"><a href="#模型局限" class="headerlink" title="模型局限"></a><strong>模型局限</strong></h3><ul>
<li>复杂图表推理（如ChartQA）仍落后于超大模型（PaLI-X），因语言组件较小。</li>
<li>存在轻微性别偏差（女性职业描述困惑度更高，图2），需进一步优化。</li>
</ul>
<hr>
<h3 id="初学者要点-8"><a href="#初学者要点-8" class="headerlink" title="初学者要点"></a><strong>初学者要点</strong></h3><ul>
<li><strong>SigLIP</strong>：一种让图像和文本“互相对齐”的预训练方法，适合图文任务。</li>
<li><strong>高分辨率</strong>：提升图像细节捕捉能力（如小文字识别）。</li>
<li><strong>无OCR</strong>：PaLI-3自身学会“读图内文字”，减少对外部工具的依赖。</li>
<li><strong>小模型大能量</strong>：证明模型设计比盲目扩大参数更重要！</li>
</ul>
<blockquote>
<p><strong>类比理解</strong>：<br>PaLI-3 像一辆<strong>高效混合动力车</strong>——SigLIP引擎（对比学习）专为图文道路优化，UL2变速箱（语言模型）灵活生成答案，虽体积小（5B参数），却跑赢大排量超跑（50B+模型）。</p>
</blockquote>
<hr>
<hr>
<h2 id="LocCa"><a href="#LocCa" class="headerlink" title="LocCa"></a><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.19596">LocCa</a></strong></h2><h3 id="核心目标-1"><a href="#核心目标-1" class="headerlink" title="核心目标"></a><strong>核心目标</strong></h3><p><strong>解决什么问题？</strong><br>传统视觉预训练模型（如CLIP）擅长整体图像理解（例如分类），但缺乏对<strong>物体位置信息</strong>的精细感知能力（例如检测物体位置）。LocCa提出一种新方法，让模型同时学习图像内容和物体位置。</p>
<hr>
<h3 id="关键创新：LocCa"><a href="#关键创新：LocCa" class="headerlink" title="关键创新：LocCa"></a><strong>关键创新：LocCa</strong></h3><h4 id="1-任务设计"><a href="#1-任务设计" class="headerlink" title="1. 任务设计"></a>1. <strong>任务设计</strong></h4><p>LocCa在预训练阶段同时执行<strong>三个任务</strong>：</p>
<ul>
<li><strong>标准图像描述（Cap）</strong>：生成整图的文字描述（如“一只猫在沙发上”）。</li>
<li><strong>自动指代表达（AREF）</strong>：输入物体描述 → 预测其<strong>边界框坐标</strong>（如“黑猫” → <code>[x1, y1, x2, y2]</code>）。</li>
<li><strong>接地描述（GCAP）</strong>：输入边界框 → 生成<strong>框内物体描述</strong>（如 <code>[x1, y1, x2, y2]</code> → “黑色猫咪”）。</li>
</ul>
<h4 id="2-模型架构"><a href="#2-模型架构" class="headerlink" title="2. 模型架构"></a>2. <strong>模型架构</strong></h4><ul>
<li><strong>编码器</strong>：Vision Transformer（ViT），提取图像特征。</li>
<li><strong>解码器</strong>：Transformer，根据<strong>任务前缀</strong>（如“ARef:”）生成对应输出（框坐标或文本）。</li>
<li><strong>特点</strong>：单一模型处理多任务，无需复杂结构（如Faster R-CNN中的区域提议网络）。</li>
</ul>
<h4 id="3-数据生成"><a href="#3-数据生成" class="headerlink" title="3. 数据生成"></a>3. <strong>数据生成</strong></h4><ul>
<li>使用公开检测模型（OWL-ViT）自动生成<strong>伪标注</strong>：为网络图片中的物体生成框坐标和文本标签。</li>
<li>简化数据标注成本，实现大规模预训练。</li>
</ul>
<hr>
<h3 id="为什么有效？-1"><a href="#为什么有效？-1" class="headerlink" title="为什么有效？"></a><strong>为什么有效？</strong></h3><ul>
<li><strong>位置感知任务</strong>强制模型学习物体位置与视觉特征的关联。</li>
<li><strong>多任务协同</strong>：GCAP任务提升物体识别能力，AREF任务提升定位能力，二者互补。</li>
<li><strong>自然语言接口</strong>：框坐标被转为文本序列（如“23 45 100 200”），模型像处理语言一样处理位置信息。</li>
</ul>
<hr>
<h3 id="实验结果-2"><a href="#实验结果-2" class="headerlink" title="实验结果"></a><strong>实验结果</strong></h3><h4 id="优势领域（显著提升）："><a href="#优势领域（显著提升）：" class="headerlink" title="优势领域（显著提升）："></a>优势领域（显著提升）：</h4><ol>
<li><strong>位置敏感任务</strong>：<ul>
<li><strong>指代表达理解（REC）</strong>：在RefCOCO系列数据集上达到SOTA（准确率提升3-5%）。</li>
<li><strong>目标检测（OD）</strong>：COCO检测任务中mAP超越基线模型。</li>
</ul>
</li>
<li><strong>细粒度理解</strong>：<ul>
<li>视觉问答（VQA&#x2F;GQA）表现更好（需理解物体属性和关系）。</li>
</ul>
</li>
</ol>
<h4 id="保持竞争力的领域："><a href="#保持竞争力的领域：" class="headerlink" title="保持竞争力的领域："></a>保持竞争力的领域：</h4><ul>
<li>图像分类（ImageNet）、整体图像描述（COCO Caption）等<strong>整体理解任务</strong>与基线模型相当。</li>
</ul>
<hr>
<h3 id="初学者要点-9"><a href="#初学者要点-9" class="headerlink" title="初学者要点"></a><strong>初学者要点</strong></h3><ol>
<li><strong>核心思想</strong>：<br><strong>“让描述图像的模型学会指出物体位置”</strong>——用自然语言统一处理视觉和位置信息。</li>
<li><strong>技术亮点</strong>：<ul>
<li>通过<strong>任务前缀</strong>（如“GCap:”）切换模型行为。</li>
<li><strong>框坐标转为文本</strong>，简化多任务学习。</li>
</ul>
</li>
<li><strong>实际价值</strong>：<ul>
<li>下游任务（如视觉问答、目标检测）性能更强。</li>
<li>模型结构简单，易于扩展（无需预训练组件）。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="局限与未来"><a href="#局限与未来" class="headerlink" title="局限与未来"></a><strong>局限与未来</strong></h3><ul>
<li><strong>当前不足</strong>：零样本分割能力弱（需像素级标注预训练）。</li>
<li><strong>未来方向</strong>：加入分割任务，提升像素级理解能力。</li>
</ul>
<blockquote>
<p><strong>一句话总结</strong>：<br>LocCa让视觉模型像“描述场景+指出物体位置”的人一样工作，通过多任务预训练同时掌握整体理解和精确定位能力。</p>
</blockquote>
<hr>
<hr>
<h2 id="PaliGemma-PaliGemma-2"><a href="#PaliGemma-PaliGemma-2" class="headerlink" title="PaliGemma, PaliGemma 2"></a><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.07726">PaliGemma</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.03555">PaliGemma 2</a></strong></h2><h3 id="核心目标-2"><a href="#核心目标-2" class="headerlink" title="核心目标"></a><strong>核心目标</strong></h3><p>提出 <strong>PaliGemma</strong>——一个开源的、轻量级（30亿参数）视觉语言模型（VLM），专为<strong>迁移学习</strong>优化，能在多种任务中高效适配且性能强劲。</p>
<hr>
<h3 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a><strong>模型架构</strong></h3><ol>
<li><strong>三大组件</strong>：<ul>
<li><strong>视觉编码器</strong>：SigLIP-So400m（高性能小尺寸ViT变体）。</li>
<li><strong>语言模型</strong>：Gemma-2B（轻量但强大的Decoder-only语言模型）。</li>
<li><strong>连接层</strong>：简单线性层（实验证明比复杂MLP更有效）。</li>
</ul>
</li>
<li><strong>输入处理</strong>：<ul>
<li>图像 → 固定分辨率（224&#x2F;448&#x2F;896px）→ SigLIP编码为图像Token。</li>
<li>文本 → Gemma分词 → 与图像Token拼接 → 输入Gemma生成答案。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="训练策略（四阶段）"><a href="#训练策略（四阶段）" class="headerlink" title="训练策略（四阶段）"></a><strong>训练策略（四阶段）</strong></h3><ol>
<li><strong>Stage0</strong>：单模态预训练<ul>
<li>直接使用预训练的SigLIP和Gemma，<strong>不额外训练</strong>。</li>
</ul>
</li>
<li><strong>Stage1</strong>：多模态预训练<ul>
<li><strong>解冻全部参数</strong>（与常见冻结视觉编码器的做法不同），用10亿样本学习跨模态关联。</li>
</ul>
</li>
<li><strong>Stage2</strong>：分辨率提升<ul>
<li>短时训练（448px和896px），增强细节理解能力（如小物体检测、OCR）。</li>
</ul>
</li>
<li><strong>Stage3</strong>：任务迁移<ul>
<li>简单微调即可适配30+任务（超参少且鲁棒性强）。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="关键优势-3"><a href="#关键优势-3" class="headerlink" title="关键优势"></a><strong>关键优势</strong></h3><ol>
<li><strong>小模型，大能量</strong>：<ul>
<li>仅3B参数，性能媲美10倍+大小的模型（如PaLI-X）。</li>
</ul>
</li>
<li><strong>分辨率灵活性</strong>：<ul>
<li>提供3种分辨率检查点（224&#x2F;448&#x2F;896px），<strong>高分辨率显著提升OCR&#x2F;分割任务</strong>（如DocVQA从43.7%→84.8%）。</li>
</ul>
</li>
<li><strong>迁移友好</strong>：<ul>
<li><strong>超参简单</strong>：多数任务用默认设置（lr&#x3D;1e-5, epoch&#x3D;10）即接近最优。</li>
<li><strong>少量数据有效</strong>：256个样本可达80%全量性能（图12）。</li>
</ul>
</li>
<li><strong>任务通用性</strong>：<br>支持图像描述、VQA、文档理解、遥感图像、视频问答等近40种任务。</li>
</ol>
<hr>
<h3 id="重要发现-1"><a href="#重要发现-1" class="headerlink" title="重要发现"></a><strong>重要发现</strong></h3><ol>
<li><strong>设计选择</strong>：<ul>
<li>线性连接层优于MLP（第5.5节）。</li>
<li>训练时<strong>不解冻视觉编码器</strong>效果更好（第5.4节）。</li>
</ul>
</li>
<li><strong>数据效率</strong>：<ul>
<li>多模态预训练需大量数据（10亿样本），但迁移阶段数据需求极低。</li>
</ul>
</li>
<li><strong>评测创新</strong>：<ul>
<li>提出<strong>CountBenchQA</strong>数据集，解决TallyQA的计数偏差问题（附录D）。</li>
<li><strong>WidgetCap榜单修正</strong>：此前SOTA结果存在评测错误（附录E）。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="初学者要点-10"><a href="#初学者要点-10" class="headerlink" title="初学者要点"></a><strong>初学者要点</strong></h3><ul>
<li><strong>PaliGemma是什么？</strong><br>一个”视觉+语言”联合模型，看图说话、问答、分割样样精通。</li>
<li><strong>为何重要？</strong><br>小模型易部署，简单微调就能适配新任务（如医学影像分析、自动驾驶）。</li>
<li><strong>如何用？</strong><ol>
<li>选预训练模型（224&#x2F;448&#x2F;896px）；</li>
<li>用任务数据微调（代码开源）；</li>
<li>无需复杂调参，默认设置就有效。</li>
</ol>
</li>
</ul>
<blockquote>
<p><strong>论文地址</strong>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.07726">arXiv:2407.07726v2</a><br><strong>代码模型</strong>：<a target="_blank" rel="noopener" href="https://github.com/google-research/big_vision">GitHub&#x2F;google-research&#x2F;big_vision</a></p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A[SigLIP 图像编码器] --&gt; C[线性投影层]</span><br><span class="line">B[Gemma-2B 语言模型] --&gt; C</span><br><span class="line">C --&gt; D[多任务输出\n描述/问答/分割]</span><br></pre></td></tr></table></figure>

<hr>
<hr>
<h2 id="SigLIP-2-Multilingual-Vision-Language-Encoders-with-Improved-Semantic-Understanding-Localization-and-Dense-Features"><a href="#SigLIP-2-Multilingual-Vision-Language-Encoders-with-Improved-Semantic-Understanding-Localization-and-Dense-Features" class="headerlink" title="SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features"></a><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.18318">SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features</a></strong></h2><h3 id="研究背景-1"><a href="#研究背景-1" class="headerlink" title="研究背景"></a><strong>研究背景</strong></h3><ul>
<li><strong>高能核碰撞</strong>（如RHIC&#x2F;LHC实验）能产生<strong>夸克胶子等离子体（QGP）</strong>，其演化可通过流体力学模拟。</li>
<li>碰撞瞬间的<strong>原子核结构</strong>（质子&#x2F;中子的空间分布）决定了QGP的初始条件，进而影响实验观测结果（如粒子流 $v_2$、$v_3$ 和多重数 $N_{\rm ch}$）。</li>
<li>传统上，原子核密度分布用<strong>伍兹-萨克森模型（WS）</strong> 描述：<br>ρ(r)∝[1+exp⁡(r−R0a)]−1<em>ρ</em>(<em>r</em>)∝[1+exp(<em>a**r</em>−<em>R</em>0​​)]−1<br>仅含两个参数：<strong>半密度半径 $R_0$</strong> 和 <strong>表面厚度 $a$</strong>。</li>
</ul>
<hr>
<h3 id="关键问题"><a href="#关键问题" class="headerlink" title="关键问题"></a><strong>关键问题</strong></h3><ul>
<li><strong>WS模型的局限</strong>：无法描述密度分布的<strong>精细结构</strong>（图1）。<br><a target="_blank" rel="noopener" href="https://via.placeholder.com/150">https://via.placeholder.com/150</a><br><em>实际分布（SHF计算）存在“摆动”（wiggles），源于质子&#x2F;中子的量子轨道填充。</em></li>
<li><strong>同量异位素碰撞</strong>（如 $^{96}\text{Ru}+^{96}\text{Ru}$ vs. $^{96}\text{Zr}+^{96}\text{Zr}$）对核结构差异极敏感（精度达0.5%）。<ul>
<li>若忽略精细结构，仅用WS拟合，会<strong>引入系统误差</strong>。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="研究方法"><a href="#研究方法" class="headerlink" title="研究方法"></a><strong>研究方法</strong></h3><ol>
<li><strong>密度分布</strong>：<ul>
<li>用<strong>密度泛函理论（SHF）</strong> 生成更真实的核密度（含精细结构）。</li>
<li>对比三种原子核：$^{96}\text{Zr}$（中心凹陷）、$^{197}\text{Au}$（中心凸起）、$^{112}\text{Sn}$（较平滑）。</li>
</ul>
</li>
<li><strong>碰撞模拟</strong>：<ul>
<li>采用<strong>Glauber模型</strong>模拟核碰撞，计算初始状态量：<ul>
<li>参与核子数 $N_{\rm part}$</li>
<li>椭圆率 $\varepsilon_2$、三角形率 $\varepsilon_3$（反映初始几何不对称性）。</li>
</ul>
</li>
</ul>
</li>
<li><strong>参数对比</strong>：<ul>
<li>比较 <strong>SHF vs. WS</strong> 对可观测量的影响。</li>
<li>模拟同量异位素差异：$\Delta a &#x3D; -0.06\ \text{fm}$ 或 $\Delta R_0 &#x3D; +0.07\ \text{fm}$。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="主要发现"><a href="#主要发现" class="headerlink" title="主要发现"></a><strong>主要发现</strong></h3><ol>
<li><strong>精细结构的影响</strong>：<ul>
<li>对 $\varepsilon_2$ <strong>影响极小</strong>（$&lt;0.5%$），但对 $\varepsilon_3$ 显著（<strong>降低约2%</strong>），尤其在中心度适中的碰撞中（图3）。<br><a target="_blank" rel="noopener" href="https://via.placeholder.com/150">https://via.placeholder.com/150</a><br><em>SHF与WS的 $\varepsilon_3$ 差异（$^{96}\text{Zr}$ 最明显）。</em></li>
<li>对 $N_{\rm part}$ 分布的影响在 $^{96}\text{Zr}$ 中达 <strong>0.4–0.6%</strong>（图2）。</li>
</ul>
</li>
<li><strong>与同量异位素信号的对比</strong>：<ul>
<li>精细结构的影响 <strong>小于</strong> $a$ 的变化（$-0.06\ \text{fm}$），但 <strong>与 $R_0$ 变化相当</strong>（图2）。<ul>
<li>例如：$\Delta a &#x3D; -0.06\ \text{fm}$ 使 $\varepsilon_2$ <strong>升高2%</strong>，而精细结构仅使 $\varepsilon_3$ <strong>降低2%</strong>。</li>
</ul>
</li>
</ul>
</li>
<li><strong>WS模型的适用性</strong>：<ul>
<li>仅调整 $R_0$ 和 $a$ <strong>无法同时拟合</strong> $\varepsilon_2$、$\varepsilon_3$、$N_{\rm part}$ 的精细结构效应（公式6）。</li>
<li>结论：研究 $\varepsilon_3$ 时 <strong>必须使用SHF</strong>，但分析 $\varepsilon_2$ 或 $N_{\rm part}$ 时WS仍可用。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="结论与意义"><a href="#结论与意义" class="headerlink" title="结论与意义"></a><strong>结论与意义</strong></h3><ul>
<li><strong>WS模型不足</strong>：无法捕捉密度分布的量子精细结构，导致 $\varepsilon_3$ 等观测量出现<strong>2%级偏差</strong>。</li>
<li><strong>对实验的影响</strong>：<ul>
<li>单次碰撞中，偏差小于实验误差，WS仍适用。</li>
<li><strong>但同量异位素比值分析</strong>（精度达0.5%）需用<strong>更真实的SHF计算</strong>，以精确约束核结构（如中子皮厚度）。</li>
</ul>
</li>
<li><strong>未来方向</strong>：拓展至形变核、其他观测量（如横动量涨落），并在流体力学模型中量化对QGP性质提取的影响。</li>
</ul>
<hr>
<h3 id="关键术语解释"><a href="#关键术语解释" class="headerlink" title="关键术语解释"></a><strong>关键术语解释</strong></h3><table>
<thead>
<tr>
<th align="left">术语</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>QGP</strong></td>
<td align="left">夸克胶子等离子体，极端高温下的核物质状态。</td>
</tr>
<tr>
<td align="left"><strong>Glauber模型</strong></td>
<td align="left">模拟核碰撞几何结构的常用方法。</td>
</tr>
<tr>
<td align="left">$\varepsilon_2$, $\varepsilon_3$</td>
<td align="left">初始空间分布的椭圆&#x2F;三角形不对称性，驱动粒子集体流 $v_2$、$v_3$。</td>
</tr>
<tr>
<td align="left"><strong>同量异位素</strong></td>
<td align="left">质量数相同、质子数不同的核（如 $^{96}\text{Zr}$ 和 $^{96}\text{Ru}$）。</td>
</tr>
</tbody></table>
<p>此总结避免了复杂公式，突出物理图像和核心结论，帮助初学者抓住研究脉络。</p>
<hr>
<hr>
<h1 id="训练"><a href="#训练" class="headerlink" title="训练"></a><strong>训练</strong></h1><h2 id="Knowledge-distillation-A-good-teacher-is-patient-and-consistent"><a href="#Knowledge-distillation-A-good-teacher-is-patient-and-consistent" class="headerlink" title="Knowledge distillation: A good teacher is patient and consistent"></a><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.05237">Knowledge distillation: A good teacher is patient and consistent</a></strong></h2><h3 id="核心问题-7"><a href="#核心问题-7" class="headerlink" title="核心问题"></a><strong>核心问题</strong></h3><p>大型视觉模型（如ResNet-152）性能强大但计算成本高，难以在实际应用（如移动设备）中部署。<br><strong>目标</strong>：将大模型（教师）压缩成小模型（学生），<strong>不损失性能</strong>且能跨架构迁移（如ResNet→MobileNet）。</p>
<hr>
<h3 id="关键发现：知识蒸馏的成功要素"><a href="#关键发现：知识蒸馏的成功要素" class="headerlink" title="关键发现：知识蒸馏的成功要素"></a><strong>关键发现：知识蒸馏的成功要素</strong></h3><ol>
<li><strong>一致性（Consistency）</strong><ul>
<li><strong>教师和学生必须接收完全相同的输入</strong>（相同的图像裁剪、增强操作）。</li>
<li>❌ 错误做法：给教师固定图像&#x2F;预计算输出，或为两者生成独立增强。</li>
<li>✅ <strong>正确做法</strong>：对同一张图像生成一次增强，同时输入教师和学生（图2右）。</li>
</ul>
</li>
<li><strong>耐心（Patience）</strong><ul>
<li>蒸馏需<strong>极长训练时间</strong>（远超常规监督学习）。</li>
<li>例如：ImageNet需训练<strong>9600个epoch</strong>（常规训练约100-300epoch）。</li>
<li>学生模型会逐渐逼近教师性能（图4），且<strong>不会过拟合</strong>。</li>
</ul>
</li>
<li><strong>函数匹配（Function Matching）</strong><ul>
<li>将蒸馏视为<strong>匹配教师与学生的函数</strong> fteacher(x)≈fstudent(x)<em>f</em>teacher(<em>x</em>)≈<em>f</em>student(<em>x</em>)。</li>
<li>使用<strong>Mixup数据增强</strong>：混合两张图像生成新样本，扩展输入空间（图2最右）。</li>
<li>即使输入扭曲，学生仍能学习教师的函数行为。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="实验验证-2"><a href="#实验验证-2" class="headerlink" title="实验验证"></a><strong>实验验证</strong></h3><ol>
<li><strong>小数据集（Flowers102等）</strong><ul>
<li>一致性+函数匹配显著优于固定教师或独立增强（图3）。</li>
<li>长训练后学生准确率逼近教师（如Flowers102达96.93%，教师97.82%）。</li>
</ul>
</li>
<li><strong>ImageNet结果</strong><ul>
<li>ResNet-50学生模型达到<strong>82.8% Top-1准确率</strong>（刷新SOTA）。</li>
<li>比原ResNet-50高4.4%，比之前最佳模型高2.2%（表2）。</li>
</ul>
</li>
<li><strong>跨架构蒸馏</strong><ul>
<li>ResNet教师→MobileNet学生：<strong>76.31%准确率</strong>（MobileNet v3 SOTA）。</li>
<li>证明方法适用于不同模型架构。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="其他重要结论"><a href="#其他重要结论" class="headerlink" title="其他重要结论"></a><strong>其他重要结论</strong></h3><ul>
<li><strong>优化器改进</strong>：二阶优化器（Shampoo）比Adam效率更高，缩短75%训练时间（图5中）。</li>
<li><strong>初始化影响</strong>：预训练初始化加速短期训练，但<strong>长期训练后从头蒸馏效果更好</strong>（图5右）。</li>
<li><strong>数据来源</strong>：使用无关数据（如用食物图像蒸馏宠物模型）也能学习，但效果较差（图6）。</li>
<li><strong>与监督学习对比</strong>：单纯延长训练+增强会导致过拟合，蒸馏损失函数必不可少（图7）。</li>
</ul>
<hr>
<h3 id="为什么此前未被发现？"><a href="#为什么此前未被发现？" class="headerlink" title="为什么此前未被发现？"></a><strong>为什么此前未被发现？</strong></h3><ol>
<li>预计算教师输出可节省计算，但损害一致性。</li>
<li>其他场景（如半监督学习）的蒸馏设计与此相反。</li>
<li>短训练结果具有误导性：最佳配置需长训练才能显现。</li>
</ol>
<hr>
<h3 id="实践建议"><a href="#实践建议" class="headerlink" title="实践建议"></a><strong>实践建议</strong></h3><ol>
<li><strong>始终为师生提供相同输入</strong>（相同裁剪+增强）。</li>
<li><strong>使用Mixup增强</strong>（混合系数从[0,1]均匀采样）。</li>
<li><strong>训练足够长</strong>（数千epoch）。</li>
<li>优先选择<strong>函数匹配模式</strong>（FunMatch），而非固定教师。</li>
</ol>
<blockquote>
<p><strong>论文贡献</strong>：未提出新算法，但通过系统实验揭示了知识蒸馏成功的<strong>关键设计原则</strong>，大幅提升模型压缩效果。</p>
</blockquote>
<hr>
<p><strong>附：核心图示</strong></p>
<ul>
<li><strong>图2</strong>：蒸馏输入策略对比（一致性+Mixup最佳）。</li>
<li><strong>图3</strong>：一致性对Flowers102数据集的影响（一致输入显著提升性能）。</li>
<li><strong>图4</strong>：长训练使学生逐步匹配教师（不同数据集均有效）。</li>
<li><strong>表2</strong>：ResNet-50在ImageNet上达到SOTA（82.8%）。</li>
</ul>
<p>初学者可重点关注第3节（实验分析）及图&#x2F;表，理解设计选择如何影响结果。代码已开源（BigVision项目）。</p>
<hr>
<hr>
<h2 id="Sharpness-Aware-Minimization-for-Efficiently-Improving-Generalization"><a href="#Sharpness-Aware-Minimization-for-Efficiently-Improving-Generalization" class="headerlink" title="Sharpness-Aware Minimization for Efficiently Improving Generalization"></a><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.01412">Sharpness-Aware Minimization for Efficiently Improving Generalization</a></strong></h2><h3 id="核心问题-8"><a href="#核心问题-8" class="headerlink" title="核心问题"></a><strong>核心问题</strong></h3><p>现代深度学习模型（如大型神经网络）容易<strong>过拟合</strong>：训练误差很低，但测试误差很高。原因是模型可能收敛到损失曲面的<strong>尖锐极小点</strong>（sharp minima），这些点对数据扰动敏感，泛化能力差。</p>
<hr>
<h3 id="关键思想：SAM（Sharpness-Aware-Minimization）"><a href="#关键思想：SAM（Sharpness-Aware-Minimization）" class="headerlink" title="关键思想：SAM（Sharpness-Aware Minimization）"></a><strong>关键思想：SAM（Sharpness-Aware Minimization）</strong></h3><p><strong>目标</strong>：同时最小化<strong>损失值</strong>和<strong>损失曲面的尖锐度</strong>（sharpness）。<br><strong>直觉</strong>：</p>
<ul>
<li>传统优化（如SGD）只找损失值低的参数点（可能位于尖锐低谷）。</li>
<li>SAM寻找<strong>平坦区域</strong>的参数（低谷宽且平），对数据扰动更鲁棒，泛化能力更强。</li>
</ul>
<p><strong>数学形式</strong>：</p>
<p>min⁡w[max⁡∥ϵ∥p≤ρLS(w+ϵ)+λ∥w∥22]<strong>w</strong>min[∥<strong>ϵ</strong>∥<em>p</em>≤<em>ρ</em>max<em>L</em>S(<strong>w</strong>+<strong>ϵ</strong>)+<em>λ</em>∥<strong>w</strong>∥22]</p>
<ul>
<li>内层 <code>max</code>：在参数邻域（半径 <code>ρ</code>）内找最差扰动 <code>ϵ</code>（衡量尖锐度）。</li>
<li>外层 <code>min</code>：优化参数 <code>w</code> 使扰动后损失最小。</li>
</ul>
<hr>
<h3 id="SAM算法步骤（简化版）"><a href="#SAM算法步骤（简化版）" class="headerlink" title="SAM算法步骤（简化版）"></a><strong>SAM算法步骤（简化版）</strong></h3><ol>
<li><p><strong>计算梯度</strong>：对当前批次数据计算损失梯度 <code>∇L(w)</code>。</p>
</li>
<li><p><strong>生成扰动</strong>：</p>
<p>ϵ^&#x3D;ρ⋅∇L(w)∥∇L(w)∥2(梯度归一化后缩放)<strong>ϵ</strong>^&#x3D;<em>ρ</em>⋅∥∇<em>L</em>(<strong>w</strong>)∥2∇<em>L</em>(<strong>w</strong>)(梯度归一化后缩放)</p>
</li>
<li><p><strong>计算扰动后梯度</strong>：在 <code>w + ϵ̂</code> 处重新计算梯度 <code>∇L(w + ϵ̂)</code>。</p>
</li>
<li><p><strong>更新参数</strong>：用 <code>∇L(w + ϵ̂)</code> 执行梯度下降（如SGD）。</p>
</li>
</ol>
<blockquote>
<p><strong>关键点</strong>：每一步需<strong>两次反向传播</strong>（计算 <code>∇L(w)</code> 和 <code>∇L(w + ϵ̂)</code>），但计算高效且易于实现。</p>
</blockquote>
<hr>
<h3 id="实验结果-3"><a href="#实验结果-3" class="headerlink" title="实验结果"></a><strong>实验结果</strong></h3><p>SAM在多个任务上显著提升泛化性能：</p>
<ol>
<li><strong>图像分类</strong>（CIFAR-10&#x2F;100, ImageNet）：<ul>
<li>WideResNet + SAM <strong>错误率降低 0.5–1.5%</strong>（表1）。</li>
<li>ResNet-152 on ImageNet：Top-1错误率从 <strong>20.3% → 18.4%</strong>（表2）。</li>
</ul>
</li>
<li><strong>迁移学习</strong>（微调预训练模型）：<ul>
<li>EfficientNet-L2 + SAM 在多个下游任务刷新SOTA（表3）。</li>
</ul>
</li>
<li><strong>标签噪声鲁棒性</strong>：<ul>
<li>SAM在噪声标签下表现媲美专用方法（如MentorMix）（表4）。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="关键优势-4"><a href="#关键优势-4" class="headerlink" title="关键优势"></a><strong>关键优势</strong></h3><ol>
<li><strong>即插即用</strong>：替换现有优化器（如SGD&#x2F;Adam）即可使用。</li>
<li><strong>兼容性强</strong>：与数据增强（AutoAugment）、正则化（ShakeDrop）等协同增效。</li>
<li><strong>理论支撑</strong>：平坦极小点与泛化能力紧密相关（附录理论证明）。</li>
<li><strong>高效扩展</strong>：仅需双倍梯度计算，适合大规模训练（支持多GPU并行）。</li>
</ol>
<hr>
<h3 id="核心概念图示"><a href="#核心概念图示" class="headerlink" title="核心概念图示"></a><strong>核心概念图示</strong></h3><ul>
<li><strong>图1（左）</strong>：SAM在各数据集上一致降低错误率。</li>
<li><strong>图1（中&#x2F;右）</strong>：SGD收敛到尖锐极小点，SAM收敛到平坦极小点。</li>
<li><strong>图3（左）</strong>：SAM显著降低Hessian矩阵特征值（曲率更低）。</li>
</ul>
<hr>
<h3 id="初学者要点-11"><a href="#初学者要点-11" class="headerlink" title="初学者要点"></a><strong>初学者要点</strong></h3><ul>
<li><strong>尖锐性（Sharpness）</strong>：损失曲面在极小点附近的陡峭程度，尖锐极小点泛化差。</li>
<li><strong>SAM本质</strong>：通过对抗扰动迫使模型逃离尖锐区域，寻找平坦优化路径。</li>
<li><strong>超参数 <code>ρ</code></strong>：控制邻域大小，<code>ρ=0.05</code> 是常用默认值。</li>
</ul>
<blockquote>
<p><strong>开源代码</strong>：<a target="_blank" rel="noopener" href="https://github.com/google-research/sam">GitHub链接</a></p>
</blockquote>
<hr>
<p><strong>总结</strong>：SAM是一种简单高效的优化器，通过<strong>同时优化损失值和损失曲面平坦性</strong>，显著提升模型泛化能力，适用于分类、微调及噪声标签场景，且易于集成到现有训练流程中。</p>
<hr>
<hr>
<h2 id="Surrogate-Gap-Minimization-Improves-Sharpness-Aware-Training"><a href="#Surrogate-Gap-Minimization-Improves-Sharpness-Aware-Training" class="headerlink" title="Surrogate Gap Minimization Improves Sharpness-Aware Training"></a><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.08065">Surrogate Gap Minimization Improves Sharpness-Aware Training</a></strong></h2><h3 id="一、研究问题"><a href="#一、研究问题" class="headerlink" title="一、研究问题"></a>一、研究问题</h3><ol>
<li><strong>背景</strong>：神经网络容易过拟合，传统优化器（如Adam&#x2F;SGD）会找到“尖锐最小值”（sharp minima），导致模型泛化能力差。</li>
<li><strong>现有方法</strong>：SAM优化器通过最小化<strong>扰动损失</strong>（$f_p(w) &#x3D; \max_{|\delta| \leq \rho} f(w+\delta)$）寻找平坦区域，提升泛化性。</li>
<li><strong>发现的问题</strong>：论文指出<strong>低扰动损失不等于平坦区域</strong>！尖锐最小值也可能有低扰动损失（图1左），说明SAM无法保证找到平坦解。</li>
</ol>
<hr>
<h3 id="二、核心创新"><a href="#二、核心创新" class="headerlink" title="二、核心创新"></a>二、核心创新</h3><ol>
<li><strong>替代间隙（Surrogate Gap）</strong><ul>
<li>定义：$h(w) &#x3D; f_p(w) - f(w)$</li>
<li><strong>物理意义</strong>：衡量损失曲面的“陡峭程度”（图1右）。$h(w)$ 越小，曲面越平坦。</li>
<li><strong>关键性质</strong>：在局部最小值处，$h(w) \approx \frac{1}{2} \rho^2 \sigma_{\text{max}}$（$\sigma_{\text{max}}$ 是Hessian矩阵最大特征值，即尖锐度的数学度量）。</li>
</ul>
</li>
<li><strong>GSAM优化算法</strong><ul>
<li><strong>目标</strong>：同时最小化扰动损失 $f_p(w)$ 和替代间隙 $h(w)$，确保找到<strong>低损失且平坦</strong>的区域。</li>
<li><strong>两步更新</strong>（图2）：<ol>
<li><strong>下降步骤</strong>：沿 $\nabla f_p(w)$ 下降（同SAM），降低 $f_p(w)$。</li>
<li><strong>上升步骤</strong>：沿 $\nabla f_{\perp}(w)$（梯度正交分量）<strong>上升</strong>，降低 $h(w)$ 且<strong>不影响</strong> $f_p(w)$。</li>
</ol>
</li>
<li><strong>计算开销</strong>：与SAM几乎相同（仅多一次梯度投影）。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="三、理论贡献"><a href="#三、理论贡献" class="headerlink" title="三、理论贡献"></a>三、理论贡献</h3><ol>
<li><strong>收敛性证明</strong>：GSAM在非凸问题上的收敛速率与Adam&#x2F;SGD相同（$O(\log T &#x2F; \sqrt{T})$）。</li>
<li><strong>泛化性证明</strong>：GSAM比SAM更显著降低替代间隙，理论上有更优泛化界（Thm 5.3）。</li>
</ol>
<hr>
<h3 id="四、实验结果"><a href="#四、实验结果" class="headerlink" title="四、实验结果"></a>四、实验结果</h3><ol>
<li><strong>性能提升</strong>（表1）：<ul>
<li><strong>ViT-B&#x2F;32</strong>：比SAM高3.2%，比AdamW高5.4%（ImageNet Top-1）。</li>
<li><strong>MLP-Mixer-B&#x2F;32</strong>：比SAM高1.2%，比AdamW高11.1%。</li>
</ul>
</li>
<li><strong>平坦性验证</strong>（图4&#x2F;8）：<br>GSAM找到的最小值 $\sigma_{\text{max}}$ 显著低于SAM，且 $h(w)$ 与 $\sigma_{\text{max}}$ 变化趋势一致。</li>
<li><strong>鲁棒性</strong>：<ul>
<li>在ImageNet-C&#x2F;R（损坏&#x2F;艺术图像）上泛化性更好。</li>
<li>适应不同数据增强（图6左）、优化器（图6中）和迁移任务（表3）。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="五、关键结论"><a href="#五、关键结论" class="headerlink" title="五、关键结论"></a>五、关键结论</h3><ol>
<li><strong>替代间隙</strong>是比扰动损失更可靠的平坦度度量。</li>
<li><strong>GSAM</strong>以可忽略的计算开销，通过“一降一升”两步优化，显著提升模型泛化能力。</li>
<li><strong>代码开源</strong>：<a target="_blank" rel="noopener" href="https://sites.google.com/view/gsam-iclr22/home">GSAM项目页</a></li>
</ol>
<blockquote>
<p><strong>初学者提示</strong>：可将优化过程想象为“寻找宽阔的山谷”。SAM只关注谷底深度，可能落入陡峭窄谷；GSAM额外修整山谷两侧使其更平坦，模型抗扰动能力更强。</p>
</blockquote>
<hr>
<hr>
<h2 id="Tuning-computer-vision-models-with-task-rewards"><a href="#Tuning-computer-vision-models-with-task-rewards" class="headerlink" title="Tuning computer vision models with task rewards"></a><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2302.08242">Tuning computer vision models with task rewards</a></strong></h2><h3 id="核心问题-9"><a href="#核心问题-9" class="headerlink" title="核心问题"></a><strong>核心问题</strong></h3><p>计算机视觉模型训练时，模型预测结果与实际任务需求（如检测准确率、图像色彩鲜艳度）常存在<strong>不匹配</strong>。传统方法通过调整模型结构或数据增强间接优化，但效果有限。</p>
<hr>
<h3 id="解决方案：任务奖励优化"><a href="#解决方案：任务奖励优化" class="headerlink" title="解决方案：任务奖励优化"></a><strong>解决方案：任务奖励优化</strong></h3><p>借鉴NLP中的强化学习思路，提出两阶段训练法：</p>
<ol>
<li><strong>预训练（MLE阶段）</strong><ul>
<li>用标准的最大似然估计（MLE）训练模型，学习数据分布（如图1）。</li>
<li>例如：在图像描述任务中，模型学习生成类似训练数据的文本描述。</li>
</ul>
</li>
<li><strong>奖励调优（REINFORCE阶段）</strong><ul>
<li>用<strong>REINFORCE算法</strong>优化与任务目标相关的奖励函数（如图2）。</li>
<li>关键：模型采样多个预测结果，根据奖励值调整输出概率（高奖励样本的概率被提升）。</li>
<li><strong>优势</strong>：无需奖励函数可微，可直接优化复杂指标（如目标检测的mAP）。</li>
</ul>
</li>
</ol>
<blockquote>
<p>✅ <strong>图例说明</strong></p>
<ul>
<li><strong>图1</strong>：MLE训练模型模仿标注数据。</li>
<li><strong>图2</strong>：奖励调优根据任务奖励调整模型输出概率。</li>
</ul>
</blockquote>
<hr>
<h3 id="实验效果（四大任务）"><a href="#实验效果（四大任务）" class="headerlink" title="实验效果（四大任务）"></a><strong>实验效果（四大任务）</strong></h3><table>
<thead>
<tr>
<th align="left"><strong>任务</strong></th>
<th align="left"><strong>奖励函数</strong></th>
<th align="left"><strong>效果提升</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>全景分割</strong></td>
<td align="left">改进版PQ（匹配IoU - 误检惩罚）</td>
<td align="left">PQ从43.1%→46.1%（超越基线45.8%）</td>
</tr>
<tr>
<td align="left"><strong>目标检测</strong></td>
<td align="left">召回率（Recall）或mAP</td>
<td align="left">召回率54.4%→68.4%；mAP 39.2%→54.3%</td>
</tr>
<tr>
<td align="left"><strong>图像上色</strong></td>
<td align="left">自定义”色彩丰富度”指标</td>
<td align="left">色彩鲜艳像素比例从46%→97%，色调多样性提升</td>
</tr>
<tr>
<td align="left"><strong>图像描述</strong></td>
<td align="left">CIDEr（文本相似度指标）</td>
<td align="left">CIDEr从120→134.5（ViT-B），达SOTA水平</td>
</tr>
</tbody></table>
<hr>
<h3 id="关键优势-5"><a href="#关键优势-5" class="headerlink" title="关键优势"></a><strong>关键优势</strong></h3><ol>
<li><strong>通用性强</strong>：同一框架适用于分割、检测、上色、描述等差异极大的任务。</li>
<li><strong>灵活奖励</strong>：支持非可微奖励（如人工评估）、复杂指标（如PQ&#x2F;mAP）。</li>
<li><strong>效果显著</strong>：无需复杂启发式设计，直接优化任务目标，性能大幅提升。</li>
</ol>
<hr>
<h3 id="局限性与挑战"><a href="#局限性与挑战" class="headerlink" title="局限性与挑战"></a><strong>局限性与挑战</strong></h3><ol>
<li><strong>奖励破解（Reward Hacking）</strong>：模型可能利用奖励漏洞（如生成虚假高奖励样本），需谨慎设计奖励函数。</li>
<li><strong>计算成本</strong>：采样多个预测结果增加了训练开销（尤其自回归模型）。</li>
<li><strong>奖励设计依赖</strong>：若任务目标与奖励函数偏离，效果可能不佳。</li>
</ol>
<hr>
<h3 id="初学者要点-12"><a href="#初学者要点-12" class="headerlink" title="初学者要点"></a><strong>初学者要点</strong></h3><ul>
<li><strong>核心思想</strong>：将任务目标转化为奖励函数，用强化学习直接优化。</li>
<li><strong>两阶段训练</strong>：先学“模仿数据”（MLE），再学“满足任务”（奖励调优）。</li>
<li><strong>REINFORCE作用</strong>：通过奖励值调整模型输出概率（高奖励→高概率）。</li>
<li><strong>应用价值</strong>：使模型更贴合实际需求（如检测更准、图像更鲜艳）。</li>
</ul>
<blockquote>
<p>💡 <strong>类比理解</strong><br>想象教机器人画画：</p>
<ol>
<li><strong>MLE阶段</strong>：临摹教科书（学会基本技巧）。</li>
<li><strong>奖励调优</strong>：根据观众评分（奖励）修改画风（如更鲜艳）→ 作品更受欢迎。</li>
</ol>
</blockquote>
<p>此方法为计算机视觉模型对齐任务目标提供了简单有效的通用框架，未来可拓展到机器人操作等复杂场景。论文代码基于开源库<code>big_vision</code>实现。</p>
<hr>
<hr>
<h2 id="VeLO-Training-Versatile-Learned-Optimizers-by-Scaling-Up"><a href="#VeLO-Training-Versatile-Learned-Optimizers-by-Scaling-Up" class="headerlink" title="VeLO: Training Versatile Learned Optimizers by Scaling Up"></a><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2211.09760">VeLO: Training Versatile Learned Optimizers by Scaling Up</a></h2><h3 id="1-研究目标-4"><a href="#1-研究目标-4" class="headerlink" title="1. 研究目标"></a><strong>1. 研究目标</strong></h3><ul>
<li><strong>问题</strong>：传统深度学习模型依赖人工设计的优化器（如Adam、SGD），需手动调整超参（如学习率），效率低且泛化性差。</li>
<li><strong>解决方案</strong>：提出 <strong>VeLO（Versatile Learned Optimizer）</strong>，用神经网络自动学习优化规则，无需人工调参，适应不同任务。</li>
</ul>
<hr>
<h3 id="2-VeLO-的核心思想"><a href="#2-VeLO-的核心思想" class="headerlink" title="2. VeLO 的核心思想"></a><strong>2. VeLO 的核心思想</strong></h3><ul>
<li><strong>学习型优化器</strong>：<br>VeLO 本身是一个小型神经网络，输入是梯度（<code>grads</code>）、损失（<code>loss</code>）等训练信息，输出参数更新量（<code>updates</code>）。</li>
<li><strong>元训练（Meta-Training）</strong>：<br>通过大规模任务分布（83类任务）训练VeLO，使其学会如何优化其他模型。类似“学会如何学习”。</li>
<li><strong>无需超参调优</strong>：<br>传统优化器需针对每个任务调学习率，而VeLO自动适应不同任务。</li>
</ul>
<hr>
<h3 id="3-关键技术"><a href="#3-关键技术" class="headerlink" title="3. 关键技术"></a><strong>3. 关键技术</strong></h3><h4 id="1-网络架构"><a href="#1-网络架构" class="headerlink" title="(1) 网络架构"></a>(1) 网络架构</h4><ul>
<li><strong>分层设计</strong>：<ul>
<li><strong>Tensor级LSTM</strong>：处理整个参数张量的统计特征（如梯度均值、方差）。</li>
<li><strong>Parameter级MLP</strong>：为每个参数生成更新量，权重由LSTM动态生成（类似超网络）。</li>
</ul>
</li>
<li><strong>高效计算</strong>：<br>MLP仅2层隐藏层（4个单元），计算开销低，适合大规模模型。</li>
</ul>
<h4 id="2-训练方法"><a href="#2-训练方法" class="headerlink" title="(2) 训练方法"></a>(2) 训练方法</h4><ul>
<li><strong>超大规模元训练</strong>：<ul>
<li>计算资源：约 <strong>4000 TPU月</strong>（远超传统方法）。</li>
<li>任务多样性：涵盖图像分类（CNN&#x2F;ResNet&#x2F;ViT）、语言模型（RNN&#x2F;Transformer）、强化学习等。</li>
</ul>
</li>
<li><strong>进化策略（ES）</strong>：<br>替代反向传播，避免元训练不稳定问题。</li>
<li><strong>课程学习</strong>：<br>从短训练周期（200步）逐步过渡到长周期（20万步），提升泛化性。</li>
</ul>
<hr>
<h3 id="4-核心优势"><a href="#4-核心优势" class="headerlink" title="4. 核心优势"></a><strong>4. 核心优势</strong></h3><h4 id="1-性能远超传统优化器"><a href="#1-性能远超传统优化器" class="headerlink" title="(1) 性能远超传统优化器"></a>(1) 性能远超传统优化器</h4><ul>
<li><strong>速度对比</strong>（在83个任务上）：<ul>
<li>比学习率调优的Adam <strong>快4倍以上</strong>（50%任务）。</li>
<li>比超参调优1000次的NAdamW <strong>更快</strong>（85%任务）。</li>
<li>部分任务速度提升高达 <strong>16倍</strong>（14%任务）。</li>
</ul>
</li>
<li><strong>泛化性</strong>：<br>在未参与训练的任务上表现优异（如NERF、MLP-Mixer、知识蒸馏）。</li>
</ul>
<h4 id="2-自适应特性"><a href="#2-自适应特性" class="headerlink" title="(2) 自适应特性"></a>(2) 自适应特性</h4><ul>
<li><strong>隐式学习率调度</strong>：<br>自动调整学习率（如预热、衰减），无需手动设计。</li>
<li><strong>支持大批次训练</strong>：<br>临界批次大小（Critical Batch Size）比Adam高10倍，适合分布式训练。</li>
</ul>
<hr>
<h3 id="5-局限性"><a href="#5-局限性" class="headerlink" title="5. 局限性"></a><strong>5. 局限性</strong></h3><ul>
<li><strong>超大模型（&gt;5亿参数）</strong>：<br>效果不如调优的Adafactor（如训练8B语言模型时不稳定）。</li>
<li><strong>长周期训练（&gt;20万步）</strong>：<br>需额外微调才能保持性能。</li>
<li><strong>强化学习任务</strong>：<br>在Ant控制任务中表现较差（因未在元训练中覆盖）。</li>
</ul>
<hr>
<h3 id="6-开源与影响"><a href="#6-开源与影响" class="headerlink" title="6. 开源与影响"></a><strong>6. 开源与影响</strong></h3><ul>
<li><strong>开源资源</strong>：<br>提供VeLO代码、训练基准库（<a target="_blank" rel="noopener" href="https://velo-code.github.io/">velo-code.github.io</a>）。</li>
<li><strong>意义</strong>：<br>证明“学习型优化器”可替代人工设计优化器，是首个通用深度学习优化器。</li>
</ul>
<hr>
<h3 id="初学者理解要点"><a href="#初学者理解要点" class="headerlink" title="初学者理解要点"></a><strong>初学者理解要点</strong></h3><ol>
<li><strong>VeLO是什么？</strong><br>一个用神经网络构建的“自动优化器”，输入梯度，输出参数更新。</li>
<li><strong>它强在哪？</strong><ul>
<li>不用调学习率，省时省力。</li>
<li>在80多个任务中比Adam快4倍以上。</li>
</ul>
</li>
<li><strong>如何训练的？</strong><br>用海量计算资源（4000 TPU月）和多样化任务“教”它如何优化模型。</li>
<li><strong>哪里能用？</strong><br>支持视觉、语言、科学计算等任务，代码已开源。</li>
</ol>
<blockquote>
<p><strong>类比</strong>：传统优化器像手动挡汽车（需换挡调参），VeLO像自动驾驶汽车（自动适应路况）。</p>
</blockquote>
<hr>
<p><strong>论文标题</strong>：VeLO: Training Versatile Learned Optimizers by Scaling Up<br><strong>作者</strong>：Google Research, Brain Team<br><strong>核心贡献</strong>：首个通用学习型优化器，通过超大规模元训练实现免调参、高性能优化。</p>
<hr>
<hr>
<h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a><strong>其他</strong></h1><h2 id="Are-we-done-with-ImageNet"><a href="#Are-we-done-with-ImageNet" class="headerlink" title="Are we done with ImageNet?"></a><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.07159">Are we done with ImageNet?</a></strong></h2><h3 id="研究背景-2"><a href="#研究背景-2" class="headerlink" title="研究背景"></a><strong>研究背景</strong></h3><p>ImageNet 是计算机视觉领域的经典基准数据集，用于评估图像分类模型。但随着模型精度逼近人类水平（原标签准确率 &gt;90%），研究者质疑：</p>
<ul>
<li><strong>模型是否真正学会泛化？</strong></li>
<li><strong>还是过度拟合了 ImageNet 标签的缺陷？</strong></li>
</ul>
<p>论文发现原标签存在 <strong>三大问题</strong>：</p>
<ol>
<li><strong>单标签限制</strong>：真实图像常含多个物体（如“键盘+显示器”），但 ImageNet 只给一个标签。</li>
<li><strong>标注偏差</strong>：标注时仅询问“某类是否在图中”，未对比其他类别（如将“羽毛围巾”误标为“羽毛笔”）。</li>
<li><strong>模糊类别</strong>：某些类别人眼难以区分（如“太阳镜” vs “墨镜”）。</li>
</ol>
<hr>
<h3 id="解决方案：Reassessed-Labels-Real"><a href="#解决方案：Reassessed-Labels-Real" class="headerlink" title="解决方案：Reassessed Labels (Real)"></a><strong>解决方案：Reassessed Labels (Real)</strong></h3><p>作者构建了更鲁棒的新标签集 <strong>Real</strong>：</p>
<ol>
<li><strong>生成候选标签</strong>：<ul>
<li>用 <strong>19 个不同模型</strong>（不同架构&#x2F;训练方法）为每张图生成预测标签。</li>
<li>加入原始 ImageNet 标签。</li>
</ul>
</li>
<li><strong>人工筛选</strong>：<ul>
<li>5 名标注者对候选标签投票（“是&#x2F;否&#x2F;不确定”）。</li>
<li>最终保留高置信度标签，支持<strong>多标签</strong>（一张图可属多个类）。</li>
</ul>
</li>
<li><strong>新评估指标 Real Accuracy</strong>：<ul>
<li>模型预测只要匹配任意一个 Real 标签即算正确。</li>
</ul>
</li>
</ol>
<blockquote>
<p>✅ <strong>优势</strong>：解决单标签偏差，更贴合人类视觉判断。</p>
</blockquote>
<hr>
<h3 id="关键发现"><a href="#关键发现" class="headerlink" title="关键发现"></a><strong>关键发现</strong></h3><ol>
<li><strong>原标签已过时</strong>：<ul>
<li>新模型（如 BiT-L、Noisy Student）的 <strong>Real 准确率超过原始 ImageNet 标签</strong>（图1）。</li>
<li>说明原标签不再是最佳评估标准。</li>
</ul>
</li>
<li><strong>近期进步被高估</strong>：<ul>
<li>早期模型（如 AlexNet、ResNet）在 ImageNet 和 Real 上的表现<strong>强相关</strong>（斜率 0.86）。</li>
<li>近期模型在 ImageNet 上的提升<strong>仅有 51% 转化为 Real 提升</strong>（图4），表明部分“进步”源于拟合标签缺陷。</li>
</ul>
</li>
<li><strong>标签缺陷被模型利用</strong>：<ul>
<li>某些类（如“台式电脑”）常与其他类共现（75% 含“显示器”）。</li>
<li>模型通过学<strong>关联性</strong>而非识别物体来猜对标签（图6），导致在 Real 上泛化差。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="改进训练方法"><a href="#改进训练方法" class="headerlink" title="改进训练方法"></a><strong>改进训练方法</strong></h3><p>针对标签噪声，提出两种有效方案：</p>
<ol>
<li><strong>Sigmoid 损失函数</strong>：<ul>
<li>替代传统的 Softmax，支持<strong>多标签预测</strong>。</li>
</ul>
</li>
<li><strong>清洗训练集</strong>：<ul>
<li>用高精度模型（如 BiT-L）过滤错误标签。</li>
<li><strong>结果</strong>：ResNet-50 的准确率提升 <strong>2.5%</strong>，且长训练时不退化（表2）。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a><strong>结论</strong></h3><ul>
<li><strong>原始 ImageNet 标签的评估价值正在终结</strong>：模型开始超越其可靠性。</li>
<li><strong>但 ImageNet 仍是重要基准</strong>：经 <strong>Real 标签修正</strong>后，它仍能推动视觉研究。</li>
<li><strong>未来方向</strong>：需关注<strong>多标签评估</strong>、<strong>细粒度分类</strong>，并探索更鲁棒的基准（如 ObjectNet）。</li>
</ul>
<blockquote>
<p>🔗 <strong>资源</strong>：Real 标签已开源：<a target="_blank" rel="noopener" href="https://github.com/google-research/reassessed-imagenet">GitHub</a></p>
</blockquote>
<hr>
<h3 id="初学者要点-13"><a href="#初学者要点-13" class="headerlink" title="初学者要点"></a><strong>初学者要点</strong></h3><table>
<thead>
<tr>
<th align="left"><strong>概念</strong></th>
<th align="left"><strong>解释</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>过拟合标签</strong></td>
<td align="left">模型学会利用数据集的标注缺陷（如物体共现规律），而非真正理解图像内容。</td>
</tr>
<tr>
<td align="left"><strong>多标签必要性</strong></td>
<td align="left">真实世界图像常含多个物体，单一标签无法全面描述。</td>
</tr>
<tr>
<td align="left"><strong>Real 的意义</strong></td>
<td align="left">提供更接近人类判断的评估标准，减少模型“作弊”空间。</td>
</tr>
<tr>
<td align="left"><strong>Sigmoid 损失</strong></td>
<td align="left">允许模型同时预测多个标签（如“猫+沙发”），更贴合实际场景。</td>
</tr>
</tbody></table>
<p>此工作揭示了基准数据集需持续迭代，也展示了如何通过改进标注和训练方法提升模型泛化能力。</p>
<hr>
<hr>
<h2 id="No-Filter-Cultural-and-Socioeconomic-Diversity-in-Contrastive-Vision-Language-Models"><a href="#No-Filter-Cultural-and-Socioeconomic-Diversity-in-Contrastive-Vision-Language-Models" class="headerlink" title="No Filter: Cultural and Socioeconomic Diversity in Contrastive Vision-Language Models"></a><strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.14680">No Filter: Cultural and Socioeconomic Diversity in Contrastive Vision-Language Models</a></strong></h2><h3 id="研究背景-3"><a href="#研究背景-3" class="headerlink" title="研究背景"></a><strong>研究背景</strong></h3><ul>
<li><strong>AI普及与信任问题</strong>：AI已渗透日常生活（医疗、金融、自动驾驶等），但用户对AI的信任（Trust）或不信任（Distrust）直接影响其应用效果。信任不足会阻碍AI的采用（如医生不信任AI诊断工具）。</li>
</ul>
<hr>
<h3 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a><strong>核心概念</strong></h3><ol>
<li><strong>信任的定义</strong><ul>
<li>用户接受AI决策、共享信息并依赖系统的意愿。</li>
<li>与技术可靠性（Trustworthiness）不同：<strong>可靠的AI不一定被信任</strong>（如高精度医疗AI因”黑箱”逻辑不被医生信任），而<strong>低可靠性AI可能因界面友好被过度信任</strong>（如美观但低效的AI）。</li>
</ul>
</li>
<li><strong>信任的维度</strong><ul>
<li><strong>技术指标</strong>：准确性、安全性、鲁棒性（抗干扰能力）。</li>
<li><strong>非技术指标</strong>：透明度、可解释性、公平性、隐私保护、道德合规性。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="关键挑战"><a href="#关键挑战" class="headerlink" title="关键挑战"></a><strong>关键挑战</strong></h3><ol>
<li><strong>“黑箱”问题</strong><ul>
<li>深度学习等复杂模型决策过程不透明 → 用户难以理解AI逻辑 → 降低信任（如医疗诊断AI不被患者接受）。</li>
</ul>
</li>
<li><strong>信任与隐私的冲突</strong><ul>
<li>AI需用户数据提供个性化服务，但数据收集引发隐私担忧 → 降低信任（如用户拒绝共享健康数据）。</li>
</ul>
</li>
<li><strong>公平性与偏见</strong><ul>
<li>训练数据隐含偏见 → AI歧视特定群体（如贷款审批歧视少数族裔）→ 损害信任。</li>
</ul>
</li>
<li><strong>过度信任与信任不足</strong><ul>
<li><strong>过度信任</strong>：用户盲目依赖AI导致事故（如特斯拉驾驶员过度信任自动驾驶）。</li>
<li><strong>信任不足</strong>：用户拒绝有益AI（如医生排斥辅助诊断工具）。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a><strong>解决方案</strong></h3><ol>
<li><strong>可解释AI（XAI）</strong><ul>
<li>提供决策解释（如“为什么诊断癌症？”），增加透明度 → 提升信任（医疗AI解释依据影像特征）。</li>
</ul>
</li>
<li><strong>建立信任框架</strong><ul>
<li><strong>技术层面</strong>：开发模型性能证书（如FactSheets披露AI训练数据）。</li>
<li><strong>伦理层面</strong>：遵循欧盟提出的7大原则（人类监督、公平性、问责制等）。</li>
</ul>
</li>
<li><strong>动态信任校准</strong><ul>
<li>根据用户反馈调整AI透明度（如自动驾驶系统在不确定时提示人工接管）。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="未来方向-1"><a href="#未来方向-1" class="headerlink" title="未来方向"></a><strong>未来方向</strong></h3><ol>
<li><strong>跨学科合作</strong><ul>
<li>技术专家需与伦理、法律、社会学专家共同设计可信AI。</li>
</ul>
</li>
<li><strong>信任公平性（Trust Equity）</strong><ul>
<li>防止AI加剧社会不平等（如AI取代女性主导岗位多于男性岗位）。</li>
</ul>
</li>
<li><strong>AI-AI信任机制</strong><ul>
<li>研究AI系统间的信任（如区块链确保无人车网络数据可信）。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="初学者要点-14"><a href="#初学者要点-14" class="headerlink" title="初学者要点"></a><strong>初学者要点</strong></h3><ul>
<li><strong>信任 ≠ 可靠性</strong>：好AI可能不被信任，坏AI可能被高估。</li>
<li><strong>核心矛盾</strong>：AI越复杂越强大，但越难解释 → 用户越不信任。</li>
<li><strong>解决关键</strong>：透明化（解释决策）、公平化（消除偏见）、人机协作（动态校准信任）。</li>
</ul>
<blockquote>
<p><strong>案例助记</strong>：</p>
<ul>
<li><strong>负面</strong>：微软聊天机器人Tay被“教坏”发表种族言论 → 暴露透明度缺失。</li>
<li><strong>正面</strong>：可解释医疗AI展示诊断依据 → 医生更愿采纳建议。</li>
</ul>
</blockquote>
<p>此总结提炼了原文核心逻辑，省略了方法论细节和文献引用，适合初学者建立对AI信任问题的整体认知。</p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/blog2025.github.io/2025/08/12/Few4/" rel="prev" title="simple-cnaps">
                  <i class="fa fa-angle-left"></i> simple-cnaps
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/blog2025.github.io/2025/08/14/Few6/" rel="next" title="nobody-1617/DETA">
                  nobody-1617/DETA <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    
    <!-- 去除心形图案
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    -->
    <span class="post-meta-divider">|</span>

    <span class="author" itemprop="copyrightHolder">lsdyun</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

<!--隐藏网页底部powered by Hexo 强力驱动-->
<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>
-->

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.umd.js" integrity="sha256-a+H7FYzJv6oU2hfsfDGM2Ohw/cR9v+hPfxHCLdmCrE8=" crossorigin="anonymous"></script>
<script src="/blog2025.github.io/js/comments.js"></script><script src="/blog2025.github.io/js/utils.js"></script><script src="/blog2025.github.io/js/motion.js"></script><script src="/blog2025.github.io/js/sidebar.js"></script><script src="/blog2025.github.io/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/blog2025.github.io/js/third-party/search/local-search.js"></script>




  <script src="/blog2025.github.io/js/third-party/fancybox.js"></script>

  <script src="/blog2025.github.io/js/third-party/pace.js"></script>


  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




<script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>

<script>
var options = {
  bottom: '64px',
  right: 'unset',
  left: '32px',
  time: '0.5s',
  mixColor: 'transparent',
  backgroundColor: 'transparent',
  buttonColorDark: '#100f2c',
  buttonColorLight: '#fff',
  saveInCookies: true,
  label: '🌓',
  autoMatchOsTheme: true
}
const darkmode = new Darkmode(options);
window.darkmode = darkmode;
darkmode.showWidget();
</script>

</body>
</html>
